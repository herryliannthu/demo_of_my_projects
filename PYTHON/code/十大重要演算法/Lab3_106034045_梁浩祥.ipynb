{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS3405 Homework 3\n",
    "#### Deep Learning Neural Networks\n",
    "#### Due: Monday, November 19, 2018\n",
    "##### 1. Implement a Convolutional Neural Network (CNN) that consists of convolutional layers and fully-connected (FC or MLP) layers\n",
    "##### 2. Instantiate a [784, 50, 10] MLP and a LeNet-5-like CNN\n",
    "##### 3. Train both MLP and CNN using the MNIST-60000 dataset\n",
    "##### 4. Exploit the impact of hyperparameters (learning rate, batch size, activation function, etc) on training accuracy and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    '''Multiple Layer Perceptron'''\n",
    "    \n",
    "    def __init__(self, Layers=(2, 5, 3), BatchSize = 4):\n",
    "        self.bs = BatchSize\n",
    "        self.net = [dict() for i in range(len(Layers))] # A list of fully-connected Layers\n",
    "       \n",
    "        self.net[0]['i'] = np.zeros((Layers[0], self.bs), dtype='float64')\n",
    "        self.net[0]['z'] = np.zeros((Layers[0], self.bs), dtype='float64')\n",
    "        self.net[0]['bnz'] = np.zeros((Layers[0], self.bs), dtype='float64')\n",
    "        self.net[0]['a'] = np.zeros((Layers[0], self.bs), dtype='float64')    \n",
    "            \n",
    "        self.net[0]['dJdi'] = np.zeros(self.net[0]['i'].shape, dtype='float64')\n",
    "        self.net[0]['dJdz'] = np.zeros(self.net[0]['z'].shape, dtype='float64') \n",
    "        self.net[0]['dJdbnz'] = np.zeros(self.net[0]['bnz'].shape, dtype='float64')\n",
    "        self.net[0]['dJda'] = np.zeros(self.net[0]['a'].shape, dtype='float64')\n",
    "        \n",
    "        for i in range(1, len(Layers), 1):\n",
    "            self.net[i]['i'] = np.zeros((Layers[i-1], self.bs), dtype='float64')\n",
    "            self.net[i]['z'] = np.zeros((Layers[i], self.bs), dtype='float64')\n",
    "            self.net[i]['bnz'] = np.zeros((Layers[i], self.bs), dtype='float64')\n",
    "            self.net[i]['a'] = np.zeros((Layers[i], self.bs), dtype='float64')\n",
    "            self.net[i]['W'] = np.random.randn(Layers[i], Layers[i-1]).astype('float64')\n",
    "            self.net[i]['b'] = np.random.randn(Layers[i], 1).astype('float64')\n",
    "            self.net[i]['gamma'] = np.random.randn(Layers[i], 1).astype('float64')\n",
    "            self.net[i]['beta'] = np.random.randn(Layers[i], 1).astype('float64')\n",
    "            \n",
    "            self.net[i]['dJdi'] = np.zeros(self.net[i]['i'].shape, dtype='float64')\n",
    "            self.net[i]['dJdz'] = np.zeros(self.net[i]['z'].shape, dtype='float64')\n",
    "            self.net[i]['dJdbnz'] = np.zeros(self.net[i]['bnz'].shape, dtype='float64')\n",
    "            self.net[i]['dJda'] = np.zeros(self.net[i]['a'].shape, dtype='float64')\n",
    "            self.net[i]['dJdW'] = np.zeros(self.net[i]['W'].shape, dtype='float64')\n",
    "            self.net[i]['dJdb'] = np.zeros(self.net[i]['b'].shape, dtype='float64')\n",
    "            self.net[i]['dJdgamma'] = np.zeros(self.net[i]['gamma'].shape, dtype='float64')\n",
    "            self.net[i]['dJdbeta'] = np.zeros(self.net[i]['beta'].shape, dtype='float64')\n",
    "        \n",
    "        self.p = np.zeros(self.net[-1]['a'].shape, dtype='float64') # Softmax Output\n",
    "        self.dJdp = np.zeros(self.p.shape, dtype='float64')\n",
    "        \n",
    "        self.yhat = np.zeros(self.bs, dtype=int) # Predicted Answer\n",
    "        self.y_onehot = np.zeros(self.p.shape, dtype=int) # One-Hot-Encoded Predicted Answer\n",
    "        \n",
    "        self.J = [] # Loss value trace\n",
    "        self.J_val = [] # Loss value trace for validation\n",
    "        self.L2_regularization = [] # L2 regularization value trace\n",
    "        \n",
    "    \n",
    "    # Feed Forward Evaluation\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # For the input layer, Just copy input to Layer 0\n",
    "        np.copyto(self.net[0]['i'], x)\n",
    "        np.copyto(self.net[0]['z'], self.net[0]['i'])\n",
    "        np.copyto(self.net[0]['bnz'], self.net[0]['z'])\n",
    "        np.copyto(self.net[0]['a'], self.net[0]['bnz'])\n",
    "        \n",
    "        for i in range(1, len(self.net)): # Frome Layer 1 to the Last layer\n",
    "             \n",
    "            np.copyto(self.net[i]['i'], self.net[i-1]['a']) # Copy last layer's output \n",
    "            np.copyto(self.net[i]['z'],\n",
    "                      np.dot(self.net[i]['W'], self.net[i]['i']) + self.net[i]['b'])           \n",
    "            np.copyto(self.net[i]['bnz'], \n",
    "                      self.batch_norm(self.net[i]['z'],\n",
    "                                      self.net[i]['gamma'],\n",
    "                                      self.net[i]['beta'],\n",
    "                                      0.01))  # 0.01 is epsilon\n",
    "            np.copyto(self.net[i]['a'], \n",
    "                      self.ReLU(self.net[i]['bnz']))# Activation function\n",
    "            \n",
    "            \n",
    "        np.copyto(self.p, self.softmax(self.net[-1]['a']))  # Softmax\n",
    "        np.copyto(self.yhat, np.argmax(self.p, axis=0)) # Prediction result\n",
    "        \n",
    "        return\n",
    "   \n",
    "    # Batch Normalization for an MLP Layer\n",
    "    # x --> bnx\n",
    "    def batch_norm(self, x, gamma, beta, eps):\n",
    "        \n",
    "        bs = x.shape[-1]\n",
    "        mu = 1.0/bs * np.sum(x, axis=-1)[:, None]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=-1)[:, None]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        bnx = gamma * xhat + beta\n",
    "        \n",
    "        return bnx\n",
    "    \n",
    "    # Backprop Batch Normalization for an MLP Layer\n",
    "    # (dJdbnx, x, gamma, beta) --> (dJdbeta, dJdgamma, dJdx)\n",
    "    def batch_norm_prime(self, dJdbnx, x, gamma, beta, eps):\n",
    "        \n",
    "        bs = x.shape[-1]\n",
    "        mu = 1.0/bs * np.sum(x, axis=-1)[:, None]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=-1)[:, None]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        dJdbeta = np.mean(dJdbnx, axis=-1)[:, None]\n",
    "        dJdgamma = np.mean(dJdbnx * xhat, axis=-1)[:, None]\n",
    "        dJdx = (1.0 - 1.0/bs) * (1.0/v - u**2 / v**3 / bs) * gamma * dJdbnx\n",
    "        \n",
    "        return dJdbeta, dJdgamma, dJdx\n",
    "    \n",
    "    def softmax(self, a):\n",
    "        return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    \n",
    "    # Sigmoid activation function: z --> a\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-1.0*z))\n",
    "    \n",
    "    # a --> dadz\n",
    "    def sigmoidPrime(self, a):\n",
    "        return a * (1.0 - a)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    def tanhPrime(self, a):\n",
    "        return (1.0 - a ** 2) # Derivative of tanh is (1.0 - tanh ** 2)\n",
    "    \n",
    "    def swish(self, z):\n",
    "        return z * self.sigmoid(z)\n",
    "    \n",
    "    def swishPrime(self, a, z):\n",
    "        return a + self.sigmoid(z) * (1.0 - a) \n",
    "    \n",
    "    def ReLU(self, z):\n",
    "        a = np.copy(z)\n",
    "        a[a<0] = 0.0\n",
    "        return a\n",
    "    \n",
    "    def ReLUPrime(self, a):\n",
    "        dadz = np.copy(a)\n",
    "        dadz[a>0] = 1.0\n",
    "        return dadz\n",
    "    \n",
    "    def LeakyReLU(self, z, leakyrate):\n",
    "        a = np.copy(z)\n",
    "        (rows, cols) = a.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                a[i, j] = z[i, j] if z[i,j] > 0 else (leakyrate * z[i, j])\n",
    "        return a\n",
    "    \n",
    "    def LeakyReLUPrime(self, a, leakyrate):\n",
    "        dadz = np.copy(a)\n",
    "        (rows, cols) = dadz.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                dadz[i, j] = 1.0 if a[i,j] > 0 else leakyrate\n",
    "        return dadz\n",
    "    \n",
    "    # Loss function\n",
    "    def loss(self, y, eta):\n",
    "        \n",
    "        self.y_onehot.fill(0)\n",
    "        for i in range(self.bs):\n",
    "            self.y_onehot[y[i], i] = 1\n",
    "        self.J.append(-1.0 * np.sum(self.y_onehot * np.log(self.p) / self.bs))\n",
    "        \n",
    "        L2 = 0.0 # L2 Regularization\n",
    "        for i in range(1, len(self.net)):\n",
    "            L2 += np.sum(self.net[i]['W'] ** 2)              \n",
    "        self.L2_regularization.append(eta / 2 * L2) # Only the MLP part\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Loss function for validation\n",
    "    def loss_val(self, y, eta):\n",
    "        \n",
    "        self.y_onehot.fill(0)\n",
    "        for i in range(self.bs):\n",
    "            self.y_onehot[y[i], i] = 1     \n",
    "        self.J_val.append(-1.0 * np.sum(self.y_onehot * np.log(self.p) / self.bs))\n",
    "        \n",
    "        return\n",
    "            \n",
    "    # Backward Propagation\n",
    "    def backprop(self):\n",
    "        \n",
    "        self.dJdp = 1.0 / (1.0 - self.y_onehot - self.p)\n",
    "        dpda = np.array([[self.p[i, :] * (1.0-self.p[j, :]) if i == j\n",
    "                          else -1 * self.p[i, :] * self.p[j, :]\n",
    "                          for i in range(self.p.shape[0])]\n",
    "                         for j in range(self.p.shape[0])])\n",
    "        for i in range(self.bs):\n",
    "            self.net[-1]['dJda'][:, i] = np.dot(dpda[:, :, i], self.dJdp[:, i])\n",
    "            \n",
    "        for i in range(len(self.net)-1, 0, -1):\n",
    "            \n",
    "            np.copyto(self.net[i]['dJdbnz'],\n",
    "                      (self.net[i]['dJda'] * self.ReLUPrime(self.net[i]['a'])))       \n",
    "           \n",
    "            dJdbeta, dJdgamma, dJdBNinput = self.batch_norm_prime(self.net[i]['dJdbnz'],\n",
    "                                                                  self.net[i]['z'],\n",
    "                                                                  self.net[i]['gamma'],\n",
    "                                                                  self.net[i]['beta'],\n",
    "                                                                  0.01)  \n",
    "            np.copyto(self.net[i]['dJdbeta'], dJdbeta)\n",
    "            np.copyto(self.net[i]['dJdgamma'], dJdgamma)\n",
    "            np.copyto(self.net[i]['dJdz'], dJdBNinput) \n",
    "            \n",
    "            np.copyto(self.net[i]['dJdb'],\n",
    "                      np.mean(self.net[i]['dJdz'], axis = -1)[:, None])         \n",
    "            np.copyto(self.net[i]['dJdW'],\n",
    "                      np.dot(self.net[i]['dJdz'], self.net[i]['i'].T) / self.bs)  \n",
    "            np.copyto(self.net[i]['dJdi'],\n",
    "                      np.dot(self.net[i]['W'].T, self.net[i]['dJdz']))\n",
    "            \n",
    "            # Copy gradient at the input to be the output gradient of the previous layer\n",
    "            np.copyto(self.net[i-1]['dJda'], self.net[i]['dJdi'])\n",
    "        \n",
    "        # Layer 0 does nothing but passing gradients backward\n",
    "        np.copyto(self.net[0]['dJdbnz'], self.net[0]['dJda'])\n",
    "        np.copyto(self.net[0]['dJdz'], self.net[0]['dJdbnz'])\n",
    "        np.copyto(self.net[0]['dJdi'], self.net[0]['dJdz']) \n",
    "        return \n",
    "    \n",
    "    # Update parameters\n",
    "    def update(self, lr, eta):\n",
    "        \n",
    "        # Update W, b, gamma, beta with Weight Decay from Layer 1 to the last\n",
    "        # Layer 0 has no parameters\n",
    "        for i in range(1, len(self.net)):\n",
    "            np.copyto(self.net[i]['W'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['W'] - lr*self.net[i]['dJdW'])\n",
    "            np.copyto(self.net[i]['b'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['b'] - lr*self.net[i]['dJdb'])\n",
    "            np.copyto(self.net[i]['gamma'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['gamma'] - lr*self.net[i]['dJdgamma'])\n",
    "            np.copyto(self.net[i]['beta'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['beta'] - lr*self.net[i]['dJdbeta'])\n",
    "        return\n",
    "    \n",
    "    # Train MLP alone. For a CNN, training is via the CNN instance\n",
    "    def train(self, train_x, train_y, epoch_count, lr, eta):\n",
    "        \n",
    "        for e in range(epoch_count):\n",
    "            print (\"Epoch \", e)\n",
    "            for i in range(train_x.shape[1]//self.bs):\n",
    "                x = train_x[:, i*self.bs:(i+1)*self.bs]\n",
    "                y = train_y[i*self.bs:(i+1)*self.bs]\n",
    "                self.forward(x)\n",
    "                self.loss(y, eta)\n",
    "                self.backprop()\n",
    "                self.update(lr, eta)         \n",
    "            accuracy = 1.0 - float(len(np.nonzero(self.yhat - y)[0])) / self.bs\n",
    "            print (\"Training Accuracy=\", accuracy,\"loss = \",self.J[-1])\n",
    "        return  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Layer(object):\n",
    "    '''Fully-Connected Layer'''\n",
    "    \n",
    "    def __init__(self, Layers=(2, 5, 3), BatchSize = 1):\n",
    "        self.bs = BatchSize\n",
    "        self.net = [dict() for i in range(len(Layers))]\n",
    "        \n",
    "        self.net[0]['a'] = np.zeros((Layers[0], self.bs))\n",
    "        self.net[0]['dJda'] = np.zeros((Layers[0], self.bs))\n",
    "        \n",
    "        for i in range(1, len(Layers)):\n",
    "            self.net[i]['a'] = np.zeros((Layers[i], self.bs))\n",
    "            self.net[i]['z'] = np.zeros((Layers[i], self.bs))\n",
    "            self.net[i]['W'] = np.random.randn(Layers[i], Layers[i-1])\n",
    "            self.net[i]['b'] = np.random.randn(Layers[i], 1)\n",
    "            self.net[i]['dJda'] = np.zeros((Layers[i], self.bs))\n",
    "            self.net[i]['dJdz'] = np.zeros((Layers[i], self.bs))\n",
    "            self.net[i]['dJdW'] = np.zeros((Layers[i], Layers[i-1]))\n",
    "            self.net[i]['dJdb'] = np.zeros((Layers[i], 1))\n",
    "        \n",
    "        self.p = np.zeros(self.net[-1]['a'].shape) # Softmax Output\n",
    "        self.yhat = np.zeros(self.bs, dtype=int) # Predicted Answer\n",
    "        self.y_onehot = np.zeros(self.p.shape, dtype=int)\n",
    "        self.J = [] # Loss value trace\n",
    "            \n",
    "    def forward(self, x):\n",
    "        np.copyto(self.net[0]['a'], x)\n",
    "        \n",
    "        for i in range(1, len(self.net)):\n",
    "            np.copyto(self.net[i]['z'],\n",
    "                      np.dot(self.net[i]['W'], self.net[i-1]['a']) / self.net[i-1]['a'].shape[0] +\n",
    "                      self.net[i]['b'])\n",
    "            np.copyto(self.net[i]['a'], self.LeakyReLU(self.net[i]['z']))\n",
    "          \n",
    "        np.copyto(self.p, self.softmax(self.net[-1]['a']))   \n",
    "        np.copyto(self.yhat, np.argmax(self.p, axis=0))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def softmax(self, a):\n",
    "        return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-1.0*z))\n",
    "    \n",
    "    def sigmoidPrime(self, a):\n",
    "        return a * (1.0 - a)\n",
    "    \n",
    "    def ReLU(self, z):\n",
    "        a = np.copy(z)\n",
    "        a[a<0] = 0.0\n",
    "        return a\n",
    "    \n",
    "    def ReLUPrime(self, a):\n",
    "        dadz = np.copy(a)\n",
    "        dadz[a>0] = 1.0\n",
    "        return dadz\n",
    "    \n",
    "    def LeakyReLU(self, z, leakyrate=0.1):\n",
    "        a = np.copy(z)\n",
    "        (rows, cols) = a.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                a[i, j] = z[i, j] if z[i,j] > 0 else (leakyrate * z[i, j])\n",
    "        return a\n",
    "    \n",
    "    def LeakyReLUPrime(self, a, leakyrate = 0.1):\n",
    "        dadz = np.copy(a)\n",
    "        (rows, cols) = dadz.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                dadz[i, j] = 1.0 if a[i,j] > 0 else leakyrate\n",
    "        return dadz\n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    def tanhPrime(self, a):\n",
    "        return (1.0 - a ** 2) # Derivative of tanh is (1.0 - tanh ** 2)\n",
    "    \n",
    "    def swish(self, z):\n",
    "        return z * self.sigmoid(z)\n",
    "    \n",
    "    def swishPrime(self, a, z):\n",
    "        return a + self.sigmoid(z) * (1.0 - a) \n",
    "    \n",
    "    def loss(self, y):\n",
    "        self.y_onehot.fill(0)\n",
    "        delta = 10e-7\n",
    "        for i in range(self.bs):\n",
    "            self.y_onehot[y[i], i] = 1\n",
    "            \n",
    "        self.J.append(-1.0 * np.sum(self.y_onehot * np.log(self.p+delta)) / self.bs)\n",
    "        \n",
    "        return\n",
    "            \n",
    "    def backprop(self):\n",
    "        np.copyto(self.net[-1]['dJda'], self.p - self.y_onehot)\n",
    "        \n",
    "        for i in range(len(self.net)-1, 0, -1):\n",
    "            \n",
    "            np.copyto(self.net[i]['dJdz'],\n",
    "                      (self.net[i]['dJda'] * self.LeakyReLUPrime(self.net[i]['a'])))\n",
    "                \n",
    "            np.copyto(self.net[i]['dJdb'],\n",
    "                      np.mean(self.net[i]['dJdz'], axis = 1)[:, None])\n",
    "            np.copyto(self.net[i]['dJdW'],\n",
    "                      np.dot(self.net[i]['dJdz'], self.net[i-1]['a'].T)/self.bs)\n",
    "            np.copyto(self.net[i-1]['dJda'],\n",
    "                      np.dot(self.net[i]['dJdz'].T, self.net[i]['W']).T)\n",
    "        return\n",
    "            \n",
    "    def update(self, lr=0.01):\n",
    "        \n",
    "        for i in range(1, len(self.net)):\n",
    "            np.copyto(self.net[i]['W'],\n",
    "                      self.net[i]['W'] - lr*self.net[i]['dJdW'])\n",
    "            np.copyto(self.net[i]['b'],\n",
    "                      self.net[i]['b'] - lr*self.net[i]['dJdb'])\n",
    "        return\n",
    "        \n",
    "    def train(self, train_x, train_y, epoch_count, lr):\n",
    "        \n",
    "        for e in range(epoch_count):\n",
    "            print (\"Epoch \", e)\n",
    "            for i in range(train_x.shape[1]//self.bs):\n",
    "                x = train_x[:, i*self.bs:(i+1)*self.bs]\n",
    "                y = train_y[i*self.bs:(i+1)*self.bs]\n",
    "                self.forward(x)\n",
    "                self.loss(y)\n",
    "                self.backprop()\n",
    "                self.update(lr)\n",
    "            accuracy = 1.0 - float(len(np.nonzero(self.yhat - y)[0])) / self.bs\n",
    "            print (\"Training Accuracy=\", accuracy,\"loss = \",self.J[-1])\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer(object):\n",
    "    '''Convolution Layer'''\n",
    "    def __init__(self,\n",
    "               bs,\n",
    "               i_ch,\n",
    "               i_h,\n",
    "               i_w,\n",
    "               k_num,\n",
    "               k_h,\n",
    "               k_w,\n",
    "               stride = 1,\n",
    "               zp = 0,\n",
    "               mph = 2,\n",
    "               mpw = 2,\n",
    "                leakyrate = 0.1):\n",
    "        \n",
    "        self.bs = bs\n",
    "        self.i_ch = i_ch\n",
    "        self.i_h = i_h\n",
    "        self.i_w = i_w\n",
    "        self.k_num = k_num\n",
    "        self.k_h = k_h\n",
    "        self.k_w = k_w\n",
    "        self.stride = stride\n",
    "        self.zp = zp\n",
    "        self.mph = mph\n",
    "        self.mpw = mpw\n",
    "        self.leakyrate = leakyrate\n",
    "       \n",
    "        self.ai = np.zeros((bs, i_ch, i_h+2*zp, i_w+2*zp))\n",
    "        self.W = np.random.randn(k_num, i_ch, k_h, k_w)\n",
    "        self.b = np.random.randn(k_num, 1)\n",
    "        self.z = np.zeros((bs, k_num, ((i_h - k_h)//stride)+1, ((i_w - k_w)//stride)+1))\n",
    "        self.r = np.zeros(self.z.shape)\n",
    "        self.mp_trace = np.zeros(self.r.shape)\n",
    "        self.ao = np.zeros((bs, k_num, (i_h - k_h+1)//stride//mph, (i_w - k_w+1)//stride//mpw))\n",
    "        \n",
    "        self.dJdai = np.zeros(self.ai.shape)\n",
    "        self.dJdW = np.zeros(self.W.shape)\n",
    "        self.dJdb = np.zeros(self.b.shape)\n",
    "        self.dJdz = np.zeros(self.z.shape)\n",
    "        self.dJdr = np.zeros(self.r.shape)\n",
    "        self.dJdao = np.zeros(self.ao.shape)\n",
    "\n",
    "    def conv2d(self, i_tensor):\n",
    "        (i_bs, i_ch, i_rows, i_cols) = i_tensor.shape\n",
    "        (k_num, k_ch, k_rows, k_cols) = self.W.shape\n",
    "        self.ai.fill(0.0)\n",
    "        np.copyto(self.ai[:, :, self.zp:self.zp+i_rows, self.zp:self.zp+i_cols], i_tensor)\n",
    "        (z_bs, z_ch, z_rows, z_cols) = self.z.shape   # z_ch == k_num\n",
    "        for b in range(z_bs):\n",
    "            for k in range(k_num):\n",
    "                for r in range(z_rows):\n",
    "                    for c in range(z_cols): \n",
    "                        img =  self.ai[b, :,\n",
    "                                       r*self.stride : r*self.stride+k_rows,\n",
    "                                       c*self.stride : c*self.stride+k_cols]\n",
    "                        kernel = self.W[k, :,  :, :]\n",
    "                        self.z[b, k, r, c] = np.sum(img * kernel) + self.b[k]\n",
    "        return \n",
    "    \n",
    "    def conv2d_gp(self, it, kt, stride):       # general purpose\n",
    "        (ir, ic) = it.shape\n",
    "        (kr, kc) = kt.shape\n",
    "        ot = np.zeros(((ir - kr + 1)//stride, (ic - kc + 1)//stride))\n",
    "        (orow, oc) = ot.shape\n",
    "        for r in range(orow):\n",
    "            for c in range(oc):\n",
    "                ot[r, c] = np.sum(kt * it[r*stride:r*stride+kr, c*stride:c*stride+kc])\n",
    "        return ot\n",
    "    \n",
    "    def conv2d_prime(self): # Only support Stride = 1\n",
    "        (z_bs, z_ch, z_rows, z_cols) = self.dJdz.shape\n",
    "        (a_bs, a_ch, a_rows, a_cols) = self.dJdai.shape\n",
    "        (k_num, k_ch, k_rows, k_cols) = self.dJdW.shape\n",
    "        (b_num, b_ch) = self.dJdb.shape\n",
    "        stride = 1 # self.stride\n",
    "        self.dJdai.fill(0.0)\n",
    "        self.dJdW.fill(0.0)\n",
    "        self.dJdW.fill(0.0)\n",
    "        \n",
    "        zp_dJdz = np.zeros((z_bs, z_ch, z_rows+2*(k_rows-1), z_cols+2*(k_cols-1)))\n",
    "        np.copyto(zp_dJdz[:, :, k_rows-1:z_rows+k_rows-1, k_cols-1:z_cols+k_cols-1], self.dJdz)\n",
    "        \n",
    "        rotated_W = np.zeros(self.W.shape)\n",
    "        for k in range(k_num):\n",
    "            for kch in range(k_ch):\n",
    "                rotated_W[k, kch, :, :] = np.rot90(self.W[k, kch, :, :], 2)\n",
    "                \n",
    "        for b in range(a_bs):\n",
    "            for ch in range(a_ch):\n",
    "                for r in range(a_rows):\n",
    "                    for c in range(a_cols):\n",
    "                        for k in range(k_num):\n",
    "                            self.dJdai[b, ch, r, c] += np.sum(rotated_W[k, ch, :, :] * \n",
    "                                                              zp_dJdz[b, k, r:r+k_rows, c:c+k_cols])\n",
    "        \n",
    "        for k in range(k_num):\n",
    "            for kch in range(k_ch):\n",
    "                for kr in range(k_rows):\n",
    "                    for kc in range(k_cols):\n",
    "                        self.dJdW[k, kch, kr, kc] = np.mean(self.ai[:,\n",
    "                                                                   kch,\n",
    "                                                                   kr:a_rows-k_rows+1+kr,\n",
    "                                                                   kc:a_cols-k_cols+1+kc] *\n",
    "                                                           self.dJdz[:, k, :, :])\n",
    "        \n",
    "        for ch in range(z_ch):\n",
    "            self.dJdb[ch] = np.mean(self.dJdz[:, ch, :, :])\n",
    "            \n",
    "        return \n",
    "\n",
    "    def LeakyReLU(self):\n",
    "        np.copyto(self.r, np.where(self.z > 0, 1.0 * self.z, self.leakyrate * self.z))\n",
    "        return\n",
    "    \n",
    "    def LeakyReLU_prime(self):\n",
    "        np.copyto(self.dJdz, np.where(self.z > 0, 1.0 * self.dJdr, self.leakyrate * self.dJdr))\n",
    "        return\n",
    "            \n",
    "    def max_pool(self):\n",
    "        (r_bs, r_ch, r_rows, r_cols) = self.r.shape\n",
    "        (a_bs, a_ch, a_rows, a_cols) = self.ao.shape\n",
    "        self.mp_trace.fill(0)\n",
    "        for b in range(a_bs):\n",
    "            for ch in range(a_ch):\n",
    "                for r in range(a_rows):\n",
    "                    for c in range(a_cols):\n",
    "                        pool_src2d = self.r[b,\n",
    "                                            ch,\n",
    "                                            r*self.mph:(r+1)*self.mph,\n",
    "                                            c*self.mpw:(c+1)*self.mpw] \n",
    "                        self.ao[b, ch, r, c] = np.max(pool_src2d)\n",
    "                        max_pos = np.unravel_index(np.argmax(pool_src2d), \n",
    "                                                   np.shape(pool_src2d))\n",
    "                        self.mp_trace[b, ch, r*self.mph+max_pos[0], c*self.mpw+max_pos[1]] = 1.0\n",
    "        return\n",
    "    \n",
    "    def max_pool_prime(self):\n",
    "        (r_bs, r_ch, r_rows, r_cols) = self.dJdr.shape\n",
    "        \n",
    "        for b in range(r_bs):\n",
    "            for ch in range(r_ch):\n",
    "                for r in range(r_rows):\n",
    "                    for c in range(r_cols):\n",
    "                        self.dJdr[b, ch, r, c] = (self.dJdao[b, ch, r//self.mph, c//self.mpw] \n",
    "                                                  if self.mp_trace[b, ch, r, c] == 1.0\n",
    "                                                  else 0.0)                       \n",
    "        return\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        self.conv2d(input_tensor)\n",
    "        self.LeakyReLU()\n",
    "        self.max_pool()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def backprop(self):\n",
    "        self.max_pool_prime()\n",
    "        self.LeakyReLU_prime()\n",
    "        self.conv2d_prime()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def update(self, lr=0.1):\n",
    "        self.W -= lr * self.dJdW\n",
    "        self.b -= lr * self.dJdb\n",
    "        return\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self,\n",
    "                input_data_spec = [10, 3, 28, 28],\n",
    "                conv_layer_spec = [{\"k_num\" : 2,\n",
    "                                    \"k_h\" : 3,\n",
    "                                    \"k_w\" : 3,\n",
    "                                    \"stride\" : 1,\n",
    "                                    \"zp\" : 0,\n",
    "                                    \"mph\" : 2,\n",
    "                                    \"mpw\" :2}],\n",
    "                 fc_layer_spec = [2, 5, 3]):\n",
    "        self.c_net = []\n",
    "        self.loss = []\n",
    "        for i in range(len(conv_layer_spec)):\n",
    "            self.c_net.append(Conv_Layer(bs = input_data_spec[0],\n",
    "                                  i_ch = input_data_spec[1],\n",
    "                                  i_h = input_data_spec[2],\n",
    "                                  i_w = input_data_spec[3],\n",
    "                                  k_num = conv_layer_spec[i]['k_num'],\n",
    "                                  k_h = conv_layer_spec[i]['k_h'],\n",
    "                                  k_w = conv_layer_spec[i]['k_w'],\n",
    "                                  stride = conv_layer_spec[i]['stride'],\n",
    "                                  zp = conv_layer_spec[i]['zp'],\n",
    "                                  mph = conv_layer_spec[i]['mph'],\n",
    "                                  mpw = conv_layer_spec[i]['mpw']))\n",
    "            input_data_spec = list(self.c_net[i].ao.shape)\n",
    "                       \n",
    "        (bs, ch, r, c) = self.c_net[-1].ao.shape\n",
    "        fc_layer_spec = (ch*r*c, 50, 10)\n",
    "        self.fc_net = FC_Layer(fc_layer_spec, BatchSize=input_data_spec[0])\n",
    "        return\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        idata = input_tensor\n",
    "        for i in range(len(self.c_net)):\n",
    "            self.c_net[i].forward(idata)\n",
    "            idata = self.c_net[i].ao  \n",
    "        input_to_fc = np.copy(self.c_net[-1].ao.reshape(self.c_net[-1].ao.shape[0], -1).T)\n",
    "        \n",
    "        self.fc_net.forward(input_to_fc)\n",
    "        return\n",
    "    \n",
    "    def backprop(self):\n",
    "        self.fc_net.backprop()\n",
    "        \n",
    "        np.copyto(self.c_net[-1].dJdao, self.fc_net.net[0]['dJda'].T.reshape(self.c_net[-1].dJdao.shape))\n",
    "        \n",
    "        for i in range(len(self.c_net)-1, 0, -1):\n",
    "            self.c_net[i].backprop()\n",
    "            np.copyto(self.c_net[i-1].dJdao, self.c_net[i].dJdai)   ####  \n",
    "        self.c_net[0].backprop()\n",
    "        return\n",
    "    \n",
    "    def update(self, lr=0.1):\n",
    "        for i in range(len(self.c_net)):\n",
    "            self.c_net[i].update(lr)\n",
    "        self.fc_net.update(lr)\n",
    "        return\n",
    "\n",
    "    def train(self, train_x, train_y, epoch_count, lr, bs):\n",
    "        learning_rate = lr\n",
    "        for e in range(epoch_count):\n",
    "            predict = np.zeros((len(train_y)))\n",
    "            temp_loss = 0.0\n",
    "            for i in range(0, train_x.shape[0], bs):\n",
    "                x = train_x[i:i+bs, :, :, :]\n",
    "                y = train_y[i:i+bs]\n",
    "                \n",
    "                self.forward(x)\n",
    "                self.fc_net.loss(y)\n",
    "                self.backprop()\n",
    "                self.update(learning_rate)\n",
    "                temp_loss += self.fc_net.J[-1]\n",
    "                np.copyto(predict[i:i+bs], self.fc_net.yhat)\n",
    "                hit = 0\n",
    "                for k in range(len(self.fc_net.yhat)):\n",
    "                    if(train_y[i+k] == self.fc_net.yhat[k]):\n",
    "                        hit += 1\n",
    "                print(\"Batch:\", i//bs, \" Accuracy:\", hit/bs*100, \"%\")\n",
    "            hit = 0\n",
    "            for i in range(len(train_y)):\n",
    "                if(predict[i] == train_y[i]):\n",
    "                    hit += 1\n",
    "            self.loss.append(temp_loss/(len(train_y)//bs))\n",
    "            print(\"Epoch:\", e, \" Accuracy:\", hit/len(train_y)*100, \"%\", \" loss:\", self.loss[e])\n",
    "            if(learning_rate >= 0.005):\n",
    "                learning_rate *= 0.9\n",
    "        return             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2xy(filename):\n",
    "    f = open(\"C:\\\\Users\\\\Herry\\\\Desktop\\\\十大重要演算法\\\\Neural Net\\\\\"+filename,\"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    x = []\n",
    "    y = []\n",
    "    for line in lines:\n",
    "        splitedLine = [int(s) for s in line.split(\",\")]\n",
    "        x.append(splitedLine[1:])\n",
    "        y.append(splitedLine[0])\n",
    "    return_x = np.array(x).T\n",
    "    return_y = np.zeros((10,len(lines)),dtype = int)\n",
    "    for i in range(len(lines)):\n",
    "        return_y[y[i],i]=1\n",
    "    return return_x,return_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Train with 100 digits using leakyReLU\n",
    "\n",
    "batch_size = 25<br>\n",
    "epoch = 150<br>\n",
    "learning rate = 0.075 ~ 0.005 becomes smaller after every epoch<br>\n",
    "convolutional layer = 2<br>\n",
    "1st conv_layer --> 4 5x5 kernels<br>\n",
    "2nd conv_layer --> 8 5x5 kernels<br>\n",
    "FC_layer = 50 neurals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0  Accuracy: 4.0 %\n",
      "Batch: 1  Accuracy: 4.0 %\n",
      "Batch: 2  Accuracy: 20.0 %\n",
      "Batch: 3  Accuracy: 28.000000000000004 %\n",
      "Epoch: 0  Accuracy: 14.000000000000002 %  loss: 2.416127397646606\n",
      "Batch: 0  Accuracy: 24.0 %\n",
      "Batch: 1  Accuracy: 24.0 %\n",
      "Batch: 2  Accuracy: 20.0 %\n",
      "Batch: 3  Accuracy: 36.0 %\n",
      "Epoch: 1  Accuracy: 26.0 %  loss: 2.205621100655698\n",
      "Batch: 0  Accuracy: 28.000000000000004 %\n",
      "Batch: 1  Accuracy: 28.000000000000004 %\n",
      "Batch: 2  Accuracy: 12.0 %\n",
      "Batch: 3  Accuracy: 32.0 %\n",
      "Epoch: 2  Accuracy: 25.0 %  loss: 2.116519221671295\n",
      "Batch: 0  Accuracy: 40.0 %\n",
      "Batch: 1  Accuracy: 48.0 %\n",
      "Batch: 2  Accuracy: 40.0 %\n",
      "Batch: 3  Accuracy: 48.0 %\n",
      "Epoch: 3  Accuracy: 44.0 %  loss: 1.896380510171616\n",
      "Batch: 0  Accuracy: 68.0 %\n",
      "Batch: 1  Accuracy: 44.0 %\n",
      "Batch: 2  Accuracy: 48.0 %\n",
      "Batch: 3  Accuracy: 48.0 %\n",
      "Epoch: 4  Accuracy: 52.0 %  loss: 1.6800771939175088\n",
      "Batch: 0  Accuracy: 60.0 %\n",
      "Batch: 1  Accuracy: 48.0 %\n",
      "Batch: 2  Accuracy: 60.0 %\n",
      "Batch: 3  Accuracy: 44.0 %\n",
      "Epoch: 5  Accuracy: 53.0 %  loss: 1.46271616983787\n",
      "Batch: 0  Accuracy: 60.0 %\n",
      "Batch: 1  Accuracy: 44.0 %\n",
      "Batch: 2  Accuracy: 60.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 6  Accuracy: 59.0 %  loss: 1.4200057382765623\n",
      "Batch: 0  Accuracy: 68.0 %\n",
      "Batch: 1  Accuracy: 32.0 %\n",
      "Batch: 2  Accuracy: 72.0 %\n",
      "Batch: 3  Accuracy: 68.0 %\n",
      "Epoch: 7  Accuracy: 60.0 %  loss: 1.3032872240537938\n",
      "Batch: 0  Accuracy: 72.0 %\n",
      "Batch: 1  Accuracy: 52.0 %\n",
      "Batch: 2  Accuracy: 76.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 8  Accuracy: 68.0 %  loss: 1.1061785916031877\n",
      "Batch: 0  Accuracy: 72.0 %\n",
      "Batch: 1  Accuracy: 52.0 %\n",
      "Batch: 2  Accuracy: 80.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 9  Accuracy: 69.0 %  loss: 0.9540555046269701\n",
      "Batch: 0  Accuracy: 76.0 %\n",
      "Batch: 1  Accuracy: 56.00000000000001 %\n",
      "Batch: 2  Accuracy: 84.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 10  Accuracy: 72.0 %  loss: 0.8782389281702003\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 56.00000000000001 %\n",
      "Batch: 2  Accuracy: 84.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 11  Accuracy: 73.0 %  loss: 0.8197713270980155\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 56.00000000000001 %\n",
      "Batch: 2  Accuracy: 84.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 12  Accuracy: 73.0 %  loss: 0.7684571896520227\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 60.0 %\n",
      "Batch: 2  Accuracy: 84.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 13  Accuracy: 74.0 %  loss: 0.7270688672454857\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 60.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 14  Accuracy: 76.0 %  loss: 0.6951634659984849\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 68.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 15  Accuracy: 78.0 %  loss: 0.6746310823530632\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 16  Accuracy: 78.0 %  loss: 0.6600463908281369\n",
      "Batch: 0  Accuracy: 80.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 17  Accuracy: 79.0 %  loss: 0.6431912516472518\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 18  Accuracy: 80.0 %  loss: 0.6351198621702713\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 19  Accuracy: 80.0 %  loss: 0.6242356961997441\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 20  Accuracy: 80.0 %  loss: 0.6130457112606\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 21  Accuracy: 80.0 %  loss: 0.6058493242125558\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 88.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 22  Accuracy: 80.0 %  loss: 0.5972137489524973\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 23  Accuracy: 81.0 %  loss: 0.5902969788277599\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 24  Accuracy: 81.0 %  loss: 0.5837227694182443\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 25  Accuracy: 81.0 %  loss: 0.5788348546538117\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 26  Accuracy: 81.0 %  loss: 0.5739424490375968\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 27  Accuracy: 81.0 %  loss: 0.5704281422253561\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 28  Accuracy: 81.0 %  loss: 0.5677686752144355\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 29  Accuracy: 81.0 %  loss: 0.5642763091192061\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 30  Accuracy: 81.0 %  loss: 0.5617519665808144\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 31  Accuracy: 81.0 %  loss: 0.5589765524722172\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 32  Accuracy: 81.0 %  loss: 0.5562916006894396\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 33  Accuracy: 81.0 %  loss: 0.553884451021276\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 34  Accuracy: 81.0 %  loss: 0.5512719626071761\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 35  Accuracy: 81.0 %  loss: 0.5488626236561763\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 36  Accuracy: 81.0 %  loss: 0.5460905583103288\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 37  Accuracy: 81.0 %  loss: 0.5439238343963388\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 38  Accuracy: 81.0 %  loss: 0.5413919174425446\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 39  Accuracy: 81.0 %  loss: 0.5392872445842503\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 40  Accuracy: 81.0 %  loss: 0.5372298533824392\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 41  Accuracy: 81.0 %  loss: 0.534635930888199\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 42  Accuracy: 81.0 %  loss: 0.5323887151512605\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 43  Accuracy: 81.0 %  loss: 0.5305421490432289\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 44  Accuracy: 81.0 %  loss: 0.5284098411126221\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 45  Accuracy: 81.0 %  loss: 0.5264517980368921\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 46  Accuracy: 81.0 %  loss: 0.524516125366773\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 47  Accuracy: 81.0 %  loss: 0.5225790886820482\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 48  Accuracy: 81.0 %  loss: 0.5210507836031621\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 49  Accuracy: 81.0 %  loss: 0.5192860107090747\n",
      "Batch: 0  Accuracy: 84.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 50  Accuracy: 81.0 %  loss: 0.5173805912544455\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 51  Accuracy: 81.0 %  loss: 0.5154964304088623\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 52  Accuracy: 81.0 %  loss: 0.5143474613556597\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 53  Accuracy: 81.0 %  loss: 0.5125860149212482\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 54  Accuracy: 81.0 %  loss: 0.5107479462301361\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 55  Accuracy: 81.0 %  loss: 0.5093690518013654\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 56  Accuracy: 81.0 %  loss: 0.5078577159478888\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 57  Accuracy: 81.0 %  loss: 0.5066526170636325\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 58  Accuracy: 81.0 %  loss: 0.5050015171622102\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 59  Accuracy: 81.0 %  loss: 0.5039327234067901\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 60  Accuracy: 81.0 %  loss: 0.502072595949678\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 61  Accuracy: 81.0 %  loss: 0.5010067475724379\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 62  Accuracy: 81.0 %  loss: 0.49959603652682605\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 63  Accuracy: 81.0 %  loss: 0.4984406230983622\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 64  Accuracy: 81.0 %  loss: 0.49719626665207317\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 65  Accuracy: 81.0 %  loss: 0.49578869091688815\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 66  Accuracy: 81.0 %  loss: 0.4947929934522775\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 67  Accuracy: 81.0 %  loss: 0.49367669351062415\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 68  Accuracy: 81.0 %  loss: 0.4925604641533518\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 69  Accuracy: 81.0 %  loss: 0.4914814975497239\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 70  Accuracy: 81.0 %  loss: 0.4904300137886621\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 71  Accuracy: 81.0 %  loss: 0.48941725560704635\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 72  Accuracy: 81.0 %  loss: 0.4884899737765428\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 73  Accuracy: 81.0 %  loss: 0.48754559698292227\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 74  Accuracy: 81.0 %  loss: 0.48647730779488074\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 75  Accuracy: 81.0 %  loss: 0.4855150978193482\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 76  Accuracy: 81.0 %  loss: 0.484559844809341\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 77  Accuracy: 81.0 %  loss: 0.48378605554776744\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 78  Accuracy: 81.0 %  loss: 0.4829801495113287\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 79  Accuracy: 81.0 %  loss: 0.4819452221357561\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 80  Accuracy: 81.0 %  loss: 0.4811244637751133\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 81  Accuracy: 81.0 %  loss: 0.48026268162762814\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 82  Accuracy: 81.0 %  loss: 0.47954111461552906\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 83  Accuracy: 81.0 %  loss: 0.4786619102048293\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 84  Accuracy: 81.0 %  loss: 0.47799033842077476\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 85  Accuracy: 81.0 %  loss: 0.477171235685958\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 86  Accuracy: 81.0 %  loss: 0.47639633341267773\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 87  Accuracy: 81.0 %  loss: 0.47562113116767074\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 88  Accuracy: 82.0 %  loss: 0.47489952746113706\n",
      "Batch: 0  Accuracy: 84.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 89  Accuracy: 81.0 %  loss: 0.47445799192051474\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 90  Accuracy: 82.0 %  loss: 0.4735675476817095\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 91  Accuracy: 82.0 %  loss: 0.4730024823946125\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 92  Accuracy: 82.0 %  loss: 0.4722006370217793\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 93  Accuracy: 82.0 %  loss: 0.4716218214404644\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 94  Accuracy: 82.0 %  loss: 0.4708882212844516\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 95  Accuracy: 82.0 %  loss: 0.4703429850492285\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 96  Accuracy: 82.0 %  loss: 0.4697345013216191\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 97  Accuracy: 82.0 %  loss: 0.4690715061014436\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 98  Accuracy: 82.0 %  loss: 0.4685113188008817\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 99  Accuracy: 82.0 %  loss: 0.46808707798789767\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 100  Accuracy: 82.0 %  loss: 0.46731239240708666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 101  Accuracy: 82.0 %  loss: 0.46693366491399035\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 102  Accuracy: 83.0 %  loss: 0.46636865019571927\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 103  Accuracy: 83.0 %  loss: 0.4657680793943335\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 104  Accuracy: 83.0 %  loss: 0.4652100791372731\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 105  Accuracy: 83.0 %  loss: 0.46479057363944526\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 106  Accuracy: 83.0 %  loss: 0.4641444143802098\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 107  Accuracy: 83.0 %  loss: 0.4637622941610893\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 108  Accuracy: 83.0 %  loss: 0.46331663660131517\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 109  Accuracy: 83.0 %  loss: 0.4628032784320628\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 110  Accuracy: 83.0 %  loss: 0.4622538253030639\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 111  Accuracy: 84.0 %  loss: 0.46207595330027773\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 112  Accuracy: 84.0 %  loss: 0.46148733693932487\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 113  Accuracy: 84.0 %  loss: 0.4610480099281925\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 114  Accuracy: 84.0 %  loss: 0.4607763539072494\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 115  Accuracy: 84.0 %  loss: 0.460063890409254\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 116  Accuracy: 84.0 %  loss: 0.4598321953613649\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 117  Accuracy: 84.0 %  loss: 0.45953190694238777\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 118  Accuracy: 84.0 %  loss: 0.4589392442165523\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 119  Accuracy: 84.0 %  loss: 0.4587475422547006\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 120  Accuracy: 84.0 %  loss: 0.4582505594784814\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 121  Accuracy: 84.0 %  loss: 0.4579829156795902\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 122  Accuracy: 84.0 %  loss: 0.4575629317941985\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 123  Accuracy: 84.0 %  loss: 0.45723716058730146\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 124  Accuracy: 84.0 %  loss: 0.45693056146040867\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 125  Accuracy: 84.0 %  loss: 0.4566199000668666\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 126  Accuracy: 84.0 %  loss: 0.4562248018239867\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 127  Accuracy: 85.0 %  loss: 0.4560342367286493\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 128  Accuracy: 84.0 %  loss: 0.45543290266869707\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 129  Accuracy: 84.0 %  loss: 0.4553016340352559\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 130  Accuracy: 85.0 %  loss: 0.45505528608057044\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 131  Accuracy: 85.0 %  loss: 0.45466509716921155\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 132  Accuracy: 85.0 %  loss: 0.45421649709369916\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 133  Accuracy: 85.0 %  loss: 0.45411876047524824\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 134  Accuracy: 85.0 %  loss: 0.45380837988162737\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 135  Accuracy: 85.0 %  loss: 0.4534596636419932\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 136  Accuracy: 85.0 %  loss: 0.4533643881062247\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 137  Accuracy: 85.0 %  loss: 0.4529685508606217\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 138  Accuracy: 85.0 %  loss: 0.45278943937880856\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 139  Accuracy: 85.0 %  loss: 0.4525187430682029\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 140  Accuracy: 85.0 %  loss: 0.45222346184095635\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 84.0 %\n",
      "Epoch: 141  Accuracy: 85.0 %  loss: 0.4515789590811049\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 80.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 142  Accuracy: 87.0 %  loss: 0.4331086501717015\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 80.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 143  Accuracy: 88.0 %  loss: 0.36886678232998427\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 144  Accuracy: 89.0 %  loss: 0.36402637902274937\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 145  Accuracy: 89.0 %  loss: 0.35502375120882257\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 146  Accuracy: 89.0 %  loss: 0.35041597598155527\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 147  Accuracy: 89.0 %  loss: 0.3464108079574997\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 148  Accuracy: 89.0 %  loss: 0.34188555053162295\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 149  Accuracy: 89.0 %  loss: 0.33720938832408764\n",
      "Time cost :  1083.8848233222961\n"
     ]
    }
   ],
   "source": [
    "input_data_spec = [25, 1, 28, 28]\n",
    "\n",
    "conv_layer_spec = [{\"k_num\" : 4, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2},\n",
    "                   {\"k_num\" : 8, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2}]\n",
    "\n",
    "cnn3 = CNN(input_data_spec=input_data_spec, conv_layer_spec=conv_layer_spec)\n",
    "f = open(\"C:\\\\Users\\\\Herry\\\\Desktop\\\\十大重要演算法\\\\Neural Net\\\\minist\\\\mnist_train_100.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "\n",
    "\n",
    "train_100_x = np.clip(np.array(x), 0, 1).reshape(100, 1, 28 ,28)\n",
    "train_100_y = np.array(y)\n",
    "\n",
    "t = time.time()\n",
    "cnn3.train(train_100_x, train_100_y, 150, 0.075, 25)\n",
    "print(\"Time cost : \", time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f5bd544390>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG4JJREFUeJzt3Xt0nPV95/H3d+6ypJFsS7Lku8GGAAXHrkNMEhI3l8OlWbK5nCwkbZKme3yakpZk2dMtpZvudv9pl202F2hYcilLloWkgbIsgbI0IQvsKRfZYGMwDgZ8kS0sWbYkS9ZtZn77x/PIGouRJUsjPzPP83mdM0fP5TfS9zzSfJ6ffs/NnHOIiEi4xIIuQEREyk/hLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJIYW7iEgITRvuZrbCzJ40s91m9oqZ3VSizRYz6zOzl/zXN+anXBERmYnEDNrkgJudc9vNrB7YZmZPOOdendTuaefcx2f6g5uamtzq1avPolQREdm2bdtR51zzdO2mDXfnXCfQ6U+fMLPdwDJgcrifldWrV9Pe3j6XbyEiEjlmtn8m7c5qzN3MVgMbgOdKrL7CzHaY2WNmdskU799qZu1m1t7d3X02P1pERM7CjMPdzOqAB4CvOef6J63eDqxyzq0Hvgs8VOp7OOfucs5tcs5tam6e9r8KERGZpRmFu5kl8YL9Xufcg5PXO+f6nXMD/vSjQNLMmspaqYiIzNhMzpYx4IfAbufcN6do0+q3w8wu979vTzkLFRGRmZvJ2TLvB34XeNnMXvKX/RmwEsA5dyfwGeArZpYDhoDrnW4ULyISmJmcLfMMYNO0uR24vVxFiYjI3OgKVRGREKq6cN/z9glue/w1ek+OBl2KiEjFqrpw398zyB1PvsHBY0NBlyIiUrGqLtyXZDMAHOkfDrgSEZHKVb3hfkLhLiIylaoL96a6FGZwpH8k6FJERCpW1YV7Ih6jqS5Nl4ZlRESmVHXhDrAkm9aYu4jIGVRnuNdnNCwjInIGVRnuLdmMeu4iImdQleG+JJumZ3CU0Vwh6FJERCpSVYZ7q386ZPeAhmZEREqpynDXhUwiImdWleHekk0D6HRIEZEpVGW4T/TcNSwjIlJKVYb7ogUpEjHTsIyIyBSqMtxjMaOlPq2eu4jIFKoy3EHnuouInEnVhrtuQSAiMrUqDnf13EVEplLV4d4/nGNoNB90KSIiFaeqwx2gSw/tEBF5h6oN9+ULawB4o3sg4EpERCpP1Yb7+uWNJGLG828dD7oUEZGKU7XhXpOKc9nyBp5/qyfoUkREKk7VhjvA5WsWs7OjTwdVRUQmqfJwX0iu4HjxoIZmRESKVXW4/+aqRZjB828dC7oUEZGKUtXh3lCT5KLWrMJdRGSSqg53gMvXLGL7geN65J6ISJFQhPvwWIFXO/uDLkVEpGJUfbivaaoF4NDxoYArERGpHFUf7roNgYjIO1V9uC9ckCQZNz24Q0SkSNWHu5nRXJdWz11EpEjVhztAczZD9wn13EVExoUi3JfUp+nSsIyIyCmhCPeWbJojGpYRETll2nA3sxVm9qSZ7TazV8zsphJtzMy+Y2Z7zWynmW2cn3JLa6nP0HtyjJGcbiAmIgIz67nngJudcxcBm4EbzeziSW2uAdb5r63A98pa5TRa6tMAGncXEfFNG+7OuU7n3HZ/+gSwG1g2qdkngHuc51mg0czayl7tFCbOdVe4i4jAWY65m9lqYAPw3KRVy4CDRfMdvHMHMG+a/Z67DqqKiHhmHO5mVgc8AHzNOTf5Ri5W4i2uxPfYambtZtbe3d19dpWeQUvWD3cdVBURAWYY7maWxAv2e51zD5Zo0gGsKJpfDhye3Mg5d5dzbpNzblNzc/Ns6i1pcW2amKnnLiIybiZnyxjwQ2C3c+6bUzR7GPiCf9bMZqDPOddZxjrPKB4zmut1laqIyLjEDNq8H/hd4GUze8lf9mfASgDn3J3Ao8C1wF7gJPB75S/1zFrqMzqgKiLimzbcnXPPUHpMvbiNA24sV1Gz0VKf5nCfeu4iIhCSK1TBO6jarWEZEREgTOFen6FncJRcXo/bExEJT7hn0zgHRwdGgy5FRCRw4Qn3eu8q1SP9GpoREQlRuI9fyKQzZkREwhPuukpVROSU0IR7U10a01WqIiJAiMI9GY+xuDalYRkREUIU7gDN9Rm6dEBVRCRc4d5Sn1bPXUSEkIX7kqxuHiYiAiEL95b6DEcHRskX3nEreRGRSAlXuGfT5AuOnkENzYhItIUr3PW4PRERIGzh7j8ou1sHVUUk4sIV7vW6SlVEBEIW7s1+uB/RsIyIRFyowj2diNO4IKmeu4hEXqjCHWBJfUYHVEUk8kIX7i1ZXaUqIhK6cG+uT+tsGRGJvNCFe0t9hq4Twzinq1RFJLpCF+5LsmnG8o7jJ8eCLkVEJDChC/fxZ6nqjBkRibLwhXtWtyAQEQlfuJ+6kEk9dxGJrhCG+/iwjHruIhJdoQv3mlSc+kxCp0OKSKSFLtxh/HF7GpYRkegKabjrFgQiEm2hDPcl2TRH1HMXkQgLZbi3ZL2eu65SFZGoCme416cZyRXoH84FXYqISCBCGe7jD+3o1tCMiERUKMN9if8sVT2RSUSiKpThrmepikjUhTPc/Z67TocUkagKZbjXpRMsSMV1CwIRiaxQhjt4QzO6eZiIRNW04W5mPzKzLjPbNcX6LWbWZ2Yv+a9vlL/Ms9eSzajnLiKRNZOe+93A1dO0edo5927/9ZdzL2vuWvQsVRGJsGnD3Tn3FHDsHNRSVt79ZTQsIyLRVK4x9yvMbIeZPWZml0zVyMy2mlm7mbV3d3eX6UeX1pJNMziaZ2BEV6mKSPSUI9y3A6ucc+uB7wIPTdXQOXeXc26Tc25Tc3NzGX701Jacetyeeu8iEj1zDnfnXL9zbsCffhRImlnTnCubIz2RSUSibM7hbmatZmb+9OX+9+yZ6/edq/GrVA8dHwq4EhGRcy8xXQMzuw/YAjSZWQfwF0ASwDl3J/AZ4CtmlgOGgOtdBdxrd3VTLUsbMtz73H4+tXEZ/v5HRCQSpg1359wN06y/Hbi9bBWVSTIe4w9/ay1//tAunnr9KB+6YH7H+EVEKklor1AF+OymFSxrrOFb//RrPbhDRCIl1OGeSsS48bfW8uKBXp7ZezTockREzplQhzvAp39zGal4TOEuIpES+nBPJ+Ksbaljd+eJoEsRETlnQh/uABe1Zdnd2R90GSIi50xEwr2e7hMjupGYiERGJML94rYsgHrvIhIZkQj3ixTuIhIxkQj3hbUpWrMZhbuIREYkwh28cXedMSMiURGZcL94aZY3ugcYHssHXYqIyLyLTLhf1JYlV3Ds7RoIuhQRkXkXqXAHeFXj7iISAZEJ99WLa4nHjP09g0GXIiIy7yIT7vGYsaQ+TWefHrsnIuEXmXAHaG3I8LbCXUQiIFLh3tZQo3AXkUiIVLgvyWZ4u39YD+4QkdCLVLi3NWQ4OZqnfzgXdCkiIvMqUuHe2pAB0NCMiIRepMK9zQ/3zr6hgCsREZlfkQp39dxFJCoiFe4t9RnM0LnuIhJ6kQr3VCJGU11aPXcRCb1IhTt44+6d/Qp3EQm3yIV7azbDEfXcRSTkIhfubQ0ZnS0jIqEXuXBvbaihfzjH4IguZBKR8IpcuI+f6/62xt1FJMQiF+46111EoiBy4T5xlarCXUTCK3LhviTrh3uvDqqKSHhFLtwzyTjN9WkOHj8ZdCkiIvMmcuEOsGZxLft6FO4iEl6RDPdVixew76gelC0i4RXJcF/dVEvXiRFOjupcdxEJp0iG+6rFCwDYr6EZEQmpSIb76sW1AOzv0dCMiITTtOFuZj8ysy4z2zXFejOz75jZXjPbaWYby19mea30e+46qCoiYTWTnvvdwNVnWH8NsM5/bQW+N/ey5lc2k2RxbUo9dxEJrWnD3Tn3FHDsDE0+AdzjPM8CjWbWVq4C58uqxQt4S2fMiEhIlWPMfRlwsGi+w19W0VY31eqAqoiEVjnC3UoscyUbmm01s3Yza+/u7i7Dj5691Ytr6ewbZngsH2gdIiLzoRzh3gGsKJpfDhwu1dA5d5dzbpNzblNzc3MZfvTsjZ8OeeCYeu8iEj7lCPeHgS/4Z81sBvqcc51l+L7zavx0SF2pKiJhlJiugZndB2wBmsysA/gLIAngnLsTeBS4FtgLnAR+b76KLadT4a4zZkQkhKYNd+fcDdOsd8CNZavoHGlYkKSlPs3Lh/qDLkVEpOwieYXquA+sa+Lp17vJF0oe/xURqVqRDvctF7bQe3KMnR29QZciIlJWkQ73K9c2ETP41Z5gT8sUESm3SIf7wtoU61c08n9/rXAXkXCJdLgDfOiCZnZ09HJ8cDToUkREyiby4b7lwhacg6deV+9dRMIj8uF+6bIGFtWmeOLVI0GXIiJSNpEP93jM+PhlbTzx6hH6h8eCLkdEpCwiH+4An964nJFcgcdervi7JoiIzIjCHbhseQPnN9fywPZDQZciIlIWCnfAzPjUxuU8/9YxDuoukSISAgp337/csAwzeFC9dxEJAYW7b1ljDR9Y28RPXjhALl8IuhwRkTlRuBf5nc2rONw3zD/t7gq6FBGROVG4F/nIu1pY2pDhx8/uC7oUEZE5UbgXScRjfH7zKv7f3h72dg0EXY6IyKwp3Cf5V+9ZQSoe455/3hd0KSIis6Zwn6SpLs0nNyzj/ucPcrh3KOhyRERmReFewh99ZC0Ox3d/+XrQpYiIzIrCvYTlCxfwuctX8tP2DvYd1QO0RaT6KNyncOOH15KMG7c9vifoUkREzprCfQot9Rn+cMtafv5yJ/+4SzcUE5HqonA/g69sOZ9Llma59R920TMwEnQ5IiIzpnA/g2Q8xt98dj39w2Pc/Pc7GMnlgy5JRGRGFO7TeFdrlv9w3SX8ak83W+/ZxtCoAl5EKp/CfQY+/95V/NWnLuWp17u54fvP6gwaEal4CvcZuv7ylfzt5zbyRvcA13z7af7Hs/txzgVdlohISQr3s3DNpW088fUP8Z41i/jzh3bxb366Q8M0IlKRFO5nqbUhw91feg83f+wCHnrpEFd96ynueHIvb/cNB12aiMgpCvdZiMWMP/rIOu758uW0ZjPc9vgervzPv+TfP7SLI/0KeREJXiLoAqrZleuauXJdM/uODnLX029y3/MH+Gn7QX5n8yq+suV8murSQZcoIhFlQR0U3LRpk2tvbw/kZ8+XAz0n+c4vX+fB7R0kYjE+eEET/2L9Uq66pJVMMh50eSISAma2zTm3adp2Cvfye6N7gP/53AEefbmTzr5hGmqSfHLDMn77sjY2rlxIPGZBlygiVUrhXgEKBcezb/Zw7/MHeOKVI4zmCzTVpfjoRUv46EVLeM+aRTTUJIMuU0SqyEzDXWPu8ygWM963ton3rW3ixPAYv9rTzf959QiP7Ozk/hcOYgYXtNSzcdVCNqxs5IIl9ZzXXEs2o8AXkblRzz0AI7k82/Ydp33/cbbtP872A8c5MZw7tb6pLsV5TXWsaaplTXMtyxfW0NZQw9LGDC31GQ3riESYeu4VLJ2In+rRgzd88+bRQd7sHuDNo4O81T3Im0cH+MVrRzjaPnraexMxY0k2Q1tDhrZGL/CXNtTQ1pBhaWMNSxtrWLggiZl2ACJRpnCvALGYsbaljrUtde9Y1z88xuHeIf81TGffEJ29wxzqHWLHwV4e3zXMaL5w2nsyyRhtxYF/akcwMV2X1q9eJMz0Ca9w2UySbGuSd7VmS64vFBw9g6Mc7h2is29iB3C4d5jDfUM88/pRuk4MU5g0+lafSbDUH+pp80O/taGGxbUpFvmvxXUpFqT0JyJSjWb0yTWzq4FvA3HgB865v5q0/kvAbcAhf9HtzrkflLFOmUIsZjTXp2muT7N+RWPJNmP5Al0nRk79B9DZN0xn7xCH/B3Bjo4+jg2OlnxvJhljcW16IvDHw79ufDrN4rqJ5XXphIaERCrAtOFuZnHgDuBjQAfwgpk97Jx7dVLTnzjnvjoPNcocJeMxljXWsKyxZso2Q6N5uk4M0zM4yrGBUY4NjnrTgyP+V++1t2uAY4OjDI2VvmFaKh47red/+k4hfWp5Y02Shpok2Zok6URMOwSRMptJz/1yYK9z7k0AM7sf+AQwOdylitWk4qxaXMuqxbUzan9yNEfPwETon7YjKNo57O85ybHBUQZGclN+r1Q8RrYmSbYm4Q1DjQd/JlE0PbG+oSZJfSZBXTpBXSZBTTKunYPIJDMJ92XAwaL5DuC9Jdp92sw+CPwa+Lpz7uDkBma2FdgKsHLlyrOvVirGglSCBYsSrFi0YEbth8fyHD85Ss+AF/p9Q2P0D43RPzzmT+foH/aW9Z0c5eCxk9700Bi5yQcMJokZ1Kb9sE8nqE0nqM8kqE154V+83JuPU5tKsCCVoCYVI5OMe9PJuPdKxUnGTTsMqWozCfdSf+GTP23/G7jPOTdiZn8A/Hfgw+94k3N3AXeBd577WdYqVSyTjPtn8Ew9NFSKc46hsTz9QzlvJzA8sVMYGMkzOJJjYDjHwIj3GhyZmD7SP3xq3eBonvw0O4li8ZhRk4z7we+FfiYVZ4Ef/qetS3nTNVO0PW1dauJ9ybhuyirzZybh3gGsKJpfDhwubuCc6yma/T7w13MvTQTMzPsvIZWgtSEz6+/jnGN4rHDaTmBoLM/QaP7MX8eni+a7T4yUbHu24jEjFY+RTsZIJ2KkEjHSiXjRdIyUPz95/eRl4+3TyRipeLxoOkY66f0nkorHSMZjJBOx0+fjMf2nEkIzCfcXgHVmtgbvbJjrgc8VNzCzNudcpz97HbC7rFWKzJGZeT3uVJzm+vLfitk5x0iuwMmiwB8ey582PzSWY2i0wNCYt25oNM9ovsDIWJ6RXIHRXIGRXIGRXN7/WqB/aOzUslPrx/z35QqU8wLzZNxIxPzgT5we/Mm4tyNJxIxE3NtpJIraJ+IxkjGvXcJvP952/P2JuJGM+e/z18djVvQ1NjEff+fy09sasfGvZiTi3vK4+e3j3nRx26iZNtydczkz+yrwON6pkD9yzr1iZn8JtDvnHgb+2MyuA3LAMeBL81izSMUxMzL+UM254pxjLO9K7iBGT9tJ5BnLO8byBe+V895zaj7vGM1Nms8XGMsVyBUmpsfXjeW9HdTYsDefy3vtxvIFcnlHrjCxfCzvGCuUdyc0W8U7hMk7i7jZpB3C+M5r0o4nbjQuSPEnV1044+NNQdG9ZURk3uXHw78wEfr5grcj8L7681MtLzjyBW/nUXATy4vbTH7lCn7bvPfe/Pj78o68K9H2tJ/3zrpyBa/+vUcGSCdjfP8Lm9iwcuE535a6t4yIVAyvpxyOB9bs7Rrgy3e/wPV3PctjN13Jec3vvG1IJdDhehGRs7C2pY77t25mLF/goRcPTf+GgCjcRUTO0tLGGt67ZjGPvNxJUEPb01G4i4jMwrWXtfFm9yB7jpwIupSSFO4iIrNw9SWtxAx+vrNz+sYBULiLiMxCc32azect5ucVOjSjcBcRmaVrL/WGZr5634v8+J/3cXRgJOiSTtGpkCIis/TJDct48UAvz+zt5uc7O/lPj+zm2ktb+eAFzVy2vJHzmmoDuzpW4S4iMku16QR/89n1OOfY2zXAvc8d4IHtHTz0knf7rfp0gkuXN3DZ8kbWL2/gshWNLG3InJP7+OgKVRGRMsoXvKDf0dHLjoO97Ozo47W3+xnLe1nbVJfiDz50Pv/6yvNm9f11haqISADiMePC1noubK3ns5u8G+qO5PLs7jzBzo5edhzsm5eb102mcBcRmWfpRJx3r2jk3Ssa4Ypz8zN1toyISAgp3EVEQkjhLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJocBuP2Bm3cD+Wb69CThaxnLmg2osD9VYHqpx7iqlvlXOuebpGgUW7nNhZu0zubdCkFRjeajG8lCNc1fp9U2mYRkRkRBSuIuIhFC1hvtdQRcwA6qxPFRjeajGuav0+k5TlWPuIiJyZtXacxcRkTOounA3s6vNbI+Z7TWzPw26HgAzW2FmT5rZbjN7xcxu8pcvMrMnzOx1/+vCgOuMm9mLZvaIP7/GzJ7z6/uJmaUCrq/RzH5mZq/52/KKCtyGX/d/x7vM7D4zywS9Hc3sR2bWZWa7ipaV3G7m+Y7/+dlpZhsDrPE2/3e908z+wcwai9bd4te4x8yuCqrGonX/1sycmTX584Fsx7NRVeFuZnHgDuAa4GLgBjO7ONiqAMgBNzvnLgI2Azf6df0p8Avn3DrgF/58kG4CdhfN/zXwX/36jgO/H0hVE74N/KNz7l3AerxaK2Ybmtky4I+BTc653wDiwPUEvx3vBq6etGyq7XYNsM5/bQW+F2CNTwC/4Zy7DPg1cAuA/9m5HrjEf8/f+p/9IGrEzFYAHwMOFC0OajvOnHOual54zzB5vGj+FuCWoOsqUef/wvtj2AO0+cvagD0B1rQc70P+YeARwPAuyEiU2rYB1JcF3sI/DlS0vJK24TLgILAI7ylmjwBXVcJ2BFYDu6bbbsB/A24o1e5c1zhp3SeBe/3p0z7XwOPAFUHVCPwMr7OxD2gKejvO9FVVPXcmPlzjOvxlFcPMVgMbgOeAJc65TgD/a0twlfEt4E+Agj+/GOh1zuX8+aC35XlAN/B3/tDRD8yslgrahs65Q8B/wevBdQJ9wDYqazuOm2q7Vepn6MvAY/50xdRoZtcBh5xzOyatqpgap1Jt4W4lllXM6T5mVgc8AHzNOdcfdD3jzOzjQJdzblvx4hJNg9yWCWAj8D3n3AZgkOCHsU7jj1t/AlgDLAVq8f49n6xi/iZLqLTfO2Z2K97Q5r3ji0o0O+c1mtkC4FbgG6VWl1hWUb/3agv3DmBF0fxy4HBAtZzGzJJ4wX6vc+5Bf/ERM2vz17cBXQGV937gOjPbB9yPNzTzLaDRzMYfkh70tuwAOpxzz/nzP8ML+0rZhgAfBd5yznU758aAB4H3UVnbcdxU262iPkNm9kXg48DnnT++QeXUeD7ejnyH/9lZDmw3s1Yqp8YpVVu4vwCs889OSOEddHk44JowMwN+COx2zn2zaNXDwBf96S/ijcWfc865W5xzy51zq/G22S+dc58HngQ+E3R9AM65t4GDZnahv+gjwKtUyDb0HQA2m9kC/3c+XmPFbMciU223h4Ev+Gd7bAb6xodvzjUzuxr4d8B1zrmTRaseBq43s7SZrcE7aPn8ua7POfeyc67FObfa/+x0ABv9v9WK2Y5TCnrQfxYHPK7FO7L+BnBr0PX4NX0A71+yncBL/utavHHtXwCv+18XVUCtW4BH/Onz8D40e4G/B9IB1/ZuoN3fjg8BCyttGwL/EXgN2AX8GEgHvR2B+/COAYzhBdDvT7Xd8IYT7vA/Py/jnfkTVI178catxz8zdxa1v9WvcQ9wTVA1Tlq/j4kDqoFsx7N56QpVEZEQqrZhGRERmQGFu4hICCncRURCSOEuIhJCCncRkRBSuIuIhJDCXUQkhBTuIiIh9P8BaWs7iyVgPUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5bd52eb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn3.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output seems fine.\n",
    "But it only hit 90%.\n",
    "Maybe it is about the size of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.76\n"
     ]
    }
   ],
   "source": [
    "f = open(\"C:\\\\Users\\\\Herry\\\\Desktop\\\\十大重要演算法\\\\Neural Net\\\\minist\\\\mnist_test_10000.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "\n",
    "\n",
    "test_10000_x = np.clip(np.array(x), 0, 1).reshape(10000, 1, 28 ,28)\n",
    "test_10000_y = np.array(y)\n",
    "test_25_x=test_10000_x[:25]\n",
    "test_25_y=test_10000_y[:25]\n",
    "cnn3.forward(test_25_x)\n",
    "predicted = cnn3.fc_net.yhat\n",
    "accuracy = 1.0 - float(len(np.nonzero(predicted - test_25_y)[0])) / len(test_25_y)\n",
    "print (\"Training Accuracy=\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to compare the different between two activiation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Train with 100 digits using ReLU\n",
    "\n",
    "batch_size = 25<br>\n",
    "epoch = 150<br>\n",
    "learning rate = 0.075 ~ 0.005 becomes smaller after every epoch<br>\n",
    "convolutional layer = 2<br>\n",
    "1st conv_layer --> 4 5x5 kernels<br>\n",
    "2nd conv_layer --> 8 5x5 kernels<br>\n",
    "FC_layer = 50 neurals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0  Accuracy: 24.0 %\n",
      "Batch: 1  Accuracy: 4.0 %\n",
      "Batch: 2  Accuracy: 16.0 %\n",
      "Batch: 3  Accuracy: 28.000000000000004 %\n",
      "Epoch: 0  Accuracy: 18.0 %  loss: 2.2966312493701473\n",
      "Batch: 0  Accuracy: 20.0 %\n",
      "Batch: 1  Accuracy: 4.0 %\n",
      "Batch: 2  Accuracy: 36.0 %\n",
      "Batch: 3  Accuracy: 24.0 %\n",
      "Epoch: 1  Accuracy: 21.0 %  loss: 2.2100480960011817\n",
      "Batch: 0  Accuracy: 32.0 %\n",
      "Batch: 1  Accuracy: 12.0 %\n",
      "Batch: 2  Accuracy: 36.0 %\n",
      "Batch: 3  Accuracy: 32.0 %\n",
      "Epoch: 2  Accuracy: 28.000000000000004 %  loss: 2.1271821346873665\n",
      "Batch: 0  Accuracy: 36.0 %\n",
      "Batch: 1  Accuracy: 8.0 %\n",
      "Batch: 2  Accuracy: 44.0 %\n",
      "Batch: 3  Accuracy: 36.0 %\n",
      "Epoch: 3  Accuracy: 31.0 %  loss: 2.063640254481164\n",
      "Batch: 0  Accuracy: 32.0 %\n",
      "Batch: 1  Accuracy: 12.0 %\n",
      "Batch: 2  Accuracy: 48.0 %\n",
      "Batch: 3  Accuracy: 44.0 %\n",
      "Epoch: 4  Accuracy: 34.0 %  loss: 2.000839292635137\n",
      "Batch: 0  Accuracy: 44.0 %\n",
      "Batch: 1  Accuracy: 24.0 %\n",
      "Batch: 2  Accuracy: 60.0 %\n",
      "Batch: 3  Accuracy: 52.0 %\n",
      "Epoch: 5  Accuracy: 45.0 %  loss: 1.8698511156570663\n",
      "Batch: 0  Accuracy: 44.0 %\n",
      "Batch: 1  Accuracy: 36.0 %\n",
      "Batch: 2  Accuracy: 56.00000000000001 %\n",
      "Batch: 3  Accuracy: 52.0 %\n",
      "Epoch: 6  Accuracy: 47.0 %  loss: 1.792258002202649\n",
      "Batch: 0  Accuracy: 44.0 %\n",
      "Batch: 1  Accuracy: 32.0 %\n",
      "Batch: 2  Accuracy: 60.0 %\n",
      "Batch: 3  Accuracy: 56.00000000000001 %\n",
      "Epoch: 7  Accuracy: 48.0 %  loss: 1.6582224931563398\n",
      "Batch: 0  Accuracy: 56.00000000000001 %\n",
      "Batch: 1  Accuracy: 40.0 %\n",
      "Batch: 2  Accuracy: 60.0 %\n",
      "Batch: 3  Accuracy: 64.0 %\n",
      "Epoch: 8  Accuracy: 55.00000000000001 %  loss: 1.5309352657388968\n",
      "Batch: 0  Accuracy: 64.0 %\n",
      "Batch: 1  Accuracy: 52.0 %\n",
      "Batch: 2  Accuracy: 60.0 %\n",
      "Batch: 3  Accuracy: 56.00000000000001 %\n",
      "Epoch: 9  Accuracy: 57.99999999999999 %  loss: 1.4381793278462793\n",
      "Batch: 0  Accuracy: 60.0 %\n",
      "Batch: 1  Accuracy: 56.00000000000001 %\n",
      "Batch: 2  Accuracy: 76.0 %\n",
      "Batch: 3  Accuracy: 68.0 %\n",
      "Epoch: 10  Accuracy: 65.0 %  loss: 1.2729288698959178\n",
      "Batch: 0  Accuracy: 56.00000000000001 %\n",
      "Batch: 1  Accuracy: 52.0 %\n",
      "Batch: 2  Accuracy: 72.0 %\n",
      "Batch: 3  Accuracy: 72.0 %\n",
      "Epoch: 11  Accuracy: 63.0 %  loss: 1.173769246552784\n",
      "Batch: 0  Accuracy: 64.0 %\n",
      "Batch: 1  Accuracy: 56.00000000000001 %\n",
      "Batch: 2  Accuracy: 80.0 %\n",
      "Batch: 3  Accuracy: 76.0 %\n",
      "Epoch: 12  Accuracy: 69.0 %  loss: 1.0266860619949536\n",
      "Batch: 0  Accuracy: 68.0 %\n",
      "Batch: 1  Accuracy: 64.0 %\n",
      "Batch: 2  Accuracy: 80.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 13  Accuracy: 73.0 %  loss: 0.9563652304899453\n",
      "Batch: 0  Accuracy: 68.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 14  Accuracy: 78.0 %  loss: 0.8644245969173853\n",
      "Batch: 0  Accuracy: 72.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 15  Accuracy: 79.0 %  loss: 0.7858711155504835\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 80.0 %\n",
      "Epoch: 16  Accuracy: 83.0 %  loss: 0.7168397424754718\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 17  Accuracy: 85.0 %  loss: 0.667667233833858\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 72.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 18  Accuracy: 85.0 %  loss: 0.6323034933518461\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 19  Accuracy: 86.0 %  loss: 0.5982740428845927\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 20  Accuracy: 86.0 %  loss: 0.5720569376868982\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 76.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 21  Accuracy: 86.0 %  loss: 0.5510579170691767\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 80.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 22  Accuracy: 87.0 %  loss: 0.5324109088137192\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 84.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 23  Accuracy: 88.0 %  loss: 0.517455949407991\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 92.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 24  Accuracy: 89.0 %  loss: 0.5046395414606272\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 88.0 %\n",
      "Epoch: 25  Accuracy: 90.0 %  loss: 0.49343152799350687\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 26  Accuracy: 91.0 %  loss: 0.48349041956219396\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 27  Accuracy: 91.0 %  loss: 0.4762800250064741\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 28  Accuracy: 91.0 %  loss: 0.46871733232381274\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 29  Accuracy: 91.0 %  loss: 0.4622223977053548\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 30  Accuracy: 91.0 %  loss: 0.45499405409665933\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 31  Accuracy: 91.0 %  loss: 0.4486062161321556\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 32  Accuracy: 91.0 %  loss: 0.4420676977501\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 33  Accuracy: 91.0 %  loss: 0.4356750213605448\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 34  Accuracy: 91.0 %  loss: 0.42910634784780693\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 35  Accuracy: 91.0 %  loss: 0.42366976458831584\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 36  Accuracy: 91.0 %  loss: 0.41771538898164784\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 37  Accuracy: 91.0 %  loss: 0.4123448832253906\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 38  Accuracy: 91.0 %  loss: 0.40615927382467787\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 39  Accuracy: 91.0 %  loss: 0.4011953159116991\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 40  Accuracy: 91.0 %  loss: 0.395727714020069\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 41  Accuracy: 91.0 %  loss: 0.3910789341021607\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 88.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 42  Accuracy: 91.0 %  loss: 0.3858869432464649\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 43  Accuracy: 92.0 %  loss: 0.3812303175149016\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 44  Accuracy: 92.0 %  loss: 0.376381349190001\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 45  Accuracy: 92.0 %  loss: 0.3717103231745516\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 46  Accuracy: 92.0 %  loss: 0.36735667130628086\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 47  Accuracy: 92.0 %  loss: 0.36256778077519336\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 48  Accuracy: 92.0 %  loss: 0.3587073512809474\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 49  Accuracy: 92.0 %  loss: 0.35410804226462367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 50  Accuracy: 92.0 %  loss: 0.3503579705248761\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 51  Accuracy: 92.0 %  loss: 0.34596170438570517\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 52  Accuracy: 92.0 %  loss: 0.3423851640309579\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 53  Accuracy: 92.0 %  loss: 0.33805467295280206\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 54  Accuracy: 92.0 %  loss: 0.33439410927064117\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 55  Accuracy: 92.0 %  loss: 0.3304312208199298\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 56  Accuracy: 92.0 %  loss: 0.3268690739071179\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 57  Accuracy: 92.0 %  loss: 0.32317262370301203\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 58  Accuracy: 92.0 %  loss: 0.3196794001442095\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 59  Accuracy: 92.0 %  loss: 0.3161768401335847\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 60  Accuracy: 92.0 %  loss: 0.3126101633345999\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 61  Accuracy: 92.0 %  loss: 0.3096537286103873\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 62  Accuracy: 92.0 %  loss: 0.30608093964347244\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 63  Accuracy: 92.0 %  loss: 0.3027687711016813\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 64  Accuracy: 92.0 %  loss: 0.2999535338677541\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 65  Accuracy: 92.0 %  loss: 0.29666826562515086\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 66  Accuracy: 92.0 %  loss: 0.2936868467383337\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 67  Accuracy: 92.0 %  loss: 0.29073397873059453\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 68  Accuracy: 92.0 %  loss: 0.2878294669506556\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 96.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 69  Accuracy: 92.0 %  loss: 0.2851570739584926\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 70  Accuracy: 93.0 %  loss: 0.28222186652829395\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 71  Accuracy: 93.0 %  loss: 0.27959040842159677\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 72  Accuracy: 93.0 %  loss: 0.2770580226879162\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 73  Accuracy: 93.0 %  loss: 0.2743018516526539\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 74  Accuracy: 93.0 %  loss: 0.27196818956553886\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 75  Accuracy: 93.0 %  loss: 0.26938656393846994\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 76  Accuracy: 93.0 %  loss: 0.26708555363667796\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 77  Accuracy: 93.0 %  loss: 0.26463406855358224\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 78  Accuracy: 93.0 %  loss: 0.2623163034343027\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 79  Accuracy: 93.0 %  loss: 0.2600884364599684\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 80  Accuracy: 93.0 %  loss: 0.2579358942940784\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 81  Accuracy: 93.0 %  loss: 0.25548774549967884\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 82  Accuracy: 93.0 %  loss: 0.2537413301592065\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 83  Accuracy: 93.0 %  loss: 0.25145469637222023\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 84  Accuracy: 93.0 %  loss: 0.2494722974569399\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 85  Accuracy: 93.0 %  loss: 0.24771758493118973\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 86  Accuracy: 93.0 %  loss: 0.2459927101275313\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 87  Accuracy: 93.0 %  loss: 0.2441052615744736\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 88  Accuracy: 93.0 %  loss: 0.24244832915328862\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 89  Accuracy: 93.0 %  loss: 0.24084208381563538\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 90  Accuracy: 93.0 %  loss: 0.23906464286683404\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 91  Accuracy: 93.0 %  loss: 0.23748461523554235\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 92  Accuracy: 93.0 %  loss: 0.2360495632150472\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 93  Accuracy: 93.0 %  loss: 0.23428365892897834\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 94  Accuracy: 93.0 %  loss: 0.23275281484951188\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 95  Accuracy: 93.0 %  loss: 0.2312683136252985\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 96  Accuracy: 93.0 %  loss: 0.23016370014702953\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 97  Accuracy: 93.0 %  loss: 0.22864694738430183\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 98  Accuracy: 93.0 %  loss: 0.22734335167500505\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 99  Accuracy: 93.0 %  loss: 0.22597639870336272\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 100  Accuracy: 93.0 %  loss: 0.22464779105267538\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 101  Accuracy: 93.0 %  loss: 0.22351880646930036\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 102  Accuracy: 93.0 %  loss: 0.22230551932158105\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 103  Accuracy: 93.0 %  loss: 0.22102683377632804\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 104  Accuracy: 93.0 %  loss: 0.21971649176306368\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 105  Accuracy: 93.0 %  loss: 0.21878795559212716\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 106  Accuracy: 93.0 %  loss: 0.21742611000828158\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 107  Accuracy: 93.0 %  loss: 0.21642540366768254\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 108  Accuracy: 93.0 %  loss: 0.21528706379493406\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 109  Accuracy: 93.0 %  loss: 0.21459777391284163\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 110  Accuracy: 93.0 %  loss: 0.21341204667847719\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 111  Accuracy: 93.0 %  loss: 0.21209440158428522\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 112  Accuracy: 93.0 %  loss: 0.2115525470304258\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 113  Accuracy: 93.0 %  loss: 0.2107351875956741\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 114  Accuracy: 93.0 %  loss: 0.20951509715117955\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 115  Accuracy: 93.0 %  loss: 0.20887480703137243\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 116  Accuracy: 93.0 %  loss: 0.20781316681455575\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 117  Accuracy: 93.0 %  loss: 0.2069115949271252\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 118  Accuracy: 93.0 %  loss: 0.20641967029638908\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 119  Accuracy: 93.0 %  loss: 0.2054826296016958\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 120  Accuracy: 93.0 %  loss: 0.20478406551331507\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 121  Accuracy: 93.0 %  loss: 0.20413189128405163\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 122  Accuracy: 93.0 %  loss: 0.20315746873071563\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 123  Accuracy: 93.0 %  loss: 0.20239354306224253\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 124  Accuracy: 93.0 %  loss: 0.20193476340337174\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 125  Accuracy: 93.0 %  loss: 0.20141171665706964\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 126  Accuracy: 93.0 %  loss: 0.20038132371644507\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 127  Accuracy: 93.0 %  loss: 0.19971024465100634\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 128  Accuracy: 93.0 %  loss: 0.19934463977321837\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 129  Accuracy: 93.0 %  loss: 0.1991413962612219\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 130  Accuracy: 93.0 %  loss: 0.19816930509553524\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 131  Accuracy: 93.0 %  loss: 0.19735417579972275\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 132  Accuracy: 93.0 %  loss: 0.1966958418881249\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 133  Accuracy: 93.0 %  loss: 0.1963257698994894\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 134  Accuracy: 93.0 %  loss: 0.19601844588814005\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 135  Accuracy: 93.0 %  loss: 0.19546012606214389\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 136  Accuracy: 93.0 %  loss: 0.19458162926636993\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 137  Accuracy: 93.0 %  loss: 0.1942070863129949\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 138  Accuracy: 93.0 %  loss: 0.19376044241965257\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 139  Accuracy: 93.0 %  loss: 0.19318580112214084\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 140  Accuracy: 93.0 %  loss: 0.19254857084318175\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 141  Accuracy: 93.0 %  loss: 0.19231834122922895\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 142  Accuracy: 93.0 %  loss: 0.19187362581719022\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 143  Accuracy: 93.0 %  loss: 0.19142432360182263\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 144  Accuracy: 93.0 %  loss: 0.1909478580772995\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 145  Accuracy: 93.0 %  loss: 0.19037561775098708\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 146  Accuracy: 93.0 %  loss: 0.19011794031757978\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 147  Accuracy: 93.0 %  loss: 0.18964053822990995\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 148  Accuracy: 93.0 %  loss: 0.1891478322892593\n",
      "Batch: 0  Accuracy: 88.0 %\n",
      "Batch: 1  Accuracy: 92.0 %\n",
      "Batch: 2  Accuracy: 100.0 %\n",
      "Batch: 3  Accuracy: 92.0 %\n",
      "Epoch: 149  Accuracy: 93.0 %  loss: 0.18908925505603952\n",
      "Time cost :  1063.8767726421356\n"
     ]
    }
   ],
   "source": [
    "input_data_spec = [25, 1, 28, 28]\n",
    "\n",
    "conv_layer_spec = [{\"k_num\" : 4, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2},\n",
    "                   {\"k_num\" : 8, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2}]\n",
    "\n",
    "cnn = CNN(input_data_spec=input_data_spec, conv_layer_spec=conv_layer_spec)\n",
    "f = open(\"C:\\\\Users\\\\Herry\\\\Desktop\\\\十大重要演算法\\\\Neural Net\\\\minist\\\\mnist_train_100.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "\n",
    "\n",
    "train_100_x = np.clip(np.array(x), 0, 1).reshape(100, 1, 28 ,28)\n",
    "train_100_y = np.array(y)\n",
    "\n",
    "t = time.time()\n",
    "cnn.train(train_100_x, train_100_y, 150, 0.075, 25)\n",
    "print(\"Time cost : \", time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f5bd66ab00>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl83FW9//HXZyZ7k65J0yVdaelGoUBlsYisUgrWC4iCCor6qwt6Rb145XpF5He94tXrij8QENwQUHYRrCIIFFm6l0IX0j3dkrRp0uwzk/P7Y77fkE6TzLRNZsv7+XjkMZnvnEzOaSfvOXO+53uOOecQEZHsEkh1BUREpO8p3EVEspDCXUQkCyncRUSykMJdRCQLKdxFRLKQwl1EJAsp3EVEspDCXUQkC+XEK2Bm44DfAKOADuAu59xPYsqcAzwBbPEOPeqcu7W35y0tLXUTJ048iiqLiAxcy5cvr3XOlcUrFzfcgTDwVefcCjMrAZab2d+cc2/FlHvJOXdpohWcOHEiy5YtS7S4iIgAZrYtkXJxh2Wcc7udcyu87w8C64Cxx1Y9ERHpT0c05m5mE4GTgde6efhMM1ttZs+Y2aw+qJuIiBylRIZlADCzYuAR4AbnXEPMwyuACc65RjNbADwOTO3mORYBiwDGjx9/1JUWEZHeJdRzN7NcosF+v3Pu0djHnXMNzrlG7/ungVwzK+2m3F3OubnOubllZXHPB4iIyFGKG+5mZsAvgXXOuR/2UGaUVw4zO8173n19WVEREUlcIsMy84BrgDfMbJV37D+A8QDOuTuBDwKfM7Mw0AJc5bQLiIhIysQNd+fcEsDilLkduL2vKiUiIscmI69QXbp1Pxv2HEx1NURE0lbCs2XSyZV3vgLA1tsuSXFNRETSU0b23EVEpHcKdxGRLKRwFxHJQgp3EZEslHHhHop0pLoKIiJpL+PCvTUU6fz+Kw+t4jt/jl15WEREMjDc3+m5P7pyJ3e/tKXz/t6GVnYdaElFtURE0krGzXPv2nOPdfp//x3Q/HcRkQzsufcc7iIiEpWBPffDT6i2hRX4IiJdZV64dxPkJ97yV/JzMu5DiIhIv8m8cO9mWKYt3EFbWFMkRUR8GdfdbWmPPwTzmd8u4/0/W5KE2oiIpKfM67kn0ENf/ObeJNRERCR9ZVzPfcEJo1j9rfeluhoiImkt48I9JxhgSGEuk8sGpboqIiJpK+PC3TduWFGqqyAikrYyNtw/c/bkVFdBRCRtZWy4v3tKKTdfOjPV1RARSUsZG+4AeXEuXPrB4g0s+s2yJNVGRCR9ZNxUyK7iXZV6+/OVSaqJiEh6yeqeu4jIQJXR6RiKuFRXQUQkLWV0uIe15Z6ISLcyOtyvOLWC6+ZNTHU1RETSTkaHe24wwLfeP4vvXTE71VUREUkrGR3uvtxgVjRDRKTPZEUqKtxFRA6VFamoKZEiIofKilTMi9NzX7m9jidX70pSbUREUi+jr1D1OXqf737Z//snAAtPGpOM6oiIpFzcnruZjTOz581snZm9aWZf6qaMmdlPzazSzNaY2Sn9U93unTF5BJ8/5zhuvGhaMn+tiEjaSmRYJgx81Tk3AzgDuN7MYpdjvBiY6n0tAu7o01rGUZSXw9fmT2doUW4yf62ISNqKG+7Oud3OuRXe9weBdcDYmGIfAH7jol4FhprZ6D6vbRw5AUv2rxQRSUtHdELVzCYCJwOvxTw0FtjR5X4Vh78BYGaLzGyZmS2rqak5spomIGAKdxEROIJwN7Ni4BHgBudcQ+zD3fzIYWc5nXN3OefmOufmlpWVHVlNExBUz11EBEgw3M0sl2iw3++ce7SbIlXAuC73K4Ckzz2MF+7Lt+3n/te2Jak2IiKpk8hsGQN+Caxzzv2wh2JPAtd6s2bOAOqdc7v7sJ4JiRfuV9zxCt94bG2SaiMikjqJzHOfB1wDvGFmq7xj/wGMB3DO3Qk8DSwAKoFm4Lq+r2p8QY25i4gACYS7c24J3Y+pdy3jgOv7qlJHS2PuIiJRWbH8gE/hLiISlVXhHunQtnsiIpBl4d4Wjm67V5CbVc0SETliWZWC7V64F+X1fiphS20Tf31zTzKqJCKSElmxKqSv3dswuzA32Gu5c3/wDwC23nZJf1dJRCQlsqrn3haKAFCU13u4i4hku6wK9/NnlAPw0dPHp7gmIiKplVXhPm54EVtvu4TZFUNSXRURkZTKqnD3ma5UFZEBLivDfV9je6qrICKSUlkZ7vOmjODiE0Zx86WxG0YdKrpqgohI9snKcC/Ky+GOj53KuOFFvZZTtotItsrKcPcF47QuonQXkSyV1eEeb9s9rUUjItkqq8M93iqR6riLSLbK7nCP13NXuotIlsrqcA/E6bl3KNxFJEtldbjHG5bp0Ji7iGSprA73eCdUle0ikq2yOtzj9dw1W0ZEslV2h3ucnvv2/U38Ze3uJNVGRCR5smqzjljhjo5eH7/ijlcAbdohItknq3vuraHew11EJFtld7iHozszDYqzM5MWEBORbJPV4T6lrBiAy0+p6LWczquKSLbJ6nAfN7yIt79zMR88tfdw16wZEck2WR3uALnBgKZEisiAk/XhDgmsDqkxdxHJMgMi3OOtIROJKNxFJLsMiHCPN+yinruIZJuBEe5xwjvexU4iIplmYIR7nJ67sl1Esk3ccDeze82s2szW9vD4OWZWb2arvK+b+76ax2bqyOh898+8d3K3j2tYRkSyTSI9918B8+OUeck5N8f7uvXYq9W3hhblsfW2S3jfzFHdPu6fUF23u4H/euotXbEqIhkvbrg7514E9iehLv2up/nufs/9Q794hXuWbKGhNZzMaomI9Lm+GnM/08xWm9kzZjarj56zz/V0LVPEG3T3x+bjXfQkIpLu+mLJ3xXABOdco5ktAB4HpnZX0MwWAYsAxo8f3we/+sj0dDHTk6t28VJlLWEv3LW3qohkumPuuTvnGpxzjd73TwO5ZlbaQ9m7nHNznXNzy8rKjvVXH7GeeuQ/fa6SldsPEI5Ee/BOs2dEJMMdc7ib2SizaJfYzE7znnPfsT5vf0h0T1XNnhGRTBd3WMbMHgDOAUrNrAr4FpAL4Jy7E/gg8DkzCwMtwFUuTaebBBN8K9OwjIhkurjh7py7Os7jtwO391mN+lG8nruvQ6tEikiGGxBXqPoSDndlu4hkuAEV7olOcdSwjIhkugEV7olmtjbvEJFMN6DCPZTgCmGvbt7Hh+58hVBEcyJFJDP1xUVMGcPvkeflBGgP9xzcNz68BoA99a2MG16UlLqJiPSlAdVzn1JWzCfnTeKua05NqLyWIRCRTDWgwj0QMG5+/0wmlxYnVN4Mmtu1iJiIZJ4BFe6+QIKt/sULm5l582JqG9v6t0IiIn1sYIZ7gvPdH1u5E4DqBoW7iGSWARnuiY6l+wuJaexdRDLNgAz3BDvuhDrXd+/HyoiI9IMBGVvBBNPd77knOowjIpIuBmS4J8q/UFXhLiKZZkCGe9eLmRKhtWZEJNMMyHAvK8nnc+ccx+8/fXpC5bXUjIhkmgG1/IDPzPj3+dOpbw4lVD5N9x4REenRgOy5+4JBre8uItlpQId7oqdJNeYuIplmQIe7f0L1PVNLey2ncBeRTDMgx9x9ucEAW2+7hDVVB3jp7doeyyW4DLyISNoY0D13X7zlBVZXHWDBT17SCpEikjEU7sQP9//681u8tbuB1Tvqk1QjEZFjo3An/nIEoYi/xoyuVBWRzKBwJ35oRzoU7iKSWRTuJB7aOQp3EckQCncSXxhMPXcRyRQKdxIPbS0OKSKZQuFO4sMtzmnDbBHJDAp3IJBguP9h2Q5m3ryYTTWN/VwjEZFjo3An8Z2Znn5jNwCV1Qp3EUlvCncS77m3haPrEOQmuJqkiEiqKNxJfMy93Qv3nID+2UQkvcVNKTO718yqzWxtD4+bmf3UzCrNbI2ZndL31exfic6Wafc2zM5Rz11E0lwiXdBfAfN7efxiYKr3tQi449irlVyJznP3V/5Vz11E0l3clHLOvQjs76XIB4DfuKhXgaFmNrqvKpgMR3rlaVDZLiJpri9iaiywo8v9Ku9YxvBPqE4rL0mofIeDqrrm/qySiMgx6Ytw767b2+3WRWa2yMyWmdmympqaPvjVfWfxDWfz60+ellDZB1/fwVnfe57l2+r6uVYiIkenL8K9ChjX5X4FsKu7gs65u5xzc51zc8vKyvrgV/edaaNKGFyY2MZUr2yK7tq0pbapP6skInLU+iLcnwSu9WbNnAHUO+d298HzJl2iJ1Y1311E0l3crqqZPQCcA5SaWRXwLSAXwDl3J/A0sACoBJqB6/qrsv3tSMNds2ZEJF3FDXfn3NVxHnfA9X1WoxRKdNJMWzgCROe7v7mrnlljhvRjrUREjpy6nl0k2nP3t917YtVOLvnpEp5bv7c/qyUicsQU7l0kusaMb/2egwBs26dpkSKSXhTuxyDs9eBzdFWTiKQZpdIx8MfecwPGdvXeRSSNKNyPgT9r5sW3azj7+8/zl7V7UlwjEZEohXs33n/SmITKtYWi4b5u90HvtqHf6iQiciQSuyRzANny3QW0hTv40+puL7I9hL8EsE8XNYlIulDPPYaZJby+e6QjekLVeWsBmxm/XLKF1lCk3+onIpII9dy7keh8d5+/Q9OfVu9i/Z6D1De385X3TeuPqomIJEQ9924c4XR3Wr1wb2wLA9Dcrp67iKSWwr0bdoQ997aYYZhg0Ljnpc00eWEvIpJsGpbpA37P3d+G7/n11Wzc20hVXQu3LJyVwpqJyEClnnsf8E+sdnjp7g/LtLRHqG8JdZ5wFRFJFoV7H/JnyfhZvqu+hZO+/Vfuf217CmslIgORwr0PtYYOnffu79T0wsYa2sMd6sGLSNIo3PtQS8yJVT/LG1pCHP+fz/C7V7eloFYiMhAp3PtB7JWrVXUtADy1Zjdrd9brIicR6XcK916cN33kUf1cq3dCtSNmGOZAc4hLf7aEbz3x5jHXTUSkNwr3Hqz/v/O5+9q5R/WzLTEnVn0HW0MAvLGznn9/eA1rd9YfUx1FRHqicO9BQW4w4TVmYoW9qZGhmOEZP+vrW0I8tGwHn/71Mha/uYfqhtZjqaqIyGEU7v0odhkCf5jGvw0YfOa3y/n4fUtZU3WAA83tSa+jiGQnhXs/8odn3lk98tDHI96B3fUtLLz9ZT5+7+s8v6GaHfu1q5OIHBstP5AEze2HrjHjh31s6L+xs57r7lvKsKJcvnPZbKaMLOb48pKk1lVEsoPCPQmaYmbP+KHuj82HvbH5nECA9kgHdc0hPn//CgBuef9Mji8v4d1TSpNdbRHJYAr3JPDDvKktGvJ+qEciMSHf0XHYz97yp7cAWHT2ZI4vL+GDp1b0e31FJPMp3JPIH4P3Z9GEvDAPdy481vPP3vXiZgAeX7mTKSOLuWnBdAJm5AZ12kREDqdwj+N9M8s587gRfNvrQfcFfxZN7Nh7IpZU1rKkspZfv7KVimGFPLjoTAwYM7Swz+onIpnPUrWY1dy5c92yZctS8ruPxsSv/znVVejVHz97Ju3hDuZpbF4kq5nZcudc3Css1XPPElfe+QoAXzh3Cvua2vn2wll0OEdBbjDFNRORVFC4Z5nbn68EYNnW/bxd3cjfvnw2u+pbee/xZSmumYgkk8I9S71d3QjAhT96EYAfXHkSK7bXcevCWbRHOijK03+9SDbTX/gRmDl6MG/tbkh1NY7Kv/1xNQDVDa08u66aF248h1U7DvCBOWNTXDMR6Q8JzaMzs/lmtsHMKs3s6908/gkzqzGzVd7Xp/u+qqm17tb5PH79vFRX45g9u64agPP/9wW+9OAq/rB0B+f97z9obg+zpupAimsnIn0lbribWRD4OXAxMBO42sxmdlP0IefcHO/rnj6uZ8oV5gXJy8meOeX+3PqvPbKGzTVNfPJXS1l4+8us3nGAmx5dQ6TDsbu+JcW1FJGjlciwzGlApXNuM4CZPQh8AOi7id+Scq9u3g/AB37+MgDtYccjK6p4+l/fw5Ord3HjRdMId3SQn6PZNyKZIJFwHwvs6HK/Cji9m3JXmNnZwEbgy865Hd2UkQzxyIoqAD5x3+tUH2wjHOngniVb+OuXz+b59dV86qxJBAOG2dGteS8i/SuRcYbu/npjr3z6EzDROXci8Czw626fyGyRmS0zs2U1NTVHVlNJieqDbQDcs2QLAJ+/fwXffWY93//rBibd9DTrdjdw75IttIUjpOqCOBE5XCLhXgWM63K/AtjVtYBzbp9zrs27ezdwandP5Jy7yzk31zk3t6wsc+ddVwwbuJf6V3pTLO9bshWALz6wklufeovbnlnPpJueZuX2On736jYa28IKe5EUSmRYZikw1cwmATuBq4CPdC1gZqOdc7u9uwuBdX1ayzTy1q0XETBj+jf/kuqqpFS7t/iZH/b3v7odiE653FTTxJu7Gnjg9e385pOnsXVfEwtmj2ZIYa4WOhNJkrjh7pwLm9kXgMVAELjXOfemmd0KLHPOPQn8q5ktBMLAfuAT/VjnlNLFP93zw35TTRMADy6Nhv2/P7KG3fWtvL5lP0+t2c3Prj6ZqroWFswexbBBeRTn5RA4yr1qRaRnWjjsKKX7QmLpKidghDscp08azmtb9nPjRdOobmhl4ZwxlA8uoLQ4X+vhiPRCC4f1swtnllNanM8Dr29PdVUyij+//rUt0amX31+8AYAnV++irjnEBTPK2d/UxofmjqM1FOGCmeW0hiIcV1YMoNk5IglSz/0YqQffv/KC0a0Hz5w8gjd31XPj/Om8umkfX33f8bz0di1Xzq1gS20TM0cPVvDLgJBoz13hfowU7qlVWpxPbWMbl58ylidX7eIHV57EU2t287X503hxYw2XnTyWzbVNnFgxBIi+WehNQDKZwj1JPnbPa+QEjX9s0Lz9dDSyJJ/qg228+7gR/HPTPj5/znEs31bHx86YwPo9DVw4cxR1ze1MKy+hLdzBiOI8SvKjo5V6E5B0pHBPso/c/SqNbWHWVNWnuipyFPzhn3HDC9nf2M57p5VRWpzPuGFFDCnMZURxHkOLcinIDTKsKI+AGSOK8+hwTksySFIp3FNEwzQDx3Flg9jb0MbVp41jc00Tl540mh37Wzj7+DJ21rVw0rgh7G1oY9qoEg40t1M+uIDmtghDinJTXXXJYJotkyUCBkewf7YkkT+n/+6Xoksz/H19dDnlH/5tIwDDinKpaw7xronDWLq1jg/MGcMTq3bx3ctn8z9/Wc/Prj6FO16o5NsLZ/HYyp186qzJ/GNDNQtmj2bp1v3MO66UzbVNTBlZTCjSoQvA5Iio597H3trVQEsozBV3vHJMzxMMGJEOR35OgLZwB4PygjS1RzADXdWfXUqL86htbGf6qBLW7znIvCkjeLlyH5edPJbHVu7khgum8uNn3+bHH57DDQ+t4pHPncntz1Vyy8JZ/Pz5Sm68aDoPL6/imjMnsHjtHhbMHs1bu+uZPXYouw60UDGskIhzOpmcJTQsk2JHOzzj99TzcgK0dwl1vxdYlBekuT3Sx7WVdOC/cftv6P6t/1rwbwtyA7SGOhhckENDa5jywfnR4Z/yEjbsPchpE4fz+tb9XHzCKJ5Zu4cPzx3HQ8t28IVzp3D785XcdPF0nt9QzdWnjWfl9gOcO30kb+1qYM64odQ0tjFpxCAaWkOUFucT7uigJD8Xs+hrMj8ngGEU5gVxOHICAZxz5OhTRdIo3FNs14EWmtvDXPDDFxMq3xnq3ok9P8T9P2A/3P37/h+4ZC//tRAb7v5tZ/h75fyrf5MxlDd2aCH7m9qZMrKYt6sPsuCE0azYHp2FtGxrHedNH8kLG2u4cm4F/9y0j4UnjeGVTfs4Z1oZ/9hQwyUnjmZJZS3nTx/JS2/Xcv6MkbxcWcv5M8p5Y2c9cyqGsmVfE5NLB1FZ3cjU8hJ217cwekghkQ5HcAAvWaFwTxN/XrOb9Xsa+Nlzlb2Wyw0aoYijMDdISyhCSUEOB1vDPYa7f+v/gYukM3+YcUhhLvUtISaVDmJLbROzxw7hjZ31nDphGMu31XHBjHKeXbeXD82t4A/LqvjI6eP5/WvbuW7eRO57eSs3XjSN7y/ewHcvn81Nj77BD648iX/742r+54oT+doja/jJVXP4xmNr+fGH53D3S5v5yoXH89jKnXzk9PE8tHQHl58ylufWV3Pe9HJe3byP0ycNZ+u+ZqaPKmFTTSMTRwyi5mAbIwfnE4p0UJyfS8AgNxgg3NHhtSVAONJBYV6Q1lAHZSX5tIYinR2ygtwg9c0hRg0poCUUYVBekOZQhMLcIA0tIUYU5x/Tv6XCPc1ce+/rvLjx8Lnw/ove74kX5+fQ2BZmaFEuB7qEeWy4+38k/n3/zUEkk/hDUf6nET8g/b8Df1jSP+7f92/9cj2V9/+u/E81/cG/kG7CiCK27WvmuLJBbKpp6nyj8s+dfPDUCh5eXkXldy4+pmGsRMNdA2VJct8n3sXG/7qYC2aUH3Lc/3CZE4j+V/gfN3t6z409IeavqOgvtjWAP61KBvJf5+3ep0//U2jEC+Ke4tg/7vem28LR81BN3vko/7yUP3TZX8EOUNsY3cpi275m4J1ZVM+u2wvAYyt3AvDw8ujuZvub2/utLl1pKmSSBANGMGDc+bFTCEUcj66s4lcvb2VfUzv7m9o7Q/loxxL9nyvMjfZcNIVSMpEf6i0hP5yjtyFvSenmUPfhnUmfWmsPtjOypKDff4/CPclyggFygvDR0yfw0dMnsLmmkVc37+e59Xt5dl01eXE+rsWbyRaICXmRTOZ3UPzwzoZpwH5Pv78p3FNsclkxk8uK+ZeTx7BtXzPBgPH717aTnxvgFy9sZmp5Ccu31ZGXc2jox54ric386JhepPMErYikB4X7AFOUl8OM0YMBuGXhLMKRDq45YwLF+Tk8vnIno4YU8NnfreDiE0bzwOvbKSvJp6E13PnzsWPx/t3coNESeueEVSqV5OdwsC3c+YbjT+ETGUj2NWrMfUDLCQaoGFYEwCfmTQJg622XEOlwzD9hFNNHlXDDg6v40Lsq+PJDq5k1ZjAvvV172PP4oZ/vhXsqZ9WUFETDfXBhDi2hCIMLc6htbO+c8ePPeEjGDAeRVKlL0glVzZbJMMGA8d7jyygfXMADi87gspMrWPLv5/LTq05mzrih3HVNdIbUp8+KviFMGBF9gwh4IV/grWCYk4JpNYMLowtmlRQceusvpFVSEO1rDPaO++UHe8eLvaV4870hKr8NmiEkmeTlTfuoqmvu99+jcM8CFcOKGDYoj8evn8eZx41g622X8IXzpnL3tXN5cNEZfHjuOO6+Nhr6p04cBkTXOYfkBuM74X3obUm+H+KHhnpn+ZiQ9+8PKTy0vEgmWL3jAL99dVu//x6Fexa7cGY5RXk5fO+DJ3LapOE8cf087vzYqXxjwQweWHQGAN+4ZCYAC08a0+/18cN7SEwPfnBhziHHB8f24Hvoyb9zX+EumaUgCXsAaMx9ADlp3FAA/s/Zk4HoGD7AlXMrKMoNcsbkEZw8fij//fQ6LphRzreefLNzpcK+EDss44f6O8M0PfXMY8I8JuSHqOcuGaYwT+EuSeD3fD9y+ngAfvup0wH4lzljyc8NsGxrHaUlefx9XTUjS/K58eE1ncsjHImC3OgHxZKC2J56bGj3MAbfQ0/e/0PxZ+FoWWRJdwU5/T9oonCXHvknOs+aWgrA9FHRqZqXnDiagBk79jdjBi9urKWpLcz6vQfZWtvE23sbu53imBMIkBcMdC6VUBIzTPPOcMuhoR6vB58bjJ44yAkahKIfeVtC2XGVrt8Gf60ULfmcHfy/gf6kcJcjVpQXfdlMLS8BYMrIkkMe39fYRksoQlu4g70NrWyqaeKFDdVMGFHE5LJBFOfnUJyf0yXkux9Df2cWTczxmPD31+Xx+SGfyUsx+HXODUZX/fSXl8jpXEsoOl00E9smCnfJUF2XND2urJh3H1fKNWdMAODT75lMXVM7588YSXF+DiMG5XHC2MFsrm3ixIqhjBlSwPjhRQQMyrzniZ0VMzhmzN7ffq63q3QzJeS7btbSGuroDPfYtuUGArTS0dk2f3XRbORf7+C/oWVDW/0hyv6kcJekGzYoj2GD8oDo8gsAp04YDkRn+LSFI8waM4TxI4rIDRqXnDiGuuZ2FswexROrdvKuicOBTUzzPjmUD46+CRTl5Rxy1a7P7/UW5UUvkkrncOga6q2hw0M9VueqoDmBjHkDS5R/wV1eToBwe/SK5tZQR1a0NT8JPXdNhZS0k58TZHbFEIYU5vKJeZMoK8nnloWzmD5qMC/ceC7nTh/Jy18/j8tPqeCJ6+dxy8JZ3HDBVP70xbM4d1oZv/nkaQB8/eLpAFx28lgA5npz/CuGFQLpdfGTf94gN3CEf5JeuAU7h2uCh9zPRP6/hb+IXjDOMtepuCDvWGkqpEgPxg6NBrQ/vfOGC44H4L7rosHuT/N838xyhhXlcfkpFUwbVcKjK3Zy7vQy7nlpCwtPGsN/Pr6Wa8+cwI0Pr+HyU8by6IqdnFQxhNVV9UlpR+dWesEAoUikc02ghOPKL+/9YNAO7cln0iYu/icq/9+ic2+DmHL+v01nW3ODNLZl1oY1GpYROUb++L//JuBP9/zmpdGLt/70xbMAuOKUCszgPy+ZSVFekMrqRkYU57G1tpkRxXms3F7H8EH5LN9Wx6C8IP/ctI8O56iqa6HmYBuhjo4jmn7pjyPnBo32yDu90Z7ELgwXL/0DnWsKBQlFwmm9Tk/X/YNbOiI9furoqfZ+8Uxoq08nVEWSxA/X4d65gBPGDgFg9JDoJ4TjvfH9C2dGd9L64vlTgegmEs5BY1uY9nAHHc7R2BYmFOmgqq6FoYW5rN3VwNihhfz1zT3Mm1LKr1/ZyvnTy/nRsxv51FmT+OlzlVx64mgeeH0H7502kj+t3sWJFUNZUlnLxNJBvLGznuGD8qhvCUXHalvDh2V7T9tlvhN80XHrdOrd+j31fG/qak/DK4l+ioltazqHvMJdJM35M3WG5+Qd9tisMdE3iNMnjwBg/gmjALji1AoAvnTBVJxz/Kv3RvHRI7+tAAAIdklEQVT5c6ZQPriARe+ZzJSRxTy3vppzp5fxyIqdfGDOGG5/rpLPnD2Z7y/ewKffM5lrf/kaP/vIyXzx9yv5zmWzue5XS/naxdP55uNr+ew5x/H9xRtYOGcMv3t1OxfNGsWjK3dy1pRSnt9Qw5xxQ1m14wAjBuWxryk5qxT6/IvM8nMCNLf3HOqH/VyCzx+IGa5Jx6WlkzEsk9AG2WY2H/gJEATucc7dFvN4PvAb4FRgH/Bh59zW3p5zoG2QLZIszjnMjD31rZSV5LPrQAujhhSwce9BJpUOYv2egxxXWszSrfuZMWYwr2/Zx8zRQ3hrdz1TR5bw0tu1TB1ZzMbqg1QMK+L59dWUDy5gf1MbRXk5rN/TQCjiaGwNs6ehlZEl+bxd3cjJ44eycvsBJo4oYuu+ZiaXDWJzTRPHlxezcW8jU0YWU1ndyAljB7N2ZwMzRw/mrd0NnRtK+xtMVwwrpKqupfONx98HIHZTeP+4fz/2eElBDge73E+nkF/5zQs7Z4wdqUQ3yI4b7mYWBDYCFwJVwFLgaufcW13KfB440Tn3WTO7CrjMOffh3p5X4S6S2fzsCHc4OpwjaEaHg/ZIB81tYQryghxoCjGkMJfdDS2UFudTWd3I1JHFrN3VwIzRJfyzch/vPm4Ez6zdw4LZo3l4eRVXzq3gvpe38LEzJvA/f9nAVy48nu/8eR3ffP9MfrB4A186fypf/sMqbr50Jh++61V+cc2pXHffUn5xzal8+aFV/PBDc/js75Zz2+Wz+fqjb3DTxdP57jPrWXT2ZO56cTNXvWscDy7dwb/MGcPjq3Zx1pRSllTWdr4ZDSvKpa451KdvBv4c/dLifGob23jz2xcxKP/oBk76MtzPBG5xzl3k3b8JwDn33S5lFntlXjGzHGAPUOZ6eXKFu4gkS6u381drOOKdeI1eS9Do7Qy2v6md4oIcDjS3U5yfQ3N7hILcIG3hCDmBAPUtIXICRnskel7FMNrCkS4bzASpbmhjRHEe27xPLRv2HOT48hJWbq/j5PHDeHFjDefPGMnf11V3Lt53NBIN90TeOsYCO7rcrwJO76mMcy5sZvXACODwrYFERJLMP4HpL50RDETv+1c/jxpSALyzIczQokN/vqwkn3j8k+7+dpn+/WmjDr31L9zrb4mM6nd3HqOnqae9lcHMFpnZMjNbVlNTk0j9RETkKCQS7lXAuC73K4BdPZXxhmWGAPtjn8g5d5dzbq5zbm5ZWdnR1VhEROJKJNyXAlPNbJKZ5QFXAU/GlHkS+Lj3/QeB53obbxcRkf4Vd8zdG0P/ArCY6FTIe51zb5rZrcAy59yTwC+B35pZJdEe+1X9WWkREeldQnNxnHNPA0/HHLu5y/etwJV9WzURETlaWhVSRCQLKdxFRLKQwl1EJAsltLZMv/xisxpg21H+eCnZc4GU2pKe1Jb0ky3tgGNrywTnXNy55CkL92NhZssSufw2E6gt6UltST/Z0g5ITls0LCMikoUU7iIiWShTw/2uVFegD6kt6UltST/Z0g5IQlsycsxdRER6l6k9dxER6UXGhbuZzTezDWZWaWZfT3V94jGze82s2szWdjk23Mz+ZmZve7fDvONmZj/12rbGzE5JXc0PZWbjzOx5M1tnZm+a2Ze845nYlgIze93MVntt+bZ3fJKZvea15SFvoTzMLN+7X+k9PjGV9e+OmQXNbKWZPeXdz8i2mNlWM3vDzFaZ2TLvWMa9xgDMbKiZPWxm672/mzOT2ZaMCneLbvn3c+BiYCZwtZnNTG2t4voVMD/m2NeBvzvnpgJ/9+5DtF1Tva9FwB1JqmMiwsBXnXMzgDOA671/+0xsSxtwnnPuJGAOMN/MzgC+B/zIa0sd8Cmv/KeAOufcFOBHXrl08yVgXZf7mdyWc51zc7pMFczE1xhE953+i3NuOnAS0f+f5LXFOZcxX8CZwOIu928Cbkp1vRKo90RgbZf7G4DR3vejgQ3e978guj/tYeXS7Qt4gui+uhndFqAIWEF0d7FaICf2tUZ0RdQzve9zvHKW6rp3aUOFFxTnAU8R3TwnU9uyFSiNOZZxrzFgMLAl9t82mW3JqJ473W/5NzZFdTkW5c653QDe7UjveEa0z/sofzLwGhnaFm8YYxVQDfwN2AQccM6FvSJd63vINpKAv41kuvgx8DXA3815BJnbFgf81cyWm9ki71gmvsYmAzXAfd5w2T1mNogktiXTwj2h7fwyWNq3z8yKgUeAG5xzDb0V7eZY2rTFORdxzs0h2us9DZjRXTHvNm3bYmaXAtXOueVdD3dTNO3b4pnnnDuF6DDF9WZ2di9l07ktOcApwB3OuZOBJt4ZgulOn7cl08I9kS3/MsFeMxsN4N1We8fTun1mlks02O93zj3qHc7ItviccweAfxA9jzDUottEwqH1TWgbyRSZByw0s63Ag0SHZn5MZrYF59wu77YaeIzoG28mvsaqgCrn3Gve/YeJhn3S2pJp4Z7Iln+ZoOu2hB8nOn7tH7/WO3N+BlDvf4RLNTMzojturXPO/bDLQ5nYljIzG+p9XwhcQPRk1/NEt4mEw9uSlttIOuducs5VOOcmEv17eM4591EysC1mNsjMSvzvgfcBa8nA15hzbg+ww8ymeYfOB94imW1J9YmHozhRsQDYSHSM9Buprk8C9X0A2A2EiL47f4roGOffgbe92+FeWSM6G2gT8AYwN9X179KOs4h+TFwDrPK+FmRoW04EVnptWQvc7B2fDLwOVAJ/BPK94wXe/Urv8cmpbkMP7ToHeCpT2+LVebX39ab/952JrzGvfnOAZd7r7HFgWDLboitURUSyUKYNy4iISAIU7iIiWUjhLiKShRTuIiJZSOEuIpKFFO4iIllI4S4ikoUU7iIiWej/A9L+MVJuzx4UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5bd5695c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn.fc_net.J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the LeakReLU method is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.76\n"
     ]
    }
   ],
   "source": [
    "cnn.forward(test_25_x)\n",
    "predicted = cnn3.fc_net.yhat\n",
    "accuracy = 1.0 - float(len(np.nonzero(predicted - test_25_y)[0])) / len(test_25_y)\n",
    "print (\"Training Accuracy=\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Train with 60000 digits using leakyRuLU\n",
    "\n",
    "batch_size = 25<br>\n",
    "epoch = 150<br>\n",
    "learning rate = 0.075 ~ 0.005 becomes smaller after every epoch<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0  Accuracy: 12.0 %\n",
      "Batch: 1  Accuracy: 14.000000000000002 %\n",
      "Batch: 2  Accuracy: 9.6 %\n",
      "Batch: 3  Accuracy: 19.6 %\n",
      "Batch: 4  Accuracy: 8.4 %\n",
      "Batch: 5  Accuracy: 8.799999999999999 %\n",
      "Batch: 6  Accuracy: 6.0 %\n",
      "Batch: 7  Accuracy: 10.0 %\n",
      "Batch: 8  Accuracy: 16.400000000000002 %\n",
      "Batch: 9  Accuracy: 13.600000000000001 %\n",
      "Batch: 10  Accuracy: 14.000000000000002 %\n",
      "Batch: 11  Accuracy: 14.799999999999999 %\n",
      "Batch: 12  Accuracy: 18.8 %\n",
      "Batch: 13  Accuracy: 12.4 %\n",
      "Batch: 14  Accuracy: 13.600000000000001 %\n",
      "Batch: 15  Accuracy: 14.000000000000002 %\n",
      "Batch: 16  Accuracy: 12.8 %\n",
      "Batch: 17  Accuracy: 14.799999999999999 %\n",
      "Batch: 18  Accuracy: 12.8 %\n",
      "Batch: 19  Accuracy: 11.600000000000001 %\n",
      "Batch: 20  Accuracy: 12.0 %\n",
      "Batch: 21  Accuracy: 14.000000000000002 %\n",
      "Batch: 22  Accuracy: 10.4 %\n",
      "Batch: 23  Accuracy: 12.4 %\n",
      "Batch: 24  Accuracy: 21.6 %\n",
      "Batch: 25  Accuracy: 19.2 %\n",
      "Batch: 26  Accuracy: 25.6 %\n",
      "Batch: 27  Accuracy: 23.599999999999998 %\n",
      "Batch: 28  Accuracy: 20.0 %\n",
      "Batch: 29  Accuracy: 16.8 %\n",
      "Batch: 30  Accuracy: 22.0 %\n",
      "Batch: 31  Accuracy: 22.400000000000002 %\n",
      "Batch: 32  Accuracy: 22.0 %\n",
      "Batch: 33  Accuracy: 18.8 %\n",
      "Batch: 34  Accuracy: 10.0 %\n",
      "Batch: 35  Accuracy: 13.200000000000001 %\n",
      "Batch: 36  Accuracy: 22.8 %\n",
      "Batch: 37  Accuracy: 9.2 %\n",
      "Batch: 38  Accuracy: 28.799999999999997 %\n",
      "Batch: 39  Accuracy: 18.8 %\n",
      "Batch: 40  Accuracy: 10.8 %\n",
      "Batch: 41  Accuracy: 26.8 %\n",
      "Batch: 42  Accuracy: 18.8 %\n",
      "Batch: 43  Accuracy: 27.6 %\n",
      "Batch: 44  Accuracy: 25.6 %\n",
      "Batch: 45  Accuracy: 22.0 %\n",
      "Batch: 46  Accuracy: 12.8 %\n",
      "Batch: 47  Accuracy: 13.600000000000001 %\n",
      "Batch: 48  Accuracy: 24.8 %\n",
      "Batch: 49  Accuracy: 38.0 %\n",
      "Batch: 50  Accuracy: 32.800000000000004 %\n",
      "Batch: 51  Accuracy: 13.200000000000001 %\n",
      "Batch: 52  Accuracy: 24.0 %\n",
      "Batch: 53  Accuracy: 28.000000000000004 %\n",
      "Batch: 54  Accuracy: 28.4 %\n",
      "Batch: 55  Accuracy: 42.8 %\n",
      "Batch: 56  Accuracy: 36.4 %\n",
      "Batch: 57  Accuracy: 26.400000000000002 %\n",
      "Batch: 58  Accuracy: 46.400000000000006 %\n",
      "Batch: 59  Accuracy: 42.4 %\n",
      "Batch: 60  Accuracy: 35.6 %\n",
      "Batch: 61  Accuracy: 36.4 %\n",
      "Batch: 62  Accuracy: 40.8 %\n",
      "Batch: 63  Accuracy: 51.6 %\n",
      "Batch: 64  Accuracy: 43.6 %\n",
      "Batch: 65  Accuracy: 42.8 %\n",
      "Batch: 66  Accuracy: 38.4 %\n",
      "Batch: 67  Accuracy: 39.6 %\n",
      "Batch: 68  Accuracy: 40.8 %\n",
      "Batch: 69  Accuracy: 50.4 %\n",
      "Batch: 70  Accuracy: 55.60000000000001 %\n",
      "Batch: 71  Accuracy: 50.8 %\n",
      "Batch: 72  Accuracy: 58.8 %\n",
      "Batch: 73  Accuracy: 52.400000000000006 %\n",
      "Batch: 74  Accuracy: 61.6 %\n",
      "Batch: 75  Accuracy: 64.4 %\n",
      "Batch: 76  Accuracy: 68.4 %\n",
      "Batch: 77  Accuracy: 67.60000000000001 %\n",
      "Batch: 78  Accuracy: 59.199999999999996 %\n",
      "Batch: 79  Accuracy: 60.0 %\n",
      "Batch: 80  Accuracy: 60.0 %\n",
      "Batch: 81  Accuracy: 70.0 %\n",
      "Batch: 82  Accuracy: 67.2 %\n",
      "Batch: 83  Accuracy: 63.2 %\n",
      "Batch: 84  Accuracy: 57.599999999999994 %\n",
      "Batch: 85  Accuracy: 60.4 %\n",
      "Batch: 86  Accuracy: 67.60000000000001 %\n",
      "Batch: 87  Accuracy: 79.2 %\n",
      "Batch: 88  Accuracy: 75.2 %\n",
      "Batch: 89  Accuracy: 73.6 %\n",
      "Batch: 90  Accuracy: 71.2 %\n",
      "Batch: 91  Accuracy: 84.8 %\n",
      "Batch: 92  Accuracy: 83.6 %\n",
      "Batch: 93  Accuracy: 76.4 %\n",
      "Batch: 94  Accuracy: 70.39999999999999 %\n",
      "Batch: 95  Accuracy: 73.6 %\n",
      "Batch: 96  Accuracy: 77.60000000000001 %\n",
      "Batch: 97  Accuracy: 81.6 %\n",
      "Batch: 98  Accuracy: 74.4 %\n",
      "Batch: 99  Accuracy: 76.0 %\n",
      "Batch: 100  Accuracy: 78.0 %\n",
      "Batch: 101  Accuracy: 85.2 %\n",
      "Batch: 102  Accuracy: 87.6 %\n",
      "Batch: 103  Accuracy: 84.0 %\n",
      "Batch: 104  Accuracy: 84.39999999999999 %\n",
      "Batch: 105  Accuracy: 80.4 %\n",
      "Batch: 106  Accuracy: 84.39999999999999 %\n",
      "Batch: 107  Accuracy: 87.2 %\n",
      "Batch: 108  Accuracy: 85.2 %\n",
      "Batch: 109  Accuracy: 83.6 %\n",
      "Batch: 110  Accuracy: 83.6 %\n",
      "Batch: 111  Accuracy: 89.2 %\n",
      "Batch: 112  Accuracy: 91.2 %\n",
      "Batch: 113  Accuracy: 91.2 %\n",
      "Batch: 114  Accuracy: 90.4 %\n",
      "Batch: 115  Accuracy: 88.8 %\n",
      "Batch: 116  Accuracy: 82.8 %\n",
      "Batch: 117  Accuracy: 77.60000000000001 %\n",
      "Batch: 118  Accuracy: 85.2 %\n",
      "Batch: 119  Accuracy: 80.80000000000001 %\n",
      "Batch: 120  Accuracy: 85.6 %\n",
      "Batch: 121  Accuracy: 93.2 %\n",
      "Batch: 122  Accuracy: 90.4 %\n",
      "Batch: 123  Accuracy: 85.6 %\n",
      "Batch: 124  Accuracy: 86.8 %\n",
      "Batch: 125  Accuracy: 87.2 %\n",
      "Batch: 126  Accuracy: 90.0 %\n",
      "Batch: 127  Accuracy: 91.60000000000001 %\n",
      "Batch: 128  Accuracy: 90.4 %\n",
      "Batch: 129  Accuracy: 88.0 %\n",
      "Batch: 130  Accuracy: 94.8 %\n",
      "Batch: 131  Accuracy: 93.60000000000001 %\n",
      "Batch: 132  Accuracy: 94.39999999999999 %\n",
      "Batch: 133  Accuracy: 93.2 %\n",
      "Batch: 134  Accuracy: 95.6 %\n",
      "Batch: 135  Accuracy: 94.39999999999999 %\n",
      "Batch: 136  Accuracy: 94.8 %\n",
      "Batch: 137  Accuracy: 93.2 %\n",
      "Batch: 138  Accuracy: 90.0 %\n",
      "Batch: 139  Accuracy: 89.2 %\n",
      "Batch: 140  Accuracy: 88.8 %\n",
      "Batch: 141  Accuracy: 87.2 %\n",
      "Batch: 142  Accuracy: 93.2 %\n",
      "Batch: 143  Accuracy: 91.60000000000001 %\n",
      "Batch: 144  Accuracy: 93.60000000000001 %\n",
      "Batch: 145  Accuracy: 93.60000000000001 %\n",
      "Batch: 146  Accuracy: 93.60000000000001 %\n",
      "Batch: 147  Accuracy: 92.80000000000001 %\n",
      "Batch: 148  Accuracy: 89.60000000000001 %\n",
      "Batch: 149  Accuracy: 86.0 %\n",
      "Batch: 150  Accuracy: 85.2 %\n",
      "Batch: 151  Accuracy: 86.4 %\n",
      "Batch: 152  Accuracy: 93.2 %\n",
      "Batch: 153  Accuracy: 93.60000000000001 %\n",
      "Batch: 154  Accuracy: 92.4 %\n",
      "Batch: 155  Accuracy: 93.60000000000001 %\n",
      "Batch: 156  Accuracy: 96.39999999999999 %\n",
      "Batch: 157  Accuracy: 88.4 %\n",
      "Batch: 158  Accuracy: 90.0 %\n",
      "Batch: 159  Accuracy: 90.4 %\n",
      "Batch: 160  Accuracy: 93.2 %\n",
      "Batch: 161  Accuracy: 94.0 %\n",
      "Batch: 162  Accuracy: 93.60000000000001 %\n",
      "Batch: 163  Accuracy: 93.60000000000001 %\n",
      "Batch: 164  Accuracy: 93.2 %\n",
      "Batch: 165  Accuracy: 91.60000000000001 %\n",
      "Batch: 166  Accuracy: 93.60000000000001 %\n",
      "Batch: 167  Accuracy: 94.39999999999999 %\n",
      "Batch: 168  Accuracy: 88.8 %\n",
      "Batch: 169  Accuracy: 86.4 %\n",
      "Batch: 170  Accuracy: 88.4 %\n",
      "Batch: 171  Accuracy: 87.2 %\n",
      "Batch: 172  Accuracy: 93.2 %\n",
      "Batch: 173  Accuracy: 95.19999999999999 %\n",
      "Batch: 174  Accuracy: 94.39999999999999 %\n",
      "Batch: 175  Accuracy: 93.60000000000001 %\n",
      "Batch: 176  Accuracy: 90.4 %\n",
      "Batch: 177  Accuracy: 92.0 %\n",
      "Batch: 178  Accuracy: 94.8 %\n",
      "Batch: 179  Accuracy: 92.0 %\n",
      "Batch: 180  Accuracy: 91.60000000000001 %\n",
      "Batch: 181  Accuracy: 95.19999999999999 %\n",
      "Batch: 182  Accuracy: 92.4 %\n",
      "Batch: 183  Accuracy: 92.4 %\n",
      "Batch: 184  Accuracy: 92.0 %\n",
      "Batch: 185  Accuracy: 88.8 %\n",
      "Batch: 186  Accuracy: 91.2 %\n",
      "Batch: 187  Accuracy: 96.8 %\n",
      "Batch: 188  Accuracy: 91.60000000000001 %\n",
      "Batch: 189  Accuracy: 91.60000000000001 %\n",
      "Batch: 190  Accuracy: 93.60000000000001 %\n",
      "Batch: 191  Accuracy: 92.80000000000001 %\n",
      "Batch: 192  Accuracy: 91.60000000000001 %\n",
      "Batch: 193  Accuracy: 92.80000000000001 %\n",
      "Batch: 194  Accuracy: 95.6 %\n",
      "Batch: 195  Accuracy: 93.2 %\n",
      "Batch: 196  Accuracy: 90.0 %\n",
      "Batch: 197  Accuracy: 94.39999999999999 %\n",
      "Batch: 198  Accuracy: 90.8 %\n",
      "Batch: 199  Accuracy: 94.8 %\n",
      "Batch: 200  Accuracy: 94.0 %\n",
      "Batch: 201  Accuracy: 90.4 %\n",
      "Batch: 202  Accuracy: 95.19999999999999 %\n",
      "Batch: 203  Accuracy: 97.2 %\n",
      "Batch: 204  Accuracy: 93.60000000000001 %\n",
      "Batch: 205  Accuracy: 93.60000000000001 %\n",
      "Batch: 206  Accuracy: 94.8 %\n",
      "Batch: 207  Accuracy: 96.39999999999999 %\n",
      "Batch: 208  Accuracy: 92.0 %\n",
      "Batch: 209  Accuracy: 94.39999999999999 %\n",
      "Batch: 210  Accuracy: 94.39999999999999 %\n",
      "Batch: 211  Accuracy: 91.60000000000001 %\n",
      "Batch: 212  Accuracy: 93.60000000000001 %\n",
      "Batch: 213  Accuracy: 96.39999999999999 %\n",
      "Batch: 214  Accuracy: 93.60000000000001 %\n",
      "Batch: 215  Accuracy: 93.60000000000001 %\n",
      "Batch: 216  Accuracy: 95.19999999999999 %\n",
      "Batch: 217  Accuracy: 95.19999999999999 %\n",
      "Batch: 218  Accuracy: 96.8 %\n",
      "Batch: 219  Accuracy: 91.60000000000001 %\n",
      "Batch: 220  Accuracy: 96.39999999999999 %\n",
      "Batch: 221  Accuracy: 94.0 %\n",
      "Batch: 222  Accuracy: 93.2 %\n",
      "Batch: 223  Accuracy: 96.0 %\n",
      "Batch: 224  Accuracy: 95.6 %\n",
      "Batch: 225  Accuracy: 96.0 %\n",
      "Batch: 226  Accuracy: 94.39999999999999 %\n",
      "Batch: 227  Accuracy: 96.39999999999999 %\n",
      "Batch: 228  Accuracy: 95.19999999999999 %\n",
      "Batch: 229  Accuracy: 94.8 %\n",
      "Batch: 230  Accuracy: 91.2 %\n",
      "Batch: 231  Accuracy: 95.6 %\n",
      "Batch: 232  Accuracy: 95.6 %\n",
      "Batch: 233  Accuracy: 95.6 %\n",
      "Batch: 234  Accuracy: 97.6 %\n",
      "Batch: 235  Accuracy: 98.0 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 91.60000000000001 %\n",
      "Batch: 238  Accuracy: 95.19999999999999 %\n",
      "Batch: 239  Accuracy: 98.8 %\n",
      "Epoch: 0  Accuracy: 68.58166666666666 %  loss: 0.9183994647893946\n",
      "Batch: 0  Accuracy: 96.0 %\n",
      "Batch: 1  Accuracy: 95.19999999999999 %\n",
      "Batch: 2  Accuracy: 95.19999999999999 %\n",
      "Batch: 3  Accuracy: 94.8 %\n",
      "Batch: 4  Accuracy: 94.39999999999999 %\n",
      "Batch: 5  Accuracy: 95.19999999999999 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 6  Accuracy: 96.8 %\n",
      "Batch: 7  Accuracy: 96.8 %\n",
      "Batch: 8  Accuracy: 96.8 %\n",
      "Batch: 9  Accuracy: 97.2 %\n",
      "Batch: 10  Accuracy: 94.8 %\n",
      "Batch: 11  Accuracy: 97.2 %\n",
      "Batch: 12  Accuracy: 97.6 %\n",
      "Batch: 13  Accuracy: 96.8 %\n",
      "Batch: 14  Accuracy: 95.19999999999999 %\n",
      "Batch: 15  Accuracy: 96.0 %\n",
      "Batch: 16  Accuracy: 94.8 %\n",
      "Batch: 17  Accuracy: 94.39999999999999 %\n",
      "Batch: 18  Accuracy: 95.6 %\n",
      "Batch: 19  Accuracy: 96.39999999999999 %\n",
      "Batch: 20  Accuracy: 94.8 %\n",
      "Batch: 21  Accuracy: 96.39999999999999 %\n",
      "Batch: 22  Accuracy: 96.0 %\n",
      "Batch: 23  Accuracy: 95.19999999999999 %\n",
      "Batch: 24  Accuracy: 98.0 %\n",
      "Batch: 25  Accuracy: 94.8 %\n",
      "Batch: 26  Accuracy: 96.0 %\n",
      "Batch: 27  Accuracy: 93.60000000000001 %\n",
      "Batch: 28  Accuracy: 92.80000000000001 %\n",
      "Batch: 29  Accuracy: 95.19999999999999 %\n",
      "Batch: 30  Accuracy: 96.39999999999999 %\n",
      "Batch: 31  Accuracy: 94.0 %\n",
      "Batch: 32  Accuracy: 94.39999999999999 %\n",
      "Batch: 33  Accuracy: 91.60000000000001 %\n",
      "Batch: 34  Accuracy: 92.0 %\n",
      "Batch: 35  Accuracy: 87.6 %\n",
      "Batch: 36  Accuracy: 92.80000000000001 %\n",
      "Batch: 37  Accuracy: 93.2 %\n",
      "Batch: 38  Accuracy: 96.0 %\n",
      "Batch: 39  Accuracy: 96.8 %\n",
      "Batch: 40  Accuracy: 92.4 %\n",
      "Batch: 41  Accuracy: 95.19999999999999 %\n",
      "Batch: 42  Accuracy: 97.6 %\n",
      "Batch: 43  Accuracy: 95.6 %\n",
      "Batch: 44  Accuracy: 95.6 %\n",
      "Batch: 45  Accuracy: 98.0 %\n",
      "Batch: 46  Accuracy: 90.8 %\n",
      "Batch: 47  Accuracy: 93.60000000000001 %\n",
      "Batch: 48  Accuracy: 94.8 %\n",
      "Batch: 49  Accuracy: 94.39999999999999 %\n",
      "Batch: 50  Accuracy: 94.8 %\n",
      "Batch: 51  Accuracy: 95.19999999999999 %\n",
      "Batch: 52  Accuracy: 93.60000000000001 %\n",
      "Batch: 53  Accuracy: 96.0 %\n",
      "Batch: 54  Accuracy: 95.6 %\n",
      "Batch: 55  Accuracy: 94.8 %\n",
      "Batch: 56  Accuracy: 95.6 %\n",
      "Batch: 57  Accuracy: 92.80000000000001 %\n",
      "Batch: 58  Accuracy: 96.0 %\n",
      "Batch: 59  Accuracy: 94.8 %\n",
      "Batch: 60  Accuracy: 96.8 %\n",
      "Batch: 61  Accuracy: 94.8 %\n",
      "Batch: 62  Accuracy: 97.2 %\n",
      "Batch: 63  Accuracy: 92.80000000000001 %\n",
      "Batch: 64  Accuracy: 95.6 %\n",
      "Batch: 65  Accuracy: 96.39999999999999 %\n",
      "Batch: 66  Accuracy: 94.8 %\n",
      "Batch: 67  Accuracy: 97.6 %\n",
      "Batch: 68  Accuracy: 94.8 %\n",
      "Batch: 69  Accuracy: 96.0 %\n",
      "Batch: 70  Accuracy: 94.8 %\n",
      "Batch: 71  Accuracy: 94.0 %\n",
      "Batch: 72  Accuracy: 96.8 %\n",
      "Batch: 73  Accuracy: 96.0 %\n",
      "Batch: 74  Accuracy: 97.2 %\n",
      "Batch: 75  Accuracy: 98.8 %\n",
      "Batch: 76  Accuracy: 96.8 %\n",
      "Batch: 77  Accuracy: 96.0 %\n",
      "Batch: 78  Accuracy: 98.4 %\n",
      "Batch: 79  Accuracy: 96.39999999999999 %\n",
      "Batch: 80  Accuracy: 95.6 %\n",
      "Batch: 81  Accuracy: 98.8 %\n",
      "Batch: 82  Accuracy: 96.39999999999999 %\n",
      "Batch: 83  Accuracy: 94.0 %\n",
      "Batch: 84  Accuracy: 96.0 %\n",
      "Batch: 85  Accuracy: 97.6 %\n",
      "Batch: 86  Accuracy: 96.0 %\n",
      "Batch: 87  Accuracy: 95.19999999999999 %\n",
      "Batch: 88  Accuracy: 94.39999999999999 %\n",
      "Batch: 89  Accuracy: 94.8 %\n",
      "Batch: 90  Accuracy: 93.2 %\n",
      "Batch: 91  Accuracy: 97.2 %\n",
      "Batch: 92  Accuracy: 95.19999999999999 %\n",
      "Batch: 93  Accuracy: 98.4 %\n",
      "Batch: 94  Accuracy: 96.39999999999999 %\n",
      "Batch: 95  Accuracy: 97.6 %\n",
      "Batch: 96  Accuracy: 95.6 %\n",
      "Batch: 97  Accuracy: 96.39999999999999 %\n",
      "Batch: 98  Accuracy: 96.8 %\n",
      "Batch: 99  Accuracy: 96.8 %\n",
      "Batch: 100  Accuracy: 96.0 %\n",
      "Batch: 101  Accuracy: 96.39999999999999 %\n",
      "Batch: 102  Accuracy: 97.6 %\n",
      "Batch: 103  Accuracy: 95.6 %\n",
      "Batch: 104  Accuracy: 98.0 %\n",
      "Batch: 105  Accuracy: 97.2 %\n",
      "Batch: 106  Accuracy: 94.0 %\n",
      "Batch: 107  Accuracy: 96.8 %\n",
      "Batch: 108  Accuracy: 96.0 %\n",
      "Batch: 109  Accuracy: 97.2 %\n",
      "Batch: 110  Accuracy: 97.2 %\n",
      "Batch: 111  Accuracy: 96.8 %\n",
      "Batch: 112  Accuracy: 96.0 %\n",
      "Batch: 113  Accuracy: 95.19999999999999 %\n",
      "Batch: 114  Accuracy: 95.19999999999999 %\n",
      "Batch: 115  Accuracy: 96.0 %\n",
      "Batch: 116  Accuracy: 97.2 %\n",
      "Batch: 117  Accuracy: 96.8 %\n",
      "Batch: 118  Accuracy: 97.2 %\n",
      "Batch: 119  Accuracy: 94.0 %\n",
      "Batch: 120  Accuracy: 96.0 %\n",
      "Batch: 121  Accuracy: 96.8 %\n",
      "Batch: 122  Accuracy: 95.19999999999999 %\n",
      "Batch: 123  Accuracy: 94.0 %\n",
      "Batch: 124  Accuracy: 97.2 %\n",
      "Batch: 125  Accuracy: 94.39999999999999 %\n",
      "Batch: 126  Accuracy: 95.6 %\n",
      "Batch: 127  Accuracy: 96.0 %\n",
      "Batch: 128  Accuracy: 96.0 %\n",
      "Batch: 129  Accuracy: 93.60000000000001 %\n",
      "Batch: 130  Accuracy: 97.6 %\n",
      "Batch: 131  Accuracy: 96.0 %\n",
      "Batch: 132  Accuracy: 96.0 %\n",
      "Batch: 133  Accuracy: 97.2 %\n",
      "Batch: 134  Accuracy: 98.8 %\n",
      "Batch: 135  Accuracy: 98.4 %\n",
      "Batch: 136  Accuracy: 98.4 %\n",
      "Batch: 137  Accuracy: 97.6 %\n",
      "Batch: 138  Accuracy: 97.6 %\n",
      "Batch: 139  Accuracy: 94.8 %\n",
      "Batch: 140  Accuracy: 96.39999999999999 %\n",
      "Batch: 141  Accuracy: 96.8 %\n",
      "Batch: 142  Accuracy: 97.6 %\n",
      "Batch: 143  Accuracy: 96.8 %\n",
      "Batch: 144  Accuracy: 97.2 %\n",
      "Batch: 145  Accuracy: 97.2 %\n",
      "Batch: 146  Accuracy: 96.8 %\n",
      "Batch: 147  Accuracy: 96.0 %\n",
      "Batch: 148  Accuracy: 96.0 %\n",
      "Batch: 149  Accuracy: 92.80000000000001 %\n",
      "Batch: 150  Accuracy: 94.39999999999999 %\n",
      "Batch: 151  Accuracy: 96.39999999999999 %\n",
      "Batch: 152  Accuracy: 98.4 %\n",
      "Batch: 153  Accuracy: 97.2 %\n",
      "Batch: 154  Accuracy: 95.19999999999999 %\n",
      "Batch: 155  Accuracy: 97.6 %\n",
      "Batch: 156  Accuracy: 98.8 %\n",
      "Batch: 157  Accuracy: 91.60000000000001 %\n",
      "Batch: 158  Accuracy: 95.6 %\n",
      "Batch: 159  Accuracy: 96.8 %\n",
      "Batch: 160  Accuracy: 97.2 %\n",
      "Batch: 161  Accuracy: 98.0 %\n",
      "Batch: 162  Accuracy: 96.8 %\n",
      "Batch: 163  Accuracy: 96.39999999999999 %\n",
      "Batch: 164  Accuracy: 95.6 %\n",
      "Batch: 165  Accuracy: 94.8 %\n",
      "Batch: 166  Accuracy: 96.8 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 94.0 %\n",
      "Batch: 169  Accuracy: 89.60000000000001 %\n",
      "Batch: 170  Accuracy: 84.8 %\n",
      "Batch: 171  Accuracy: 82.0 %\n",
      "Batch: 172  Accuracy: 95.6 %\n",
      "Batch: 173  Accuracy: 97.6 %\n",
      "Batch: 174  Accuracy: 97.2 %\n",
      "Batch: 175  Accuracy: 97.2 %\n",
      "Batch: 176  Accuracy: 96.8 %\n",
      "Batch: 177  Accuracy: 93.60000000000001 %\n",
      "Batch: 178  Accuracy: 98.0 %\n",
      "Batch: 179  Accuracy: 94.0 %\n",
      "Batch: 180  Accuracy: 95.6 %\n",
      "Batch: 181  Accuracy: 96.39999999999999 %\n",
      "Batch: 182  Accuracy: 94.8 %\n",
      "Batch: 183  Accuracy: 94.8 %\n",
      "Batch: 184  Accuracy: 94.0 %\n",
      "Batch: 185  Accuracy: 94.0 %\n",
      "Batch: 186  Accuracy: 94.8 %\n",
      "Batch: 187  Accuracy: 98.0 %\n",
      "Batch: 188  Accuracy: 95.6 %\n",
      "Batch: 189  Accuracy: 95.19999999999999 %\n",
      "Batch: 190  Accuracy: 95.19999999999999 %\n",
      "Batch: 191  Accuracy: 95.19999999999999 %\n",
      "Batch: 192  Accuracy: 97.2 %\n",
      "Batch: 193  Accuracy: 95.6 %\n",
      "Batch: 194  Accuracy: 98.0 %\n",
      "Batch: 195  Accuracy: 96.39999999999999 %\n",
      "Batch: 196  Accuracy: 92.0 %\n",
      "Batch: 197  Accuracy: 96.8 %\n",
      "Batch: 198  Accuracy: 92.80000000000001 %\n",
      "Batch: 199  Accuracy: 96.0 %\n",
      "Batch: 200  Accuracy: 95.6 %\n",
      "Batch: 201  Accuracy: 94.39999999999999 %\n",
      "Batch: 202  Accuracy: 96.0 %\n",
      "Batch: 203  Accuracy: 97.2 %\n",
      "Batch: 204  Accuracy: 97.6 %\n",
      "Batch: 205  Accuracy: 96.39999999999999 %\n",
      "Batch: 206  Accuracy: 97.6 %\n",
      "Batch: 207  Accuracy: 97.2 %\n",
      "Batch: 208  Accuracy: 94.8 %\n",
      "Batch: 209  Accuracy: 97.6 %\n",
      "Batch: 210  Accuracy: 98.0 %\n",
      "Batch: 211  Accuracy: 94.39999999999999 %\n",
      "Batch: 212  Accuracy: 97.6 %\n",
      "Batch: 213  Accuracy: 98.8 %\n",
      "Batch: 214  Accuracy: 97.2 %\n",
      "Batch: 215  Accuracy: 96.0 %\n",
      "Batch: 216  Accuracy: 97.6 %\n",
      "Batch: 217  Accuracy: 98.0 %\n",
      "Batch: 218  Accuracy: 98.0 %\n",
      "Batch: 219  Accuracy: 95.19999999999999 %\n",
      "Batch: 220  Accuracy: 98.4 %\n",
      "Batch: 221  Accuracy: 97.6 %\n",
      "Batch: 222  Accuracy: 96.0 %\n",
      "Batch: 223  Accuracy: 97.6 %\n",
      "Batch: 224  Accuracy: 97.2 %\n",
      "Batch: 225  Accuracy: 96.8 %\n",
      "Batch: 226  Accuracy: 96.0 %\n",
      "Batch: 227  Accuracy: 97.2 %\n",
      "Batch: 228  Accuracy: 97.6 %\n",
      "Batch: 229  Accuracy: 96.39999999999999 %\n",
      "Batch: 230  Accuracy: 95.19999999999999 %\n",
      "Batch: 231  Accuracy: 97.2 %\n",
      "Batch: 232  Accuracy: 96.8 %\n",
      "Batch: 233  Accuracy: 97.6 %\n",
      "Batch: 234  Accuracy: 98.4 %\n",
      "Batch: 235  Accuracy: 98.4 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 96.0 %\n",
      "Batch: 238  Accuracy: 96.39999999999999 %\n",
      "Batch: 239  Accuracy: 98.8 %\n",
      "Epoch: 1  Accuracy: 95.87 %  loss: 0.13344967709690056\n",
      "Batch: 0  Accuracy: 96.39999999999999 %\n",
      "Batch: 1  Accuracy: 97.2 %\n",
      "Batch: 2  Accuracy: 97.2 %\n",
      "Batch: 3  Accuracy: 96.39999999999999 %\n",
      "Batch: 4  Accuracy: 96.8 %\n",
      "Batch: 5  Accuracy: 97.2 %\n",
      "Batch: 6  Accuracy: 98.0 %\n",
      "Batch: 7  Accuracy: 98.0 %\n",
      "Batch: 8  Accuracy: 98.0 %\n",
      "Batch: 9  Accuracy: 98.4 %\n",
      "Batch: 10  Accuracy: 97.2 %\n",
      "Batch: 11  Accuracy: 98.8 %\n",
      "Batch: 12  Accuracy: 98.4 %\n",
      "Batch: 13  Accuracy: 98.4 %\n",
      "Batch: 14  Accuracy: 96.39999999999999 %\n",
      "Batch: 15  Accuracy: 98.0 %\n",
      "Batch: 16  Accuracy: 97.2 %\n",
      "Batch: 17  Accuracy: 95.6 %\n",
      "Batch: 18  Accuracy: 97.6 %\n",
      "Batch: 19  Accuracy: 97.6 %\n",
      "Batch: 20  Accuracy: 98.0 %\n",
      "Batch: 21  Accuracy: 98.0 %\n",
      "Batch: 22  Accuracy: 96.39999999999999 %\n",
      "Batch: 23  Accuracy: 94.39999999999999 %\n",
      "Batch: 24  Accuracy: 98.8 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 25  Accuracy: 95.19999999999999 %\n",
      "Batch: 26  Accuracy: 96.8 %\n",
      "Batch: 27  Accuracy: 94.39999999999999 %\n",
      "Batch: 28  Accuracy: 95.6 %\n",
      "Batch: 29  Accuracy: 96.8 %\n",
      "Batch: 30  Accuracy: 96.8 %\n",
      "Batch: 31  Accuracy: 98.0 %\n",
      "Batch: 32  Accuracy: 94.8 %\n",
      "Batch: 33  Accuracy: 93.2 %\n",
      "Batch: 34  Accuracy: 94.39999999999999 %\n",
      "Batch: 35  Accuracy: 92.4 %\n",
      "Batch: 36  Accuracy: 96.8 %\n",
      "Batch: 37  Accuracy: 94.8 %\n",
      "Batch: 38  Accuracy: 96.8 %\n",
      "Batch: 39  Accuracy: 99.2 %\n",
      "Batch: 40  Accuracy: 93.60000000000001 %\n",
      "Batch: 41  Accuracy: 96.8 %\n",
      "Batch: 42  Accuracy: 98.8 %\n",
      "Batch: 43  Accuracy: 96.39999999999999 %\n",
      "Batch: 44  Accuracy: 98.0 %\n",
      "Batch: 45  Accuracy: 99.2 %\n",
      "Batch: 46  Accuracy: 97.2 %\n",
      "Batch: 47  Accuracy: 96.8 %\n",
      "Batch: 48  Accuracy: 95.6 %\n",
      "Batch: 49  Accuracy: 97.6 %\n",
      "Batch: 50  Accuracy: 95.6 %\n",
      "Batch: 51  Accuracy: 95.19999999999999 %\n",
      "Batch: 52  Accuracy: 95.19999999999999 %\n",
      "Batch: 53  Accuracy: 97.2 %\n",
      "Batch: 54  Accuracy: 96.39999999999999 %\n",
      "Batch: 55  Accuracy: 96.8 %\n",
      "Batch: 56  Accuracy: 97.6 %\n",
      "Batch: 57  Accuracy: 97.2 %\n",
      "Batch: 58  Accuracy: 98.0 %\n",
      "Batch: 59  Accuracy: 95.6 %\n",
      "Batch: 60  Accuracy: 98.8 %\n",
      "Batch: 61  Accuracy: 96.39999999999999 %\n",
      "Batch: 62  Accuracy: 98.0 %\n",
      "Batch: 63  Accuracy: 94.0 %\n",
      "Batch: 64  Accuracy: 97.6 %\n",
      "Batch: 65  Accuracy: 98.4 %\n",
      "Batch: 66  Accuracy: 96.0 %\n",
      "Batch: 67  Accuracy: 97.6 %\n",
      "Batch: 68  Accuracy: 96.39999999999999 %\n",
      "Batch: 69  Accuracy: 97.6 %\n",
      "Batch: 70  Accuracy: 97.2 %\n",
      "Batch: 71  Accuracy: 96.0 %\n",
      "Batch: 72  Accuracy: 98.0 %\n",
      "Batch: 73  Accuracy: 97.2 %\n",
      "Batch: 74  Accuracy: 98.4 %\n",
      "Batch: 75  Accuracy: 99.2 %\n",
      "Batch: 76  Accuracy: 97.6 %\n",
      "Batch: 77  Accuracy: 96.0 %\n",
      "Batch: 78  Accuracy: 98.4 %\n",
      "Batch: 79  Accuracy: 98.8 %\n",
      "Batch: 80  Accuracy: 97.2 %\n",
      "Batch: 81  Accuracy: 99.2 %\n",
      "Batch: 82  Accuracy: 97.6 %\n",
      "Batch: 83  Accuracy: 95.6 %\n",
      "Batch: 84  Accuracy: 97.6 %\n",
      "Batch: 85  Accuracy: 98.4 %\n",
      "Batch: 86  Accuracy: 98.0 %\n",
      "Batch: 87  Accuracy: 95.19999999999999 %\n",
      "Batch: 88  Accuracy: 96.39999999999999 %\n",
      "Batch: 89  Accuracy: 97.6 %\n",
      "Batch: 90  Accuracy: 96.39999999999999 %\n",
      "Batch: 91  Accuracy: 97.6 %\n",
      "Batch: 92  Accuracy: 97.2 %\n",
      "Batch: 93  Accuracy: 99.2 %\n",
      "Batch: 94  Accuracy: 96.8 %\n",
      "Batch: 95  Accuracy: 98.4 %\n",
      "Batch: 96  Accuracy: 96.8 %\n",
      "Batch: 97  Accuracy: 98.4 %\n",
      "Batch: 98  Accuracy: 98.0 %\n",
      "Batch: 99  Accuracy: 97.6 %\n",
      "Batch: 100  Accuracy: 96.8 %\n",
      "Batch: 101  Accuracy: 96.39999999999999 %\n",
      "Batch: 102  Accuracy: 98.4 %\n",
      "Batch: 103  Accuracy: 96.8 %\n",
      "Batch: 104  Accuracy: 98.4 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 94.39999999999999 %\n",
      "Batch: 107  Accuracy: 98.0 %\n",
      "Batch: 108  Accuracy: 96.8 %\n",
      "Batch: 109  Accuracy: 98.0 %\n",
      "Batch: 110  Accuracy: 97.2 %\n",
      "Batch: 111  Accuracy: 97.2 %\n",
      "Batch: 112  Accuracy: 98.0 %\n",
      "Batch: 113  Accuracy: 95.6 %\n",
      "Batch: 114  Accuracy: 96.39999999999999 %\n",
      "Batch: 115  Accuracy: 97.6 %\n",
      "Batch: 116  Accuracy: 98.4 %\n",
      "Batch: 117  Accuracy: 96.8 %\n",
      "Batch: 118  Accuracy: 98.4 %\n",
      "Batch: 119  Accuracy: 98.0 %\n",
      "Batch: 120  Accuracy: 98.0 %\n",
      "Batch: 121  Accuracy: 98.0 %\n",
      "Batch: 122  Accuracy: 96.39999999999999 %\n",
      "Batch: 123  Accuracy: 97.2 %\n",
      "Batch: 124  Accuracy: 98.0 %\n",
      "Batch: 125  Accuracy: 95.6 %\n",
      "Batch: 126  Accuracy: 96.8 %\n",
      "Batch: 127  Accuracy: 96.8 %\n",
      "Batch: 128  Accuracy: 96.8 %\n",
      "Batch: 129  Accuracy: 94.8 %\n",
      "Batch: 130  Accuracy: 98.0 %\n",
      "Batch: 131  Accuracy: 98.0 %\n",
      "Batch: 132  Accuracy: 97.6 %\n",
      "Batch: 133  Accuracy: 98.0 %\n",
      "Batch: 134  Accuracy: 98.8 %\n",
      "Batch: 135  Accuracy: 99.2 %\n",
      "Batch: 136  Accuracy: 99.2 %\n",
      "Batch: 137  Accuracy: 98.4 %\n",
      "Batch: 138  Accuracy: 97.6 %\n",
      "Batch: 139  Accuracy: 95.19999999999999 %\n",
      "Batch: 140  Accuracy: 96.8 %\n",
      "Batch: 141  Accuracy: 98.0 %\n",
      "Batch: 142  Accuracy: 98.0 %\n",
      "Batch: 143  Accuracy: 97.6 %\n",
      "Batch: 144  Accuracy: 97.2 %\n",
      "Batch: 145  Accuracy: 97.2 %\n",
      "Batch: 146  Accuracy: 97.6 %\n",
      "Batch: 147  Accuracy: 97.6 %\n",
      "Batch: 148  Accuracy: 96.39999999999999 %\n",
      "Batch: 149  Accuracy: 95.6 %\n",
      "Batch: 150  Accuracy: 96.0 %\n",
      "Batch: 151  Accuracy: 97.6 %\n",
      "Batch: 152  Accuracy: 98.8 %\n",
      "Batch: 153  Accuracy: 98.0 %\n",
      "Batch: 154  Accuracy: 97.2 %\n",
      "Batch: 155  Accuracy: 98.0 %\n",
      "Batch: 156  Accuracy: 98.8 %\n",
      "Batch: 157  Accuracy: 94.8 %\n",
      "Batch: 158  Accuracy: 96.8 %\n",
      "Batch: 159  Accuracy: 97.6 %\n",
      "Batch: 160  Accuracy: 98.0 %\n",
      "Batch: 161  Accuracy: 98.8 %\n",
      "Batch: 162  Accuracy: 98.4 %\n",
      "Batch: 163  Accuracy: 97.6 %\n",
      "Batch: 164  Accuracy: 97.6 %\n",
      "Batch: 165  Accuracy: 94.8 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 94.8 %\n",
      "Batch: 169  Accuracy: 95.6 %\n",
      "Batch: 170  Accuracy: 96.0 %\n",
      "Batch: 171  Accuracy: 96.0 %\n",
      "Batch: 172  Accuracy: 97.2 %\n",
      "Batch: 173  Accuracy: 97.2 %\n",
      "Batch: 174  Accuracy: 97.2 %\n",
      "Batch: 175  Accuracy: 97.6 %\n",
      "Batch: 176  Accuracy: 97.2 %\n",
      "Batch: 177  Accuracy: 96.0 %\n",
      "Batch: 178  Accuracy: 98.4 %\n",
      "Batch: 179  Accuracy: 96.39999999999999 %\n",
      "Batch: 180  Accuracy: 97.2 %\n",
      "Batch: 181  Accuracy: 97.2 %\n",
      "Batch: 182  Accuracy: 97.2 %\n",
      "Batch: 183  Accuracy: 96.8 %\n",
      "Batch: 184  Accuracy: 96.8 %\n",
      "Batch: 185  Accuracy: 94.8 %\n",
      "Batch: 186  Accuracy: 96.8 %\n",
      "Batch: 187  Accuracy: 98.4 %\n",
      "Batch: 188  Accuracy: 96.39999999999999 %\n",
      "Batch: 189  Accuracy: 96.8 %\n",
      "Batch: 190  Accuracy: 95.6 %\n",
      "Batch: 191  Accuracy: 96.39999999999999 %\n",
      "Batch: 192  Accuracy: 97.2 %\n",
      "Batch: 193  Accuracy: 96.39999999999999 %\n",
      "Batch: 194  Accuracy: 98.8 %\n",
      "Batch: 195  Accuracy: 97.6 %\n",
      "Batch: 196  Accuracy: 95.6 %\n",
      "Batch: 197  Accuracy: 98.4 %\n",
      "Batch: 198  Accuracy: 94.8 %\n",
      "Batch: 199  Accuracy: 95.19999999999999 %\n",
      "Batch: 200  Accuracy: 97.6 %\n",
      "Batch: 201  Accuracy: 95.6 %\n",
      "Batch: 202  Accuracy: 96.8 %\n",
      "Batch: 203  Accuracy: 97.6 %\n",
      "Batch: 204  Accuracy: 98.0 %\n",
      "Batch: 205  Accuracy: 96.8 %\n",
      "Batch: 206  Accuracy: 98.4 %\n",
      "Batch: 207  Accuracy: 97.6 %\n",
      "Batch: 208  Accuracy: 96.8 %\n",
      "Batch: 209  Accuracy: 98.0 %\n",
      "Batch: 210  Accuracy: 98.8 %\n",
      "Batch: 211  Accuracy: 96.0 %\n",
      "Batch: 212  Accuracy: 98.4 %\n",
      "Batch: 213  Accuracy: 99.2 %\n",
      "Batch: 214  Accuracy: 98.4 %\n",
      "Batch: 215  Accuracy: 96.8 %\n",
      "Batch: 216  Accuracy: 98.0 %\n",
      "Batch: 217  Accuracy: 98.4 %\n",
      "Batch: 218  Accuracy: 98.4 %\n",
      "Batch: 219  Accuracy: 96.8 %\n",
      "Batch: 220  Accuracy: 99.2 %\n",
      "Batch: 221  Accuracy: 97.6 %\n",
      "Batch: 222  Accuracy: 96.39999999999999 %\n",
      "Batch: 223  Accuracy: 99.2 %\n",
      "Batch: 224  Accuracy: 98.4 %\n",
      "Batch: 225  Accuracy: 98.4 %\n",
      "Batch: 226  Accuracy: 96.8 %\n",
      "Batch: 227  Accuracy: 97.6 %\n",
      "Batch: 228  Accuracy: 98.0 %\n",
      "Batch: 229  Accuracy: 97.2 %\n",
      "Batch: 230  Accuracy: 96.8 %\n",
      "Batch: 231  Accuracy: 98.4 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 98.8 %\n",
      "Batch: 234  Accuracy: 99.2 %\n",
      "Batch: 235  Accuracy: 98.4 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 96.39999999999999 %\n",
      "Batch: 238  Accuracy: 96.39999999999999 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 2  Accuracy: 97.21499999999999 %  loss: 0.09150277670475228\n",
      "Batch: 0  Accuracy: 96.0 %\n",
      "Batch: 1  Accuracy: 98.0 %\n",
      "Batch: 2  Accuracy: 98.0 %\n",
      "Batch: 3  Accuracy: 96.39999999999999 %\n",
      "Batch: 4  Accuracy: 98.0 %\n",
      "Batch: 5  Accuracy: 96.8 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 97.2 %\n",
      "Batch: 8  Accuracy: 98.0 %\n",
      "Batch: 9  Accuracy: 98.8 %\n",
      "Batch: 10  Accuracy: 97.6 %\n",
      "Batch: 11  Accuracy: 98.8 %\n",
      "Batch: 12  Accuracy: 98.4 %\n",
      "Batch: 13  Accuracy: 98.8 %\n",
      "Batch: 14  Accuracy: 97.2 %\n",
      "Batch: 15  Accuracy: 99.2 %\n",
      "Batch: 16  Accuracy: 97.2 %\n",
      "Batch: 17  Accuracy: 96.39999999999999 %\n",
      "Batch: 18  Accuracy: 97.6 %\n",
      "Batch: 19  Accuracy: 97.6 %\n",
      "Batch: 20  Accuracy: 98.0 %\n",
      "Batch: 21  Accuracy: 98.8 %\n",
      "Batch: 22  Accuracy: 97.2 %\n",
      "Batch: 23  Accuracy: 96.8 %\n",
      "Batch: 24  Accuracy: 98.8 %\n",
      "Batch: 25  Accuracy: 96.8 %\n",
      "Batch: 26  Accuracy: 97.6 %\n",
      "Batch: 27  Accuracy: 94.39999999999999 %\n",
      "Batch: 28  Accuracy: 96.8 %\n",
      "Batch: 29  Accuracy: 97.6 %\n",
      "Batch: 30  Accuracy: 96.8 %\n",
      "Batch: 31  Accuracy: 98.8 %\n",
      "Batch: 32  Accuracy: 96.0 %\n",
      "Batch: 33  Accuracy: 95.19999999999999 %\n",
      "Batch: 34  Accuracy: 95.6 %\n",
      "Batch: 35  Accuracy: 94.39999999999999 %\n",
      "Batch: 36  Accuracy: 97.6 %\n",
      "Batch: 37  Accuracy: 94.8 %\n",
      "Batch: 38  Accuracy: 98.0 %\n",
      "Batch: 39  Accuracy: 100.0 %\n",
      "Batch: 40  Accuracy: 94.8 %\n",
      "Batch: 41  Accuracy: 97.2 %\n",
      "Batch: 42  Accuracy: 99.2 %\n",
      "Batch: 43  Accuracy: 97.2 %\n",
      "Batch: 44  Accuracy: 98.4 %\n",
      "Batch: 45  Accuracy: 99.2 %\n",
      "Batch: 46  Accuracy: 98.8 %\n",
      "Batch: 47  Accuracy: 97.6 %\n",
      "Batch: 48  Accuracy: 97.2 %\n",
      "Batch: 49  Accuracy: 97.6 %\n",
      "Batch: 50  Accuracy: 96.8 %\n",
      "Batch: 51  Accuracy: 96.8 %\n",
      "Batch: 52  Accuracy: 96.39999999999999 %\n",
      "Batch: 53  Accuracy: 98.0 %\n",
      "Batch: 54  Accuracy: 97.2 %\n",
      "Batch: 55  Accuracy: 97.2 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 56  Accuracy: 98.0 %\n",
      "Batch: 57  Accuracy: 97.6 %\n",
      "Batch: 58  Accuracy: 98.8 %\n",
      "Batch: 59  Accuracy: 97.2 %\n",
      "Batch: 60  Accuracy: 98.8 %\n",
      "Batch: 61  Accuracy: 97.2 %\n",
      "Batch: 62  Accuracy: 98.8 %\n",
      "Batch: 63  Accuracy: 94.8 %\n",
      "Batch: 64  Accuracy: 98.4 %\n",
      "Batch: 65  Accuracy: 98.8 %\n",
      "Batch: 66  Accuracy: 96.39999999999999 %\n",
      "Batch: 67  Accuracy: 98.0 %\n",
      "Batch: 68  Accuracy: 96.39999999999999 %\n",
      "Batch: 69  Accuracy: 98.4 %\n",
      "Batch: 70  Accuracy: 97.6 %\n",
      "Batch: 71  Accuracy: 96.8 %\n",
      "Batch: 72  Accuracy: 97.6 %\n",
      "Batch: 73  Accuracy: 98.0 %\n",
      "Batch: 74  Accuracy: 98.4 %\n",
      "Batch: 75  Accuracy: 99.2 %\n",
      "Batch: 76  Accuracy: 98.0 %\n",
      "Batch: 77  Accuracy: 96.39999999999999 %\n",
      "Batch: 78  Accuracy: 98.0 %\n",
      "Batch: 79  Accuracy: 99.2 %\n",
      "Batch: 80  Accuracy: 97.6 %\n",
      "Batch: 81  Accuracy: 99.6 %\n",
      "Batch: 82  Accuracy: 97.2 %\n",
      "Batch: 83  Accuracy: 97.2 %\n",
      "Batch: 84  Accuracy: 98.0 %\n",
      "Batch: 85  Accuracy: 98.4 %\n",
      "Batch: 86  Accuracy: 98.4 %\n",
      "Batch: 87  Accuracy: 96.0 %\n",
      "Batch: 88  Accuracy: 97.2 %\n",
      "Batch: 89  Accuracy: 98.8 %\n",
      "Batch: 90  Accuracy: 97.6 %\n",
      "Batch: 91  Accuracy: 98.0 %\n",
      "Batch: 92  Accuracy: 98.0 %\n",
      "Batch: 93  Accuracy: 99.6 %\n",
      "Batch: 94  Accuracy: 98.0 %\n",
      "Batch: 95  Accuracy: 98.4 %\n",
      "Batch: 96  Accuracy: 98.0 %\n",
      "Batch: 97  Accuracy: 98.8 %\n",
      "Batch: 98  Accuracy: 98.4 %\n",
      "Batch: 99  Accuracy: 98.4 %\n",
      "Batch: 100  Accuracy: 97.2 %\n",
      "Batch: 101  Accuracy: 97.2 %\n",
      "Batch: 102  Accuracy: 98.8 %\n",
      "Batch: 103  Accuracy: 97.6 %\n",
      "Batch: 104  Accuracy: 98.4 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 94.8 %\n",
      "Batch: 107  Accuracy: 98.8 %\n",
      "Batch: 108  Accuracy: 98.0 %\n",
      "Batch: 109  Accuracy: 98.4 %\n",
      "Batch: 110  Accuracy: 97.2 %\n",
      "Batch: 111  Accuracy: 98.0 %\n",
      "Batch: 112  Accuracy: 98.4 %\n",
      "Batch: 113  Accuracy: 96.8 %\n",
      "Batch: 114  Accuracy: 97.2 %\n",
      "Batch: 115  Accuracy: 98.8 %\n",
      "Batch: 116  Accuracy: 97.6 %\n",
      "Batch: 117  Accuracy: 98.0 %\n",
      "Batch: 118  Accuracy: 98.4 %\n",
      "Batch: 119  Accuracy: 98.4 %\n",
      "Batch: 120  Accuracy: 97.6 %\n",
      "Batch: 121  Accuracy: 99.2 %\n",
      "Batch: 122  Accuracy: 97.6 %\n",
      "Batch: 123  Accuracy: 97.6 %\n",
      "Batch: 124  Accuracy: 98.4 %\n",
      "Batch: 125  Accuracy: 96.0 %\n",
      "Batch: 126  Accuracy: 96.0 %\n",
      "Batch: 127  Accuracy: 97.2 %\n",
      "Batch: 128  Accuracy: 97.6 %\n",
      "Batch: 129  Accuracy: 96.0 %\n",
      "Batch: 130  Accuracy: 98.4 %\n",
      "Batch: 131  Accuracy: 98.0 %\n",
      "Batch: 132  Accuracy: 98.0 %\n",
      "Batch: 133  Accuracy: 97.6 %\n",
      "Batch: 134  Accuracy: 99.2 %\n",
      "Batch: 135  Accuracy: 99.2 %\n",
      "Batch: 136  Accuracy: 99.2 %\n",
      "Batch: 137  Accuracy: 98.4 %\n",
      "Batch: 138  Accuracy: 97.6 %\n",
      "Batch: 139  Accuracy: 96.0 %\n",
      "Batch: 140  Accuracy: 97.6 %\n",
      "Batch: 141  Accuracy: 98.0 %\n",
      "Batch: 142  Accuracy: 98.0 %\n",
      "Batch: 143  Accuracy: 98.8 %\n",
      "Batch: 144  Accuracy: 96.8 %\n",
      "Batch: 145  Accuracy: 97.2 %\n",
      "Batch: 146  Accuracy: 97.6 %\n",
      "Batch: 147  Accuracy: 97.6 %\n",
      "Batch: 148  Accuracy: 97.2 %\n",
      "Batch: 149  Accuracy: 96.8 %\n",
      "Batch: 150  Accuracy: 97.6 %\n",
      "Batch: 151  Accuracy: 98.0 %\n",
      "Batch: 152  Accuracy: 98.8 %\n",
      "Batch: 153  Accuracy: 98.0 %\n",
      "Batch: 154  Accuracy: 98.0 %\n",
      "Batch: 155  Accuracy: 98.0 %\n",
      "Batch: 156  Accuracy: 98.8 %\n",
      "Batch: 157  Accuracy: 96.0 %\n",
      "Batch: 158  Accuracy: 97.2 %\n",
      "Batch: 159  Accuracy: 99.2 %\n",
      "Batch: 160  Accuracy: 98.0 %\n",
      "Batch: 161  Accuracy: 98.8 %\n",
      "Batch: 162  Accuracy: 98.8 %\n",
      "Batch: 163  Accuracy: 98.0 %\n",
      "Batch: 164  Accuracy: 98.0 %\n",
      "Batch: 165  Accuracy: 96.0 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 96.39999999999999 %\n",
      "Batch: 169  Accuracy: 97.2 %\n",
      "Batch: 170  Accuracy: 97.6 %\n",
      "Batch: 171  Accuracy: 97.2 %\n",
      "Batch: 172  Accuracy: 98.0 %\n",
      "Batch: 173  Accuracy: 97.2 %\n",
      "Batch: 174  Accuracy: 97.6 %\n",
      "Batch: 175  Accuracy: 98.0 %\n",
      "Batch: 176  Accuracy: 98.4 %\n",
      "Batch: 177  Accuracy: 97.2 %\n",
      "Batch: 178  Accuracy: 98.4 %\n",
      "Batch: 179  Accuracy: 98.0 %\n",
      "Batch: 180  Accuracy: 98.0 %\n",
      "Batch: 181  Accuracy: 97.2 %\n",
      "Batch: 182  Accuracy: 97.2 %\n",
      "Batch: 183  Accuracy: 97.6 %\n",
      "Batch: 184  Accuracy: 96.8 %\n",
      "Batch: 185  Accuracy: 95.6 %\n",
      "Batch: 186  Accuracy: 97.2 %\n",
      "Batch: 187  Accuracy: 98.8 %\n",
      "Batch: 188  Accuracy: 97.6 %\n",
      "Batch: 189  Accuracy: 96.8 %\n",
      "Batch: 190  Accuracy: 96.0 %\n",
      "Batch: 191  Accuracy: 97.6 %\n",
      "Batch: 192  Accuracy: 98.0 %\n",
      "Batch: 193  Accuracy: 96.39999999999999 %\n",
      "Batch: 194  Accuracy: 98.4 %\n",
      "Batch: 195  Accuracy: 98.8 %\n",
      "Batch: 196  Accuracy: 96.39999999999999 %\n",
      "Batch: 197  Accuracy: 99.2 %\n",
      "Batch: 198  Accuracy: 95.6 %\n",
      "Batch: 199  Accuracy: 96.39999999999999 %\n",
      "Batch: 200  Accuracy: 97.6 %\n",
      "Batch: 201  Accuracy: 96.8 %\n",
      "Batch: 202  Accuracy: 98.0 %\n",
      "Batch: 203  Accuracy: 98.0 %\n",
      "Batch: 204  Accuracy: 98.0 %\n",
      "Batch: 205  Accuracy: 97.2 %\n",
      "Batch: 206  Accuracy: 98.8 %\n",
      "Batch: 207  Accuracy: 97.2 %\n",
      "Batch: 208  Accuracy: 97.2 %\n",
      "Batch: 209  Accuracy: 98.4 %\n",
      "Batch: 210  Accuracy: 99.6 %\n",
      "Batch: 211  Accuracy: 96.0 %\n",
      "Batch: 212  Accuracy: 98.8 %\n",
      "Batch: 213  Accuracy: 99.6 %\n",
      "Batch: 214  Accuracy: 98.4 %\n",
      "Batch: 215  Accuracy: 97.6 %\n",
      "Batch: 216  Accuracy: 98.4 %\n",
      "Batch: 217  Accuracy: 98.8 %\n",
      "Batch: 218  Accuracy: 98.4 %\n",
      "Batch: 219  Accuracy: 96.8 %\n",
      "Batch: 220  Accuracy: 99.2 %\n",
      "Batch: 221  Accuracy: 98.4 %\n",
      "Batch: 222  Accuracy: 97.6 %\n",
      "Batch: 223  Accuracy: 98.8 %\n",
      "Batch: 224  Accuracy: 98.8 %\n",
      "Batch: 225  Accuracy: 98.4 %\n",
      "Batch: 226  Accuracy: 97.2 %\n",
      "Batch: 227  Accuracy: 98.0 %\n",
      "Batch: 228  Accuracy: 98.0 %\n",
      "Batch: 229  Accuracy: 97.2 %\n",
      "Batch: 230  Accuracy: 96.8 %\n",
      "Batch: 231  Accuracy: 98.8 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 98.8 %\n",
      "Batch: 234  Accuracy: 99.2 %\n",
      "Batch: 235  Accuracy: 98.0 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 98.0 %\n",
      "Batch: 238  Accuracy: 96.39999999999999 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 3  Accuracy: 97.73333333333333 %  loss: 0.07505937196212667\n",
      "Batch: 0  Accuracy: 97.2 %\n",
      "Batch: 1  Accuracy: 97.6 %\n",
      "Batch: 2  Accuracy: 98.4 %\n",
      "Batch: 3  Accuracy: 97.2 %\n",
      "Batch: 4  Accuracy: 98.0 %\n",
      "Batch: 5  Accuracy: 97.6 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 98.8 %\n",
      "Batch: 8  Accuracy: 99.2 %\n",
      "Batch: 9  Accuracy: 99.2 %\n",
      "Batch: 10  Accuracy: 97.6 %\n",
      "Batch: 11  Accuracy: 99.2 %\n",
      "Batch: 12  Accuracy: 98.8 %\n",
      "Batch: 13  Accuracy: 98.8 %\n",
      "Batch: 14  Accuracy: 97.6 %\n",
      "Batch: 15  Accuracy: 99.2 %\n",
      "Batch: 16  Accuracy: 98.4 %\n",
      "Batch: 17  Accuracy: 96.39999999999999 %\n",
      "Batch: 18  Accuracy: 98.0 %\n",
      "Batch: 19  Accuracy: 97.6 %\n",
      "Batch: 20  Accuracy: 98.0 %\n",
      "Batch: 21  Accuracy: 98.8 %\n",
      "Batch: 22  Accuracy: 97.2 %\n",
      "Batch: 23  Accuracy: 96.8 %\n",
      "Batch: 24  Accuracy: 98.8 %\n",
      "Batch: 25  Accuracy: 96.0 %\n",
      "Batch: 26  Accuracy: 98.4 %\n",
      "Batch: 27  Accuracy: 95.6 %\n",
      "Batch: 28  Accuracy: 97.2 %\n",
      "Batch: 29  Accuracy: 98.0 %\n",
      "Batch: 30  Accuracy: 98.4 %\n",
      "Batch: 31  Accuracy: 98.4 %\n",
      "Batch: 32  Accuracy: 96.39999999999999 %\n",
      "Batch: 33  Accuracy: 96.0 %\n",
      "Batch: 34  Accuracy: 96.39999999999999 %\n",
      "Batch: 35  Accuracy: 94.8 %\n",
      "Batch: 36  Accuracy: 97.6 %\n",
      "Batch: 37  Accuracy: 95.6 %\n",
      "Batch: 38  Accuracy: 98.4 %\n",
      "Batch: 39  Accuracy: 100.0 %\n",
      "Batch: 40  Accuracy: 95.6 %\n",
      "Batch: 41  Accuracy: 97.6 %\n",
      "Batch: 42  Accuracy: 99.2 %\n",
      "Batch: 43  Accuracy: 98.0 %\n",
      "Batch: 44  Accuracy: 98.8 %\n",
      "Batch: 45  Accuracy: 100.0 %\n",
      "Batch: 46  Accuracy: 99.2 %\n",
      "Batch: 47  Accuracy: 98.0 %\n",
      "Batch: 48  Accuracy: 98.0 %\n",
      "Batch: 49  Accuracy: 98.4 %\n",
      "Batch: 50  Accuracy: 96.8 %\n",
      "Batch: 51  Accuracy: 97.2 %\n",
      "Batch: 52  Accuracy: 96.0 %\n",
      "Batch: 53  Accuracy: 98.8 %\n",
      "Batch: 54  Accuracy: 98.0 %\n",
      "Batch: 55  Accuracy: 97.2 %\n",
      "Batch: 56  Accuracy: 98.4 %\n",
      "Batch: 57  Accuracy: 98.0 %\n",
      "Batch: 58  Accuracy: 98.8 %\n",
      "Batch: 59  Accuracy: 98.4 %\n",
      "Batch: 60  Accuracy: 98.8 %\n",
      "Batch: 61  Accuracy: 96.8 %\n",
      "Batch: 62  Accuracy: 99.2 %\n",
      "Batch: 63  Accuracy: 95.19999999999999 %\n",
      "Batch: 64  Accuracy: 98.0 %\n",
      "Batch: 65  Accuracy: 98.8 %\n",
      "Batch: 66  Accuracy: 97.2 %\n",
      "Batch: 67  Accuracy: 98.0 %\n",
      "Batch: 68  Accuracy: 97.2 %\n",
      "Batch: 69  Accuracy: 98.8 %\n",
      "Batch: 70  Accuracy: 97.6 %\n",
      "Batch: 71  Accuracy: 97.2 %\n",
      "Batch: 72  Accuracy: 97.6 %\n",
      "Batch: 73  Accuracy: 98.0 %\n",
      "Batch: 74  Accuracy: 98.8 %\n",
      "Batch: 75  Accuracy: 99.2 %\n",
      "Batch: 76  Accuracy: 98.4 %\n",
      "Batch: 77  Accuracy: 97.6 %\n",
      "Batch: 78  Accuracy: 99.2 %\n",
      "Batch: 79  Accuracy: 99.2 %\n",
      "Batch: 80  Accuracy: 97.6 %\n",
      "Batch: 81  Accuracy: 99.6 %\n",
      "Batch: 82  Accuracy: 97.2 %\n",
      "Batch: 83  Accuracy: 97.2 %\n",
      "Batch: 84  Accuracy: 98.4 %\n",
      "Batch: 85  Accuracy: 98.4 %\n",
      "Batch: 86  Accuracy: 98.4 %\n",
      "Batch: 87  Accuracy: 97.6 %\n",
      "Batch: 88  Accuracy: 97.6 %\n",
      "Batch: 89  Accuracy: 99.2 %\n",
      "Batch: 90  Accuracy: 98.0 %\n",
      "Batch: 91  Accuracy: 98.0 %\n",
      "Batch: 92  Accuracy: 98.0 %\n",
      "Batch: 93  Accuracy: 99.6 %\n",
      "Batch: 94  Accuracy: 98.8 %\n",
      "Batch: 95  Accuracy: 98.4 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 96  Accuracy: 98.0 %\n",
      "Batch: 97  Accuracy: 98.8 %\n",
      "Batch: 98  Accuracy: 99.2 %\n",
      "Batch: 99  Accuracy: 98.8 %\n",
      "Batch: 100  Accuracy: 97.6 %\n",
      "Batch: 101  Accuracy: 97.2 %\n",
      "Batch: 102  Accuracy: 98.8 %\n",
      "Batch: 103  Accuracy: 97.6 %\n",
      "Batch: 104  Accuracy: 98.4 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 94.8 %\n",
      "Batch: 107  Accuracy: 98.8 %\n",
      "Batch: 108  Accuracy: 97.6 %\n",
      "Batch: 109  Accuracy: 98.4 %\n",
      "Batch: 110  Accuracy: 97.2 %\n",
      "Batch: 111  Accuracy: 98.8 %\n",
      "Batch: 112  Accuracy: 98.4 %\n",
      "Batch: 113  Accuracy: 98.0 %\n",
      "Batch: 114  Accuracy: 97.2 %\n",
      "Batch: 115  Accuracy: 98.8 %\n",
      "Batch: 116  Accuracy: 98.0 %\n",
      "Batch: 117  Accuracy: 98.0 %\n",
      "Batch: 118  Accuracy: 99.2 %\n",
      "Batch: 119  Accuracy: 98.8 %\n",
      "Batch: 120  Accuracy: 97.6 %\n",
      "Batch: 121  Accuracy: 99.2 %\n",
      "Batch: 122  Accuracy: 98.0 %\n",
      "Batch: 123  Accuracy: 97.2 %\n",
      "Batch: 124  Accuracy: 98.4 %\n",
      "Batch: 125  Accuracy: 97.2 %\n",
      "Batch: 126  Accuracy: 96.0 %\n",
      "Batch: 127  Accuracy: 97.2 %\n",
      "Batch: 128  Accuracy: 98.0 %\n",
      "Batch: 129  Accuracy: 96.8 %\n",
      "Batch: 130  Accuracy: 98.8 %\n",
      "Batch: 131  Accuracy: 98.0 %\n",
      "Batch: 132  Accuracy: 97.6 %\n",
      "Batch: 133  Accuracy: 98.0 %\n",
      "Batch: 134  Accuracy: 99.2 %\n",
      "Batch: 135  Accuracy: 99.2 %\n",
      "Batch: 136  Accuracy: 99.2 %\n",
      "Batch: 137  Accuracy: 98.4 %\n",
      "Batch: 138  Accuracy: 97.2 %\n",
      "Batch: 139  Accuracy: 96.39999999999999 %\n",
      "Batch: 140  Accuracy: 98.0 %\n",
      "Batch: 141  Accuracy: 98.0 %\n",
      "Batch: 142  Accuracy: 98.4 %\n",
      "Batch: 143  Accuracy: 98.8 %\n",
      "Batch: 144  Accuracy: 97.2 %\n",
      "Batch: 145  Accuracy: 97.2 %\n",
      "Batch: 146  Accuracy: 97.6 %\n",
      "Batch: 147  Accuracy: 97.6 %\n",
      "Batch: 148  Accuracy: 97.6 %\n",
      "Batch: 149  Accuracy: 97.6 %\n",
      "Batch: 150  Accuracy: 98.4 %\n",
      "Batch: 151  Accuracy: 97.6 %\n",
      "Batch: 152  Accuracy: 99.2 %\n",
      "Batch: 153  Accuracy: 98.0 %\n",
      "Batch: 154  Accuracy: 98.0 %\n",
      "Batch: 155  Accuracy: 98.0 %\n",
      "Batch: 156  Accuracy: 99.2 %\n",
      "Batch: 157  Accuracy: 95.6 %\n",
      "Batch: 158  Accuracy: 97.6 %\n",
      "Batch: 159  Accuracy: 99.2 %\n",
      "Batch: 160  Accuracy: 98.8 %\n",
      "Batch: 161  Accuracy: 98.8 %\n",
      "Batch: 162  Accuracy: 99.2 %\n",
      "Batch: 163  Accuracy: 98.0 %\n",
      "Batch: 164  Accuracy: 98.4 %\n",
      "Batch: 165  Accuracy: 96.8 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 96.8 %\n",
      "Batch: 169  Accuracy: 97.2 %\n",
      "Batch: 170  Accuracy: 98.0 %\n",
      "Batch: 171  Accuracy: 97.6 %\n",
      "Batch: 172  Accuracy: 98.0 %\n",
      "Batch: 173  Accuracy: 98.0 %\n",
      "Batch: 174  Accuracy: 98.0 %\n",
      "Batch: 175  Accuracy: 98.0 %\n",
      "Batch: 176  Accuracy: 98.4 %\n",
      "Batch: 177  Accuracy: 97.2 %\n",
      "Batch: 178  Accuracy: 98.8 %\n",
      "Batch: 179  Accuracy: 98.4 %\n",
      "Batch: 180  Accuracy: 98.4 %\n",
      "Batch: 181  Accuracy: 97.2 %\n",
      "Batch: 182  Accuracy: 97.2 %\n",
      "Batch: 183  Accuracy: 97.6 %\n",
      "Batch: 184  Accuracy: 96.8 %\n",
      "Batch: 185  Accuracy: 96.0 %\n",
      "Batch: 186  Accuracy: 97.6 %\n",
      "Batch: 187  Accuracy: 98.8 %\n",
      "Batch: 188  Accuracy: 97.6 %\n",
      "Batch: 189  Accuracy: 98.0 %\n",
      "Batch: 190  Accuracy: 96.39999999999999 %\n",
      "Batch: 191  Accuracy: 98.4 %\n",
      "Batch: 192  Accuracy: 98.0 %\n",
      "Batch: 193  Accuracy: 97.6 %\n",
      "Batch: 194  Accuracy: 99.2 %\n",
      "Batch: 195  Accuracy: 98.4 %\n",
      "Batch: 196  Accuracy: 96.8 %\n",
      "Batch: 197  Accuracy: 99.2 %\n",
      "Batch: 198  Accuracy: 96.0 %\n",
      "Batch: 199  Accuracy: 96.8 %\n",
      "Batch: 200  Accuracy: 98.0 %\n",
      "Batch: 201  Accuracy: 97.2 %\n",
      "Batch: 202  Accuracy: 98.4 %\n",
      "Batch: 203  Accuracy: 97.6 %\n",
      "Batch: 204  Accuracy: 98.4 %\n",
      "Batch: 205  Accuracy: 97.2 %\n",
      "Batch: 206  Accuracy: 99.2 %\n",
      "Batch: 207  Accuracy: 97.6 %\n",
      "Batch: 208  Accuracy: 97.6 %\n",
      "Batch: 209  Accuracy: 98.4 %\n",
      "Batch: 210  Accuracy: 99.6 %\n",
      "Batch: 211  Accuracy: 96.8 %\n",
      "Batch: 212  Accuracy: 98.4 %\n",
      "Batch: 213  Accuracy: 99.6 %\n",
      "Batch: 214  Accuracy: 98.4 %\n",
      "Batch: 215  Accuracy: 98.0 %\n",
      "Batch: 216  Accuracy: 98.8 %\n",
      "Batch: 217  Accuracy: 98.8 %\n",
      "Batch: 218  Accuracy: 98.8 %\n",
      "Batch: 219  Accuracy: 97.6 %\n",
      "Batch: 220  Accuracy: 99.6 %\n",
      "Batch: 221  Accuracy: 98.4 %\n",
      "Batch: 222  Accuracy: 98.4 %\n",
      "Batch: 223  Accuracy: 98.8 %\n",
      "Batch: 224  Accuracy: 98.8 %\n",
      "Batch: 225  Accuracy: 98.4 %\n",
      "Batch: 226  Accuracy: 98.0 %\n",
      "Batch: 227  Accuracy: 98.0 %\n",
      "Batch: 228  Accuracy: 98.4 %\n",
      "Batch: 229  Accuracy: 97.2 %\n",
      "Batch: 230  Accuracy: 97.2 %\n",
      "Batch: 231  Accuracy: 99.2 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 99.2 %\n",
      "Batch: 234  Accuracy: 99.6 %\n",
      "Batch: 235  Accuracy: 98.4 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 99.2 %\n",
      "Batch: 238  Accuracy: 96.39999999999999 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 4  Accuracy: 98.01666666666667 %  loss: 0.06536177017471603\n",
      "Batch: 0  Accuracy: 97.6 %\n",
      "Batch: 1  Accuracy: 98.0 %\n",
      "Batch: 2  Accuracy: 98.8 %\n",
      "Batch: 3  Accuracy: 97.2 %\n",
      "Batch: 4  Accuracy: 98.0 %\n",
      "Batch: 5  Accuracy: 97.6 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 98.4 %\n",
      "Batch: 8  Accuracy: 99.2 %\n",
      "Batch: 9  Accuracy: 99.2 %\n",
      "Batch: 10  Accuracy: 97.6 %\n",
      "Batch: 11  Accuracy: 99.6 %\n",
      "Batch: 12  Accuracy: 99.2 %\n",
      "Batch: 13  Accuracy: 98.8 %\n",
      "Batch: 14  Accuracy: 98.0 %\n",
      "Batch: 15  Accuracy: 98.4 %\n",
      "Batch: 16  Accuracy: 98.8 %\n",
      "Batch: 17  Accuracy: 96.8 %\n",
      "Batch: 18  Accuracy: 98.4 %\n",
      "Batch: 19  Accuracy: 98.0 %\n",
      "Batch: 20  Accuracy: 98.4 %\n",
      "Batch: 21  Accuracy: 99.2 %\n",
      "Batch: 22  Accuracy: 97.2 %\n",
      "Batch: 23  Accuracy: 97.2 %\n",
      "Batch: 24  Accuracy: 98.8 %\n",
      "Batch: 25  Accuracy: 96.39999999999999 %\n",
      "Batch: 26  Accuracy: 98.8 %\n",
      "Batch: 27  Accuracy: 96.0 %\n",
      "Batch: 28  Accuracy: 97.2 %\n",
      "Batch: 29  Accuracy: 98.0 %\n",
      "Batch: 30  Accuracy: 98.4 %\n",
      "Batch: 31  Accuracy: 98.8 %\n",
      "Batch: 32  Accuracy: 96.39999999999999 %\n",
      "Batch: 33  Accuracy: 96.39999999999999 %\n",
      "Batch: 34  Accuracy: 96.0 %\n",
      "Batch: 35  Accuracy: 96.0 %\n",
      "Batch: 36  Accuracy: 97.6 %\n",
      "Batch: 37  Accuracy: 96.0 %\n",
      "Batch: 38  Accuracy: 98.4 %\n",
      "Batch: 39  Accuracy: 100.0 %\n",
      "Batch: 40  Accuracy: 95.6 %\n",
      "Batch: 41  Accuracy: 97.6 %\n",
      "Batch: 42  Accuracy: 99.6 %\n",
      "Batch: 43  Accuracy: 98.8 %\n",
      "Batch: 44  Accuracy: 99.6 %\n",
      "Batch: 45  Accuracy: 100.0 %\n",
      "Batch: 46  Accuracy: 99.2 %\n",
      "Batch: 47  Accuracy: 98.0 %\n",
      "Batch: 48  Accuracy: 97.6 %\n",
      "Batch: 49  Accuracy: 98.4 %\n",
      "Batch: 50  Accuracy: 96.8 %\n",
      "Batch: 51  Accuracy: 98.0 %\n",
      "Batch: 52  Accuracy: 96.0 %\n",
      "Batch: 53  Accuracy: 99.2 %\n",
      "Batch: 54  Accuracy: 98.0 %\n",
      "Batch: 55  Accuracy: 98.0 %\n",
      "Batch: 56  Accuracy: 98.4 %\n",
      "Batch: 57  Accuracy: 98.0 %\n",
      "Batch: 58  Accuracy: 98.8 %\n",
      "Batch: 59  Accuracy: 98.4 %\n",
      "Batch: 60  Accuracy: 100.0 %\n",
      "Batch: 61  Accuracy: 97.2 %\n",
      "Batch: 62  Accuracy: 99.2 %\n",
      "Batch: 63  Accuracy: 95.19999999999999 %\n",
      "Batch: 64  Accuracy: 98.8 %\n",
      "Batch: 65  Accuracy: 98.8 %\n",
      "Batch: 66  Accuracy: 97.6 %\n",
      "Batch: 67  Accuracy: 98.4 %\n",
      "Batch: 68  Accuracy: 97.2 %\n",
      "Batch: 69  Accuracy: 98.8 %\n",
      "Batch: 70  Accuracy: 98.4 %\n",
      "Batch: 71  Accuracy: 98.0 %\n",
      "Batch: 72  Accuracy: 98.0 %\n",
      "Batch: 73  Accuracy: 98.0 %\n",
      "Batch: 74  Accuracy: 98.8 %\n",
      "Batch: 75  Accuracy: 99.2 %\n",
      "Batch: 76  Accuracy: 98.4 %\n",
      "Batch: 77  Accuracy: 97.6 %\n",
      "Batch: 78  Accuracy: 99.2 %\n",
      "Batch: 79  Accuracy: 99.2 %\n",
      "Batch: 80  Accuracy: 97.6 %\n",
      "Batch: 81  Accuracy: 99.6 %\n",
      "Batch: 82  Accuracy: 97.2 %\n",
      "Batch: 83  Accuracy: 97.6 %\n",
      "Batch: 84  Accuracy: 98.4 %\n",
      "Batch: 85  Accuracy: 98.8 %\n",
      "Batch: 86  Accuracy: 98.8 %\n",
      "Batch: 87  Accuracy: 98.4 %\n",
      "Batch: 88  Accuracy: 97.6 %\n",
      "Batch: 89  Accuracy: 99.6 %\n",
      "Batch: 90  Accuracy: 98.8 %\n",
      "Batch: 91  Accuracy: 98.4 %\n",
      "Batch: 92  Accuracy: 98.8 %\n",
      "Batch: 93  Accuracy: 99.2 %\n",
      "Batch: 94  Accuracy: 98.8 %\n",
      "Batch: 95  Accuracy: 98.4 %\n",
      "Batch: 96  Accuracy: 98.0 %\n",
      "Batch: 97  Accuracy: 99.6 %\n",
      "Batch: 98  Accuracy: 99.2 %\n",
      "Batch: 99  Accuracy: 99.2 %\n",
      "Batch: 100  Accuracy: 98.0 %\n",
      "Batch: 101  Accuracy: 98.0 %\n",
      "Batch: 102  Accuracy: 98.8 %\n",
      "Batch: 103  Accuracy: 97.6 %\n",
      "Batch: 104  Accuracy: 98.4 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 95.6 %\n",
      "Batch: 107  Accuracy: 98.8 %\n",
      "Batch: 108  Accuracy: 98.0 %\n",
      "Batch: 109  Accuracy: 98.8 %\n",
      "Batch: 110  Accuracy: 97.6 %\n",
      "Batch: 111  Accuracy: 98.8 %\n",
      "Batch: 112  Accuracy: 98.8 %\n",
      "Batch: 113  Accuracy: 98.0 %\n",
      "Batch: 114  Accuracy: 97.6 %\n",
      "Batch: 115  Accuracy: 98.8 %\n",
      "Batch: 116  Accuracy: 98.0 %\n",
      "Batch: 117  Accuracy: 98.0 %\n",
      "Batch: 118  Accuracy: 99.2 %\n",
      "Batch: 119  Accuracy: 98.8 %\n",
      "Batch: 120  Accuracy: 98.0 %\n",
      "Batch: 121  Accuracy: 99.6 %\n",
      "Batch: 122  Accuracy: 98.4 %\n",
      "Batch: 123  Accuracy: 97.6 %\n",
      "Batch: 124  Accuracy: 98.4 %\n",
      "Batch: 125  Accuracy: 97.2 %\n",
      "Batch: 126  Accuracy: 96.0 %\n",
      "Batch: 127  Accuracy: 97.2 %\n",
      "Batch: 128  Accuracy: 98.4 %\n",
      "Batch: 129  Accuracy: 97.2 %\n",
      "Batch: 130  Accuracy: 99.2 %\n",
      "Batch: 131  Accuracy: 98.0 %\n",
      "Batch: 132  Accuracy: 98.8 %\n",
      "Batch: 133  Accuracy: 98.4 %\n",
      "Batch: 134  Accuracy: 99.2 %\n",
      "Batch: 135  Accuracy: 99.2 %\n",
      "Batch: 136  Accuracy: 99.2 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 137  Accuracy: 98.8 %\n",
      "Batch: 138  Accuracy: 97.2 %\n",
      "Batch: 139  Accuracy: 97.2 %\n",
      "Batch: 140  Accuracy: 98.4 %\n",
      "Batch: 141  Accuracy: 98.4 %\n",
      "Batch: 142  Accuracy: 98.4 %\n",
      "Batch: 143  Accuracy: 98.8 %\n",
      "Batch: 144  Accuracy: 97.6 %\n",
      "Batch: 145  Accuracy: 98.0 %\n",
      "Batch: 146  Accuracy: 97.6 %\n",
      "Batch: 147  Accuracy: 97.6 %\n",
      "Batch: 148  Accuracy: 97.2 %\n",
      "Batch: 149  Accuracy: 97.6 %\n",
      "Batch: 150  Accuracy: 98.4 %\n",
      "Batch: 151  Accuracy: 97.6 %\n",
      "Batch: 152  Accuracy: 99.2 %\n",
      "Batch: 153  Accuracy: 98.0 %\n",
      "Batch: 154  Accuracy: 98.4 %\n",
      "Batch: 155  Accuracy: 99.2 %\n",
      "Batch: 156  Accuracy: 99.2 %\n",
      "Batch: 157  Accuracy: 95.6 %\n",
      "Batch: 158  Accuracy: 97.6 %\n",
      "Batch: 159  Accuracy: 98.8 %\n",
      "Batch: 160  Accuracy: 98.8 %\n",
      "Batch: 161  Accuracy: 98.8 %\n",
      "Batch: 162  Accuracy: 99.2 %\n",
      "Batch: 163  Accuracy: 98.0 %\n",
      "Batch: 164  Accuracy: 98.8 %\n",
      "Batch: 165  Accuracy: 96.8 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 96.8 %\n",
      "Batch: 169  Accuracy: 98.4 %\n",
      "Batch: 170  Accuracy: 98.4 %\n",
      "Batch: 171  Accuracy: 98.0 %\n",
      "Batch: 172  Accuracy: 98.0 %\n",
      "Batch: 173  Accuracy: 98.8 %\n",
      "Batch: 174  Accuracy: 98.4 %\n",
      "Batch: 175  Accuracy: 98.8 %\n",
      "Batch: 176  Accuracy: 99.2 %\n",
      "Batch: 177  Accuracy: 97.2 %\n",
      "Batch: 178  Accuracy: 98.8 %\n",
      "Batch: 179  Accuracy: 98.4 %\n",
      "Batch: 180  Accuracy: 98.4 %\n",
      "Batch: 181  Accuracy: 98.0 %\n",
      "Batch: 182  Accuracy: 97.6 %\n",
      "Batch: 183  Accuracy: 97.6 %\n",
      "Batch: 184  Accuracy: 97.2 %\n",
      "Batch: 185  Accuracy: 95.6 %\n",
      "Batch: 186  Accuracy: 97.6 %\n",
      "Batch: 187  Accuracy: 98.8 %\n",
      "Batch: 188  Accuracy: 97.6 %\n",
      "Batch: 189  Accuracy: 98.0 %\n",
      "Batch: 190  Accuracy: 96.39999999999999 %\n",
      "Batch: 191  Accuracy: 98.4 %\n",
      "Batch: 192  Accuracy: 98.0 %\n",
      "Batch: 193  Accuracy: 98.4 %\n",
      "Batch: 194  Accuracy: 99.2 %\n",
      "Batch: 195  Accuracy: 98.4 %\n",
      "Batch: 196  Accuracy: 97.2 %\n",
      "Batch: 197  Accuracy: 99.2 %\n",
      "Batch: 198  Accuracy: 96.8 %\n",
      "Batch: 199  Accuracy: 96.8 %\n",
      "Batch: 200  Accuracy: 97.6 %\n",
      "Batch: 201  Accuracy: 97.6 %\n",
      "Batch: 202  Accuracy: 98.8 %\n",
      "Batch: 203  Accuracy: 98.0 %\n",
      "Batch: 204  Accuracy: 98.4 %\n",
      "Batch: 205  Accuracy: 98.0 %\n",
      "Batch: 206  Accuracy: 99.2 %\n",
      "Batch: 207  Accuracy: 97.6 %\n",
      "Batch: 208  Accuracy: 97.2 %\n",
      "Batch: 209  Accuracy: 98.4 %\n",
      "Batch: 210  Accuracy: 99.6 %\n",
      "Batch: 211  Accuracy: 97.6 %\n",
      "Batch: 212  Accuracy: 98.8 %\n",
      "Batch: 213  Accuracy: 99.6 %\n",
      "Batch: 214  Accuracy: 98.4 %\n",
      "Batch: 215  Accuracy: 98.4 %\n",
      "Batch: 216  Accuracy: 98.8 %\n",
      "Batch: 217  Accuracy: 98.8 %\n",
      "Batch: 218  Accuracy: 98.8 %\n",
      "Batch: 219  Accuracy: 97.6 %\n",
      "Batch: 220  Accuracy: 99.6 %\n",
      "Batch: 221  Accuracy: 98.0 %\n",
      "Batch: 222  Accuracy: 98.4 %\n",
      "Batch: 223  Accuracy: 98.8 %\n",
      "Batch: 224  Accuracy: 98.8 %\n",
      "Batch: 225  Accuracy: 98.8 %\n",
      "Batch: 226  Accuracy: 98.0 %\n",
      "Batch: 227  Accuracy: 98.0 %\n",
      "Batch: 228  Accuracy: 98.4 %\n",
      "Batch: 229  Accuracy: 97.6 %\n",
      "Batch: 230  Accuracy: 98.0 %\n",
      "Batch: 231  Accuracy: 99.2 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 99.6 %\n",
      "Batch: 234  Accuracy: 99.6 %\n",
      "Batch: 235  Accuracy: 98.8 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 99.2 %\n",
      "Batch: 238  Accuracy: 96.8 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 5  Accuracy: 98.21333333333332 %  loss: 0.058646873034672684\n",
      "Batch: 0  Accuracy: 97.6 %\n",
      "Batch: 1  Accuracy: 98.4 %\n",
      "Batch: 2  Accuracy: 98.8 %\n",
      "Batch: 3  Accuracy: 97.6 %\n",
      "Batch: 4  Accuracy: 98.4 %\n",
      "Batch: 5  Accuracy: 97.6 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 98.4 %\n",
      "Batch: 8  Accuracy: 99.2 %\n",
      "Batch: 9  Accuracy: 99.6 %\n",
      "Batch: 10  Accuracy: 98.0 %\n",
      "Batch: 11  Accuracy: 99.6 %\n",
      "Batch: 12  Accuracy: 99.2 %\n",
      "Batch: 13  Accuracy: 99.6 %\n",
      "Batch: 14  Accuracy: 98.0 %\n",
      "Batch: 15  Accuracy: 98.8 %\n",
      "Batch: 16  Accuracy: 99.2 %\n",
      "Batch: 17  Accuracy: 97.6 %\n",
      "Batch: 18  Accuracy: 98.8 %\n",
      "Batch: 19  Accuracy: 98.0 %\n",
      "Batch: 20  Accuracy: 98.4 %\n",
      "Batch: 21  Accuracy: 99.2 %\n",
      "Batch: 22  Accuracy: 98.0 %\n",
      "Batch: 23  Accuracy: 97.2 %\n",
      "Batch: 24  Accuracy: 98.8 %\n",
      "Batch: 25  Accuracy: 96.8 %\n",
      "Batch: 26  Accuracy: 99.2 %\n",
      "Batch: 27  Accuracy: 97.2 %\n",
      "Batch: 28  Accuracy: 97.2 %\n",
      "Batch: 29  Accuracy: 98.4 %\n",
      "Batch: 30  Accuracy: 98.4 %\n",
      "Batch: 31  Accuracy: 98.8 %\n",
      "Batch: 32  Accuracy: 96.8 %\n",
      "Batch: 33  Accuracy: 97.6 %\n",
      "Batch: 34  Accuracy: 96.0 %\n",
      "Batch: 35  Accuracy: 96.0 %\n",
      "Batch: 36  Accuracy: 98.0 %\n",
      "Batch: 37  Accuracy: 96.8 %\n",
      "Batch: 38  Accuracy: 98.0 %\n",
      "Batch: 39  Accuracy: 100.0 %\n",
      "Batch: 40  Accuracy: 96.39999999999999 %\n",
      "Batch: 41  Accuracy: 98.0 %\n",
      "Batch: 42  Accuracy: 99.6 %\n",
      "Batch: 43  Accuracy: 98.8 %\n",
      "Batch: 44  Accuracy: 99.6 %\n",
      "Batch: 45  Accuracy: 100.0 %\n",
      "Batch: 46  Accuracy: 99.6 %\n",
      "Batch: 47  Accuracy: 98.0 %\n",
      "Batch: 48  Accuracy: 97.6 %\n",
      "Batch: 49  Accuracy: 98.8 %\n",
      "Batch: 50  Accuracy: 97.2 %\n",
      "Batch: 51  Accuracy: 98.0 %\n",
      "Batch: 52  Accuracy: 95.6 %\n",
      "Batch: 53  Accuracy: 99.2 %\n",
      "Batch: 54  Accuracy: 98.0 %\n",
      "Batch: 55  Accuracy: 98.0 %\n",
      "Batch: 56  Accuracy: 98.4 %\n",
      "Batch: 57  Accuracy: 98.4 %\n",
      "Batch: 58  Accuracy: 98.8 %\n",
      "Batch: 59  Accuracy: 98.4 %\n",
      "Batch: 60  Accuracy: 100.0 %\n",
      "Batch: 61  Accuracy: 97.6 %\n",
      "Batch: 62  Accuracy: 99.2 %\n",
      "Batch: 63  Accuracy: 95.6 %\n",
      "Batch: 64  Accuracy: 99.2 %\n",
      "Batch: 65  Accuracy: 99.2 %\n",
      "Batch: 66  Accuracy: 97.6 %\n",
      "Batch: 67  Accuracy: 98.4 %\n",
      "Batch: 68  Accuracy: 97.2 %\n",
      "Batch: 69  Accuracy: 99.2 %\n",
      "Batch: 70  Accuracy: 98.4 %\n",
      "Batch: 71  Accuracy: 98.8 %\n",
      "Batch: 72  Accuracy: 98.4 %\n",
      "Batch: 73  Accuracy: 98.4 %\n",
      "Batch: 74  Accuracy: 98.8 %\n",
      "Batch: 75  Accuracy: 99.6 %\n",
      "Batch: 76  Accuracy: 98.4 %\n",
      "Batch: 77  Accuracy: 98.4 %\n",
      "Batch: 78  Accuracy: 99.2 %\n",
      "Batch: 79  Accuracy: 99.2 %\n",
      "Batch: 80  Accuracy: 97.6 %\n",
      "Batch: 81  Accuracy: 99.6 %\n",
      "Batch: 82  Accuracy: 97.6 %\n",
      "Batch: 83  Accuracy: 98.8 %\n",
      "Batch: 84  Accuracy: 98.4 %\n",
      "Batch: 85  Accuracy: 98.4 %\n",
      "Batch: 86  Accuracy: 99.6 %\n",
      "Batch: 87  Accuracy: 98.4 %\n",
      "Batch: 88  Accuracy: 98.0 %\n",
      "Batch: 89  Accuracy: 99.6 %\n",
      "Batch: 90  Accuracy: 98.8 %\n",
      "Batch: 91  Accuracy: 98.8 %\n",
      "Batch: 92  Accuracy: 98.8 %\n",
      "Batch: 93  Accuracy: 99.2 %\n",
      "Batch: 94  Accuracy: 98.8 %\n",
      "Batch: 95  Accuracy: 98.4 %\n",
      "Batch: 96  Accuracy: 98.0 %\n",
      "Batch: 97  Accuracy: 99.6 %\n",
      "Batch: 98  Accuracy: 99.2 %\n",
      "Batch: 99  Accuracy: 99.2 %\n",
      "Batch: 100  Accuracy: 98.0 %\n",
      "Batch: 101  Accuracy: 98.8 %\n",
      "Batch: 102  Accuracy: 98.8 %\n",
      "Batch: 103  Accuracy: 97.6 %\n",
      "Batch: 104  Accuracy: 98.8 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 95.6 %\n",
      "Batch: 107  Accuracy: 98.8 %\n",
      "Batch: 108  Accuracy: 98.0 %\n",
      "Batch: 109  Accuracy: 98.8 %\n",
      "Batch: 110  Accuracy: 97.6 %\n",
      "Batch: 111  Accuracy: 99.2 %\n",
      "Batch: 112  Accuracy: 98.8 %\n",
      "Batch: 113  Accuracy: 98.4 %\n",
      "Batch: 114  Accuracy: 98.0 %\n",
      "Batch: 115  Accuracy: 98.8 %\n",
      "Batch: 116  Accuracy: 98.8 %\n",
      "Batch: 117  Accuracy: 98.4 %\n",
      "Batch: 118  Accuracy: 99.2 %\n",
      "Batch: 119  Accuracy: 99.2 %\n",
      "Batch: 120  Accuracy: 98.0 %\n",
      "Batch: 121  Accuracy: 100.0 %\n",
      "Batch: 122  Accuracy: 98.4 %\n",
      "Batch: 123  Accuracy: 98.0 %\n",
      "Batch: 124  Accuracy: 98.8 %\n",
      "Batch: 125  Accuracy: 98.0 %\n",
      "Batch: 126  Accuracy: 96.0 %\n",
      "Batch: 127  Accuracy: 97.6 %\n",
      "Batch: 128  Accuracy: 98.4 %\n",
      "Batch: 129  Accuracy: 97.2 %\n",
      "Batch: 130  Accuracy: 99.2 %\n",
      "Batch: 131  Accuracy: 98.4 %\n",
      "Batch: 132  Accuracy: 98.8 %\n",
      "Batch: 133  Accuracy: 98.4 %\n",
      "Batch: 134  Accuracy: 99.2 %\n",
      "Batch: 135  Accuracy: 99.6 %\n",
      "Batch: 136  Accuracy: 99.2 %\n",
      "Batch: 137  Accuracy: 98.8 %\n",
      "Batch: 138  Accuracy: 97.2 %\n",
      "Batch: 139  Accuracy: 97.2 %\n",
      "Batch: 140  Accuracy: 98.4 %\n",
      "Batch: 141  Accuracy: 98.4 %\n",
      "Batch: 142  Accuracy: 98.8 %\n",
      "Batch: 143  Accuracy: 98.8 %\n",
      "Batch: 144  Accuracy: 97.6 %\n",
      "Batch: 145  Accuracy: 98.0 %\n",
      "Batch: 146  Accuracy: 97.6 %\n",
      "Batch: 147  Accuracy: 98.0 %\n",
      "Batch: 148  Accuracy: 97.2 %\n",
      "Batch: 149  Accuracy: 97.6 %\n",
      "Batch: 150  Accuracy: 98.4 %\n",
      "Batch: 151  Accuracy: 98.0 %\n",
      "Batch: 152  Accuracy: 99.2 %\n",
      "Batch: 153  Accuracy: 98.0 %\n",
      "Batch: 154  Accuracy: 98.4 %\n",
      "Batch: 155  Accuracy: 99.2 %\n",
      "Batch: 156  Accuracy: 99.2 %\n",
      "Batch: 157  Accuracy: 95.6 %\n",
      "Batch: 158  Accuracy: 98.0 %\n",
      "Batch: 159  Accuracy: 99.2 %\n",
      "Batch: 160  Accuracy: 99.2 %\n",
      "Batch: 161  Accuracy: 98.8 %\n",
      "Batch: 162  Accuracy: 100.0 %\n",
      "Batch: 163  Accuracy: 98.0 %\n",
      "Batch: 164  Accuracy: 98.8 %\n",
      "Batch: 165  Accuracy: 96.8 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 97.6 %\n",
      "Batch: 169  Accuracy: 98.8 %\n",
      "Batch: 170  Accuracy: 98.4 %\n",
      "Batch: 171  Accuracy: 97.6 %\n",
      "Batch: 172  Accuracy: 97.2 %\n",
      "Batch: 173  Accuracy: 98.8 %\n",
      "Batch: 174  Accuracy: 98.4 %\n",
      "Batch: 175  Accuracy: 98.8 %\n",
      "Batch: 176  Accuracy: 99.2 %\n",
      "Batch: 177  Accuracy: 97.2 %\n",
      "Batch: 178  Accuracy: 98.8 %\n",
      "Batch: 179  Accuracy: 98.4 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 180  Accuracy: 98.8 %\n",
      "Batch: 181  Accuracy: 98.0 %\n",
      "Batch: 182  Accuracy: 98.0 %\n",
      "Batch: 183  Accuracy: 97.2 %\n",
      "Batch: 184  Accuracy: 97.2 %\n",
      "Batch: 185  Accuracy: 95.19999999999999 %\n",
      "Batch: 186  Accuracy: 98.4 %\n",
      "Batch: 187  Accuracy: 98.8 %\n",
      "Batch: 188  Accuracy: 98.4 %\n",
      "Batch: 189  Accuracy: 97.6 %\n",
      "Batch: 190  Accuracy: 96.8 %\n",
      "Batch: 191  Accuracy: 98.4 %\n",
      "Batch: 192  Accuracy: 98.0 %\n",
      "Batch: 193  Accuracy: 98.4 %\n",
      "Batch: 194  Accuracy: 99.6 %\n",
      "Batch: 195  Accuracy: 98.0 %\n",
      "Batch: 196  Accuracy: 97.6 %\n",
      "Batch: 197  Accuracy: 99.2 %\n",
      "Batch: 198  Accuracy: 97.2 %\n",
      "Batch: 199  Accuracy: 96.8 %\n",
      "Batch: 200  Accuracy: 97.6 %\n",
      "Batch: 201  Accuracy: 98.0 %\n",
      "Batch: 202  Accuracy: 98.8 %\n",
      "Batch: 203  Accuracy: 98.0 %\n",
      "Batch: 204  Accuracy: 98.8 %\n",
      "Batch: 205  Accuracy: 98.0 %\n",
      "Batch: 206  Accuracy: 99.6 %\n",
      "Batch: 207  Accuracy: 97.6 %\n",
      "Batch: 208  Accuracy: 97.2 %\n",
      "Batch: 209  Accuracy: 98.4 %\n",
      "Batch: 210  Accuracy: 99.6 %\n",
      "Batch: 211  Accuracy: 98.0 %\n",
      "Batch: 212  Accuracy: 98.4 %\n",
      "Batch: 213  Accuracy: 99.6 %\n",
      "Batch: 214  Accuracy: 98.4 %\n",
      "Batch: 215  Accuracy: 98.4 %\n",
      "Batch: 216  Accuracy: 99.2 %\n",
      "Batch: 217  Accuracy: 98.8 %\n",
      "Batch: 218  Accuracy: 98.8 %\n",
      "Batch: 219  Accuracy: 97.6 %\n",
      "Batch: 220  Accuracy: 100.0 %\n",
      "Batch: 221  Accuracy: 98.0 %\n",
      "Batch: 222  Accuracy: 98.4 %\n",
      "Batch: 223  Accuracy: 99.2 %\n",
      "Batch: 224  Accuracy: 98.8 %\n",
      "Batch: 225  Accuracy: 98.8 %\n",
      "Batch: 226  Accuracy: 98.8 %\n",
      "Batch: 227  Accuracy: 98.0 %\n",
      "Batch: 228  Accuracy: 98.8 %\n",
      "Batch: 229  Accuracy: 97.6 %\n",
      "Batch: 230  Accuracy: 98.4 %\n",
      "Batch: 231  Accuracy: 99.2 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 99.6 %\n",
      "Batch: 234  Accuracy: 99.6 %\n",
      "Batch: 235  Accuracy: 98.4 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 99.2 %\n",
      "Batch: 238  Accuracy: 96.8 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 6  Accuracy: 98.36666666666667 %  loss: 0.05353143770625602\n",
      "Batch: 0  Accuracy: 97.6 %\n",
      "Batch: 1  Accuracy: 98.4 %\n",
      "Batch: 2  Accuracy: 98.8 %\n",
      "Batch: 3  Accuracy: 97.6 %\n",
      "Batch: 4  Accuracy: 98.4 %\n",
      "Batch: 5  Accuracy: 97.6 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 98.4 %\n",
      "Batch: 8  Accuracy: 99.6 %\n",
      "Batch: 9  Accuracy: 99.6 %\n",
      "Batch: 10  Accuracy: 98.0 %\n",
      "Batch: 11  Accuracy: 99.6 %\n",
      "Batch: 12  Accuracy: 99.2 %\n",
      "Batch: 13  Accuracy: 99.6 %\n",
      "Batch: 14  Accuracy: 98.0 %\n",
      "Batch: 15  Accuracy: 98.8 %\n",
      "Batch: 16  Accuracy: 99.2 %\n",
      "Batch: 17  Accuracy: 98.0 %\n",
      "Batch: 18  Accuracy: 99.2 %\n",
      "Batch: 19  Accuracy: 98.0 %\n",
      "Batch: 20  Accuracy: 98.4 %\n",
      "Batch: 21  Accuracy: 99.2 %\n",
      "Batch: 22  Accuracy: 98.0 %\n",
      "Batch: 23  Accuracy: 97.2 %\n",
      "Batch: 24  Accuracy: 98.8 %\n",
      "Batch: 25  Accuracy: 96.8 %\n",
      "Batch: 26  Accuracy: 99.6 %\n",
      "Batch: 27  Accuracy: 97.6 %\n",
      "Batch: 28  Accuracy: 97.6 %\n",
      "Batch: 29  Accuracy: 98.4 %\n",
      "Batch: 30  Accuracy: 98.4 %\n",
      "Batch: 31  Accuracy: 98.8 %\n",
      "Batch: 32  Accuracy: 96.8 %\n",
      "Batch: 33  Accuracy: 98.0 %\n",
      "Batch: 34  Accuracy: 97.2 %\n",
      "Batch: 35  Accuracy: 96.0 %\n",
      "Batch: 36  Accuracy: 98.0 %\n",
      "Batch: 37  Accuracy: 97.2 %\n",
      "Batch: 38  Accuracy: 98.4 %\n",
      "Batch: 39  Accuracy: 100.0 %\n",
      "Batch: 40  Accuracy: 96.8 %\n",
      "Batch: 41  Accuracy: 98.0 %\n",
      "Batch: 42  Accuracy: 99.6 %\n",
      "Batch: 43  Accuracy: 98.8 %\n",
      "Batch: 44  Accuracy: 99.6 %\n",
      "Batch: 45  Accuracy: 100.0 %\n",
      "Batch: 46  Accuracy: 99.6 %\n",
      "Batch: 47  Accuracy: 98.0 %\n",
      "Batch: 48  Accuracy: 97.6 %\n",
      "Batch: 49  Accuracy: 99.6 %\n",
      "Batch: 50  Accuracy: 97.2 %\n",
      "Batch: 51  Accuracy: 98.4 %\n",
      "Batch: 52  Accuracy: 96.39999999999999 %\n",
      "Batch: 53  Accuracy: 99.2 %\n",
      "Batch: 54  Accuracy: 98.0 %\n",
      "Batch: 55  Accuracy: 98.4 %\n",
      "Batch: 56  Accuracy: 98.4 %\n",
      "Batch: 57  Accuracy: 98.8 %\n",
      "Batch: 58  Accuracy: 98.8 %\n",
      "Batch: 59  Accuracy: 98.4 %\n",
      "Batch: 60  Accuracy: 100.0 %\n",
      "Batch: 61  Accuracy: 97.6 %\n",
      "Batch: 62  Accuracy: 99.2 %\n",
      "Batch: 63  Accuracy: 96.8 %\n",
      "Batch: 64  Accuracy: 99.2 %\n",
      "Batch: 65  Accuracy: 99.2 %\n",
      "Batch: 66  Accuracy: 97.6 %\n",
      "Batch: 67  Accuracy: 99.2 %\n",
      "Batch: 68  Accuracy: 98.0 %\n",
      "Batch: 69  Accuracy: 99.2 %\n",
      "Batch: 70  Accuracy: 98.8 %\n",
      "Batch: 71  Accuracy: 99.2 %\n",
      "Batch: 72  Accuracy: 98.0 %\n",
      "Batch: 73  Accuracy: 98.0 %\n",
      "Batch: 74  Accuracy: 98.8 %\n",
      "Batch: 75  Accuracy: 100.0 %\n",
      "Batch: 76  Accuracy: 98.4 %\n",
      "Batch: 77  Accuracy: 98.4 %\n",
      "Batch: 78  Accuracy: 99.2 %\n",
      "Batch: 79  Accuracy: 99.2 %\n",
      "Batch: 80  Accuracy: 98.0 %\n",
      "Batch: 81  Accuracy: 99.6 %\n",
      "Batch: 82  Accuracy: 98.0 %\n",
      "Batch: 83  Accuracy: 98.8 %\n",
      "Batch: 84  Accuracy: 98.0 %\n",
      "Batch: 85  Accuracy: 98.8 %\n",
      "Batch: 86  Accuracy: 99.6 %\n",
      "Batch: 87  Accuracy: 98.4 %\n",
      "Batch: 88  Accuracy: 98.8 %\n",
      "Batch: 89  Accuracy: 99.6 %\n",
      "Batch: 90  Accuracy: 98.8 %\n",
      "Batch: 91  Accuracy: 98.8 %\n",
      "Batch: 92  Accuracy: 98.4 %\n",
      "Batch: 93  Accuracy: 99.2 %\n",
      "Batch: 94  Accuracy: 98.8 %\n",
      "Batch: 95  Accuracy: 98.4 %\n",
      "Batch: 96  Accuracy: 98.0 %\n",
      "Batch: 97  Accuracy: 100.0 %\n",
      "Batch: 98  Accuracy: 99.2 %\n",
      "Batch: 99  Accuracy: 99.2 %\n",
      "Batch: 100  Accuracy: 98.0 %\n",
      "Batch: 101  Accuracy: 99.2 %\n",
      "Batch: 102  Accuracy: 98.8 %\n",
      "Batch: 103  Accuracy: 97.6 %\n",
      "Batch: 104  Accuracy: 98.8 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 96.8 %\n",
      "Batch: 107  Accuracy: 98.8 %\n",
      "Batch: 108  Accuracy: 98.4 %\n",
      "Batch: 109  Accuracy: 98.8 %\n",
      "Batch: 110  Accuracy: 97.6 %\n",
      "Batch: 111  Accuracy: 99.2 %\n",
      "Batch: 112  Accuracy: 98.8 %\n",
      "Batch: 113  Accuracy: 98.0 %\n",
      "Batch: 114  Accuracy: 98.0 %\n",
      "Batch: 115  Accuracy: 98.8 %\n",
      "Batch: 116  Accuracy: 98.8 %\n",
      "Batch: 117  Accuracy: 98.0 %\n",
      "Batch: 118  Accuracy: 99.2 %\n",
      "Batch: 119  Accuracy: 99.2 %\n",
      "Batch: 120  Accuracy: 98.4 %\n",
      "Batch: 121  Accuracy: 100.0 %\n",
      "Batch: 122  Accuracy: 98.8 %\n",
      "Batch: 123  Accuracy: 98.0 %\n",
      "Batch: 124  Accuracy: 98.8 %\n",
      "Batch: 125  Accuracy: 98.0 %\n",
      "Batch: 126  Accuracy: 96.39999999999999 %\n",
      "Batch: 127  Accuracy: 97.6 %\n",
      "Batch: 128  Accuracy: 98.0 %\n",
      "Batch: 129  Accuracy: 97.2 %\n",
      "Batch: 130  Accuracy: 99.2 %\n",
      "Batch: 131  Accuracy: 98.4 %\n",
      "Batch: 132  Accuracy: 99.2 %\n",
      "Batch: 133  Accuracy: 98.0 %\n",
      "Batch: 134  Accuracy: 99.2 %\n",
      "Batch: 135  Accuracy: 99.6 %\n",
      "Batch: 136  Accuracy: 99.2 %\n",
      "Batch: 137  Accuracy: 99.2 %\n",
      "Batch: 138  Accuracy: 98.0 %\n",
      "Batch: 139  Accuracy: 97.2 %\n",
      "Batch: 140  Accuracy: 98.4 %\n",
      "Batch: 141  Accuracy: 98.4 %\n",
      "Batch: 142  Accuracy: 98.8 %\n",
      "Batch: 143  Accuracy: 98.8 %\n",
      "Batch: 144  Accuracy: 97.6 %\n",
      "Batch: 145  Accuracy: 98.0 %\n",
      "Batch: 146  Accuracy: 98.0 %\n",
      "Batch: 147  Accuracy: 98.0 %\n",
      "Batch: 148  Accuracy: 97.6 %\n",
      "Batch: 149  Accuracy: 98.0 %\n",
      "Batch: 150  Accuracy: 98.8 %\n",
      "Batch: 151  Accuracy: 98.0 %\n",
      "Batch: 152  Accuracy: 99.2 %\n",
      "Batch: 153  Accuracy: 98.0 %\n",
      "Batch: 154  Accuracy: 98.4 %\n",
      "Batch: 155  Accuracy: 99.2 %\n",
      "Batch: 156  Accuracy: 99.2 %\n",
      "Batch: 157  Accuracy: 96.0 %\n",
      "Batch: 158  Accuracy: 98.4 %\n",
      "Batch: 159  Accuracy: 99.2 %\n",
      "Batch: 160  Accuracy: 99.2 %\n",
      "Batch: 161  Accuracy: 99.2 %\n",
      "Batch: 162  Accuracy: 100.0 %\n",
      "Batch: 163  Accuracy: 98.0 %\n",
      "Batch: 164  Accuracy: 98.8 %\n",
      "Batch: 165  Accuracy: 96.8 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 97.6 %\n",
      "Batch: 169  Accuracy: 99.6 %\n",
      "Batch: 170  Accuracy: 98.4 %\n",
      "Batch: 171  Accuracy: 98.0 %\n",
      "Batch: 172  Accuracy: 97.6 %\n",
      "Batch: 173  Accuracy: 99.2 %\n",
      "Batch: 174  Accuracy: 98.4 %\n",
      "Batch: 175  Accuracy: 98.8 %\n",
      "Batch: 176  Accuracy: 99.2 %\n",
      "Batch: 177  Accuracy: 97.2 %\n",
      "Batch: 178  Accuracy: 98.8 %\n",
      "Batch: 179  Accuracy: 98.4 %\n",
      "Batch: 180  Accuracy: 99.2 %\n",
      "Batch: 181  Accuracy: 98.0 %\n",
      "Batch: 182  Accuracy: 98.4 %\n",
      "Batch: 183  Accuracy: 97.2 %\n",
      "Batch: 184  Accuracy: 96.8 %\n",
      "Batch: 185  Accuracy: 95.6 %\n",
      "Batch: 186  Accuracy: 98.4 %\n",
      "Batch: 187  Accuracy: 98.8 %\n",
      "Batch: 188  Accuracy: 98.8 %\n",
      "Batch: 189  Accuracy: 97.6 %\n",
      "Batch: 190  Accuracy: 97.6 %\n",
      "Batch: 191  Accuracy: 98.4 %\n",
      "Batch: 192  Accuracy: 98.0 %\n",
      "Batch: 193  Accuracy: 98.8 %\n",
      "Batch: 194  Accuracy: 99.6 %\n",
      "Batch: 195  Accuracy: 98.0 %\n",
      "Batch: 196  Accuracy: 98.0 %\n",
      "Batch: 197  Accuracy: 99.2 %\n",
      "Batch: 198  Accuracy: 97.6 %\n",
      "Batch: 199  Accuracy: 96.8 %\n",
      "Batch: 200  Accuracy: 98.0 %\n",
      "Batch: 201  Accuracy: 98.8 %\n",
      "Batch: 202  Accuracy: 98.8 %\n",
      "Batch: 203  Accuracy: 98.0 %\n",
      "Batch: 204  Accuracy: 98.8 %\n",
      "Batch: 205  Accuracy: 98.4 %\n",
      "Batch: 206  Accuracy: 99.6 %\n",
      "Batch: 207  Accuracy: 98.4 %\n",
      "Batch: 208  Accuracy: 97.2 %\n",
      "Batch: 209  Accuracy: 98.4 %\n",
      "Batch: 210  Accuracy: 99.6 %\n",
      "Batch: 211  Accuracy: 98.0 %\n",
      "Batch: 212  Accuracy: 98.4 %\n",
      "Batch: 213  Accuracy: 99.6 %\n",
      "Batch: 214  Accuracy: 98.8 %\n",
      "Batch: 215  Accuracy: 98.4 %\n",
      "Batch: 216  Accuracy: 99.2 %\n",
      "Batch: 217  Accuracy: 98.8 %\n",
      "Batch: 218  Accuracy: 98.8 %\n",
      "Batch: 219  Accuracy: 97.6 %\n",
      "Batch: 220  Accuracy: 100.0 %\n",
      "Batch: 221  Accuracy: 98.0 %\n",
      "Batch: 222  Accuracy: 98.8 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 223  Accuracy: 99.2 %\n",
      "Batch: 224  Accuracy: 99.2 %\n",
      "Batch: 225  Accuracy: 98.8 %\n",
      "Batch: 226  Accuracy: 98.8 %\n",
      "Batch: 227  Accuracy: 98.0 %\n",
      "Batch: 228  Accuracy: 98.8 %\n",
      "Batch: 229  Accuracy: 98.0 %\n",
      "Batch: 230  Accuracy: 98.4 %\n",
      "Batch: 231  Accuracy: 99.2 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 99.6 %\n",
      "Batch: 234  Accuracy: 99.6 %\n",
      "Batch: 235  Accuracy: 98.4 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 99.6 %\n",
      "Batch: 238  Accuracy: 97.2 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 7  Accuracy: 98.48666666666666 %  loss: 0.04955412420137391\n",
      "Batch: 0  Accuracy: 97.6 %\n",
      "Batch: 1  Accuracy: 98.8 %\n",
      "Batch: 2  Accuracy: 98.8 %\n",
      "Batch: 3  Accuracy: 97.6 %\n",
      "Batch: 4  Accuracy: 98.4 %\n",
      "Batch: 5  Accuracy: 97.6 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 98.4 %\n",
      "Batch: 8  Accuracy: 99.6 %\n",
      "Batch: 9  Accuracy: 99.6 %\n",
      "Batch: 10  Accuracy: 98.0 %\n",
      "Batch: 11  Accuracy: 100.0 %\n",
      "Batch: 12  Accuracy: 99.2 %\n",
      "Batch: 13  Accuracy: 99.6 %\n",
      "Batch: 14  Accuracy: 98.0 %\n",
      "Batch: 15  Accuracy: 99.2 %\n",
      "Batch: 16  Accuracy: 99.2 %\n",
      "Batch: 17  Accuracy: 98.0 %\n",
      "Batch: 18  Accuracy: 99.2 %\n",
      "Batch: 19  Accuracy: 98.4 %\n",
      "Batch: 20  Accuracy: 98.4 %\n",
      "Batch: 21  Accuracy: 99.2 %\n",
      "Batch: 22  Accuracy: 98.0 %\n",
      "Batch: 23  Accuracy: 97.6 %\n",
      "Batch: 24  Accuracy: 98.8 %\n",
      "Batch: 25  Accuracy: 97.2 %\n",
      "Batch: 26  Accuracy: 99.6 %\n",
      "Batch: 27  Accuracy: 98.0 %\n",
      "Batch: 28  Accuracy: 97.6 %\n",
      "Batch: 29  Accuracy: 98.4 %\n",
      "Batch: 30  Accuracy: 98.4 %\n",
      "Batch: 31  Accuracy: 99.2 %\n",
      "Batch: 32  Accuracy: 96.8 %\n",
      "Batch: 33  Accuracy: 98.0 %\n",
      "Batch: 34  Accuracy: 96.8 %\n",
      "Batch: 35  Accuracy: 96.39999999999999 %\n",
      "Batch: 36  Accuracy: 98.0 %\n",
      "Batch: 37  Accuracy: 97.6 %\n",
      "Batch: 38  Accuracy: 98.4 %\n",
      "Batch: 39  Accuracy: 100.0 %\n",
      "Batch: 40  Accuracy: 97.6 %\n",
      "Batch: 41  Accuracy: 98.0 %\n",
      "Batch: 42  Accuracy: 99.6 %\n",
      "Batch: 43  Accuracy: 99.2 %\n",
      "Batch: 44  Accuracy: 99.6 %\n",
      "Batch: 45  Accuracy: 100.0 %\n",
      "Batch: 46  Accuracy: 99.6 %\n",
      "Batch: 47  Accuracy: 98.0 %\n",
      "Batch: 48  Accuracy: 97.6 %\n",
      "Batch: 49  Accuracy: 99.6 %\n",
      "Batch: 50  Accuracy: 97.2 %\n",
      "Batch: 51  Accuracy: 98.0 %\n",
      "Batch: 52  Accuracy: 97.2 %\n",
      "Batch: 53  Accuracy: 99.2 %\n",
      "Batch: 54  Accuracy: 98.0 %\n",
      "Batch: 55  Accuracy: 98.4 %\n",
      "Batch: 56  Accuracy: 98.8 %\n",
      "Batch: 57  Accuracy: 98.8 %\n",
      "Batch: 58  Accuracy: 98.8 %\n",
      "Batch: 59  Accuracy: 98.4 %\n",
      "Batch: 60  Accuracy: 100.0 %\n",
      "Batch: 61  Accuracy: 97.6 %\n",
      "Batch: 62  Accuracy: 99.6 %\n",
      "Batch: 63  Accuracy: 96.8 %\n",
      "Batch: 64  Accuracy: 99.6 %\n",
      "Batch: 65  Accuracy: 99.6 %\n",
      "Batch: 66  Accuracy: 97.6 %\n",
      "Batch: 67  Accuracy: 99.2 %\n",
      "Batch: 68  Accuracy: 98.4 %\n",
      "Batch: 69  Accuracy: 99.2 %\n",
      "Batch: 70  Accuracy: 98.8 %\n",
      "Batch: 71  Accuracy: 99.2 %\n",
      "Batch: 72  Accuracy: 98.4 %\n",
      "Batch: 73  Accuracy: 98.0 %\n",
      "Batch: 74  Accuracy: 98.8 %\n",
      "Batch: 75  Accuracy: 100.0 %\n",
      "Batch: 76  Accuracy: 98.4 %\n",
      "Batch: 77  Accuracy: 98.8 %\n",
      "Batch: 78  Accuracy: 99.2 %\n",
      "Batch: 79  Accuracy: 99.2 %\n",
      "Batch: 80  Accuracy: 98.4 %\n",
      "Batch: 81  Accuracy: 99.6 %\n",
      "Batch: 82  Accuracy: 98.0 %\n",
      "Batch: 83  Accuracy: 98.8 %\n",
      "Batch: 84  Accuracy: 98.0 %\n",
      "Batch: 85  Accuracy: 98.8 %\n",
      "Batch: 86  Accuracy: 99.6 %\n",
      "Batch: 87  Accuracy: 98.4 %\n",
      "Batch: 88  Accuracy: 98.8 %\n",
      "Batch: 89  Accuracy: 100.0 %\n",
      "Batch: 90  Accuracy: 99.2 %\n",
      "Batch: 91  Accuracy: 98.8 %\n",
      "Batch: 92  Accuracy: 98.8 %\n",
      "Batch: 93  Accuracy: 99.2 %\n",
      "Batch: 94  Accuracy: 98.8 %\n",
      "Batch: 95  Accuracy: 98.4 %\n",
      "Batch: 96  Accuracy: 98.4 %\n",
      "Batch: 97  Accuracy: 100.0 %\n",
      "Batch: 98  Accuracy: 99.2 %\n",
      "Batch: 99  Accuracy: 99.2 %\n",
      "Batch: 100  Accuracy: 98.4 %\n",
      "Batch: 101  Accuracy: 99.2 %\n",
      "Batch: 102  Accuracy: 98.8 %\n",
      "Batch: 103  Accuracy: 97.6 %\n",
      "Batch: 104  Accuracy: 98.8 %\n",
      "Batch: 105  Accuracy: 98.0 %\n",
      "Batch: 106  Accuracy: 96.8 %\n",
      "Batch: 107  Accuracy: 98.8 %\n",
      "Batch: 108  Accuracy: 98.4 %\n",
      "Batch: 109  Accuracy: 98.8 %\n",
      "Batch: 110  Accuracy: 98.4 %\n",
      "Batch: 111  Accuracy: 99.2 %\n",
      "Batch: 112  Accuracy: 99.2 %\n",
      "Batch: 113  Accuracy: 98.0 %\n",
      "Batch: 114  Accuracy: 98.0 %\n",
      "Batch: 115  Accuracy: 98.8 %\n",
      "Batch: 116  Accuracy: 98.8 %\n",
      "Batch: 117  Accuracy: 98.8 %\n",
      "Batch: 118  Accuracy: 99.2 %\n",
      "Batch: 119  Accuracy: 99.2 %\n",
      "Batch: 120  Accuracy: 98.4 %\n",
      "Batch: 121  Accuracy: 100.0 %\n",
      "Batch: 122  Accuracy: 99.2 %\n",
      "Batch: 123  Accuracy: 98.0 %\n",
      "Batch: 124  Accuracy: 98.8 %\n",
      "Batch: 125  Accuracy: 98.0 %\n",
      "Batch: 126  Accuracy: 96.8 %\n",
      "Batch: 127  Accuracy: 98.0 %\n",
      "Batch: 128  Accuracy: 98.4 %\n",
      "Batch: 129  Accuracy: 97.2 %\n",
      "Batch: 130  Accuracy: 99.2 %\n",
      "Batch: 131  Accuracy: 98.4 %\n",
      "Batch: 132  Accuracy: 99.2 %\n",
      "Batch: 133  Accuracy: 98.4 %\n",
      "Batch: 134  Accuracy: 99.2 %\n",
      "Batch: 135  Accuracy: 99.6 %\n",
      "Batch: 136  Accuracy: 99.2 %\n",
      "Batch: 137  Accuracy: 99.2 %\n",
      "Batch: 138  Accuracy: 98.0 %\n",
      "Batch: 139  Accuracy: 97.2 %\n",
      "Batch: 140  Accuracy: 98.8 %\n",
      "Batch: 141  Accuracy: 98.4 %\n",
      "Batch: 142  Accuracy: 98.8 %\n",
      "Batch: 143  Accuracy: 98.8 %\n",
      "Batch: 144  Accuracy: 97.6 %\n",
      "Batch: 145  Accuracy: 98.0 %\n",
      "Batch: 146  Accuracy: 98.4 %\n",
      "Batch: 147  Accuracy: 98.0 %\n",
      "Batch: 148  Accuracy: 98.0 %\n",
      "Batch: 149  Accuracy: 98.4 %\n",
      "Batch: 150  Accuracy: 98.8 %\n",
      "Batch: 151  Accuracy: 98.0 %\n",
      "Batch: 152  Accuracy: 99.2 %\n",
      "Batch: 153  Accuracy: 98.4 %\n",
      "Batch: 154  Accuracy: 98.4 %\n",
      "Batch: 155  Accuracy: 99.2 %\n",
      "Batch: 156  Accuracy: 99.2 %\n",
      "Batch: 157  Accuracy: 96.8 %\n",
      "Batch: 158  Accuracy: 98.4 %\n",
      "Batch: 159  Accuracy: 99.2 %\n",
      "Batch: 160  Accuracy: 99.2 %\n",
      "Batch: 161  Accuracy: 99.2 %\n",
      "Batch: 162  Accuracy: 100.0 %\n",
      "Batch: 163  Accuracy: 98.0 %\n",
      "Batch: 164  Accuracy: 98.8 %\n",
      "Batch: 165  Accuracy: 96.8 %\n",
      "Batch: 166  Accuracy: 98.4 %\n",
      "Batch: 167  Accuracy: 98.0 %\n",
      "Batch: 168  Accuracy: 97.6 %\n",
      "Batch: 169  Accuracy: 99.6 %\n",
      "Batch: 170  Accuracy: 98.4 %\n",
      "Batch: 171  Accuracy: 98.0 %\n",
      "Batch: 172  Accuracy: 97.6 %\n",
      "Batch: 173  Accuracy: 99.2 %\n",
      "Batch: 174  Accuracy: 98.4 %\n",
      "Batch: 175  Accuracy: 99.6 %\n",
      "Batch: 176  Accuracy: 99.2 %\n",
      "Batch: 177  Accuracy: 97.2 %\n",
      "Batch: 178  Accuracy: 98.8 %\n",
      "Batch: 179  Accuracy: 98.4 %\n",
      "Batch: 180  Accuracy: 99.2 %\n",
      "Batch: 181  Accuracy: 98.0 %\n",
      "Batch: 182  Accuracy: 98.4 %\n",
      "Batch: 183  Accuracy: 97.2 %\n",
      "Batch: 184  Accuracy: 96.8 %\n",
      "Batch: 185  Accuracy: 95.6 %\n",
      "Batch: 186  Accuracy: 98.4 %\n",
      "Batch: 187  Accuracy: 98.8 %\n",
      "Batch: 188  Accuracy: 98.8 %\n",
      "Batch: 189  Accuracy: 97.6 %\n",
      "Batch: 190  Accuracy: 97.6 %\n",
      "Batch: 191  Accuracy: 98.4 %\n",
      "Batch: 192  Accuracy: 98.0 %\n",
      "Batch: 193  Accuracy: 98.8 %\n",
      "Batch: 194  Accuracy: 99.6 %\n",
      "Batch: 195  Accuracy: 97.6 %\n",
      "Batch: 196  Accuracy: 98.0 %\n",
      "Batch: 197  Accuracy: 99.6 %\n",
      "Batch: 198  Accuracy: 97.6 %\n",
      "Batch: 199  Accuracy: 96.8 %\n",
      "Batch: 200  Accuracy: 98.8 %\n",
      "Batch: 201  Accuracy: 99.2 %\n",
      "Batch: 202  Accuracy: 98.8 %\n",
      "Batch: 203  Accuracy: 98.0 %\n",
      "Batch: 204  Accuracy: 98.8 %\n",
      "Batch: 205  Accuracy: 98.4 %\n",
      "Batch: 206  Accuracy: 99.6 %\n",
      "Batch: 207  Accuracy: 98.4 %\n",
      "Batch: 208  Accuracy: 97.2 %\n",
      "Batch: 209  Accuracy: 98.4 %\n",
      "Batch: 210  Accuracy: 99.6 %\n",
      "Batch: 211  Accuracy: 98.0 %\n",
      "Batch: 212  Accuracy: 98.8 %\n",
      "Batch: 213  Accuracy: 99.6 %\n",
      "Batch: 214  Accuracy: 98.8 %\n",
      "Batch: 215  Accuracy: 98.4 %\n",
      "Batch: 216  Accuracy: 99.2 %\n",
      "Batch: 217  Accuracy: 98.4 %\n",
      "Batch: 218  Accuracy: 98.8 %\n",
      "Batch: 219  Accuracy: 97.6 %\n",
      "Batch: 220  Accuracy: 100.0 %\n",
      "Batch: 221  Accuracy: 98.0 %\n",
      "Batch: 222  Accuracy: 98.8 %\n",
      "Batch: 223  Accuracy: 99.2 %\n",
      "Batch: 224  Accuracy: 99.2 %\n",
      "Batch: 225  Accuracy: 98.8 %\n",
      "Batch: 226  Accuracy: 98.8 %\n",
      "Batch: 227  Accuracy: 98.0 %\n",
      "Batch: 228  Accuracy: 99.2 %\n",
      "Batch: 229  Accuracy: 98.4 %\n",
      "Batch: 230  Accuracy: 98.4 %\n",
      "Batch: 231  Accuracy: 99.2 %\n",
      "Batch: 232  Accuracy: 99.2 %\n",
      "Batch: 233  Accuracy: 99.6 %\n",
      "Batch: 234  Accuracy: 99.6 %\n",
      "Batch: 235  Accuracy: 98.4 %\n",
      "Batch: 236  Accuracy: 100.0 %\n",
      "Batch: 237  Accuracy: 100.0 %\n",
      "Batch: 238  Accuracy: 97.6 %\n",
      "Batch: 239  Accuracy: 99.6 %\n",
      "Epoch: 8  Accuracy: 98.57333333333334 %  loss: 0.04637087022637904\n",
      "Batch: 0  Accuracy: 98.0 %\n",
      "Batch: 1  Accuracy: 98.8 %\n",
      "Batch: 2  Accuracy: 98.8 %\n",
      "Batch: 3  Accuracy: 98.0 %\n",
      "Batch: 4  Accuracy: 98.8 %\n",
      "Batch: 5  Accuracy: 97.6 %\n",
      "Batch: 6  Accuracy: 99.2 %\n",
      "Batch: 7  Accuracy: 98.4 %\n",
      "Batch: 8  Accuracy: 99.6 %\n",
      "Batch: 9  Accuracy: 99.6 %\n",
      "Batch: 10  Accuracy: 98.0 %\n",
      "Batch: 11  Accuracy: 100.0 %\n",
      "Batch: 12  Accuracy: 99.6 %\n",
      "Batch: 13  Accuracy: 99.6 %\n",
      "Batch: 14  Accuracy: 98.4 %\n",
      "Batch: 15  Accuracy: 99.2 %\n",
      "Batch: 16  Accuracy: 99.6 %\n",
      "Batch: 17  Accuracy: 98.0 %\n",
      "Batch: 18  Accuracy: 99.2 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a43e2a7196f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mcnn5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_100_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_100_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.075\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time cost : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-80d1398969f4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_x, train_y, epoch_count, lr, bs)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mtemp_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-80d1398969f4>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdJdao\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdJdai\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m####\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0842da01c849>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool_prime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLeakyReLU_prime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_prime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-0842da01c849>\u001b[0m in \u001b[0;36mconv2d_prime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                             self.dJdai[b, ch, r, c] += np.sum(rotated_W[k, ch, :, :] * \n\u001b[1;32m---> 96\u001b[1;33m                                                               zp_dJdz[b, k, r:r+k_rows, c:c+k_cols])\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "input_data_spec = [250, 1, 28, 28]\n",
    "conv_layer_spec = [{\"k_num\" : 6, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2},\n",
    "                   {\"k_num\" : 16, \"k_h\" : 3, \"k_w\" : 3, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2}]\n",
    "fc_layer_spec = [400,120, 84, 10]\n",
    "cnn5 = CNN(input_data_spec=input_data_spec, conv_layer_spec=conv_layer_spec)\n",
    "f = open(\"C:\\\\Users\\\\Herry\\\\Desktop\\\\十大重要演算法\\\\Neural Net\\\\minist\\\\mnist_train_60000.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "\n",
    "\n",
    "train_100_x = np.clip(np.array(x), 0, 1).reshape(60000, 1, 28 ,28)\n",
    "train_100_y = np.array(y)\n",
    "\n",
    "t = time.time()\n",
    "cnn5.train(train_100_x, train_100_y, 150, 0.075, 250)\n",
    "print(\"Time cost : \", time.time() - t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Train with 60000 digits using leakyReLU\n",
    "\n",
    "batch_size = 100<br>\n",
    "epoch = 10000<br>\n",
    "learning rate = 0.1 <br>\n",
    "Eta = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:\\\\Users\\\\Herry\\\\Desktop\\\\十大重要演算法\\\\Neural Net\\\\minist\\\\mnist_train_60000.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "\n",
    "train_60000_x = np.array(x).T\n",
    "train_60000_y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Training Accuracy= 0.5800000000000001 loss =  1.3283172753469286\n",
      "Epoch  1\n",
      "Training Accuracy= 0.73 loss =  0.9769998353342552\n",
      "Epoch  2\n",
      "Training Accuracy= 0.76 loss =  0.7915607635621521\n",
      "Epoch  3\n",
      "Training Accuracy= 0.8 loss =  0.6767651655873393\n",
      "Epoch  4\n",
      "Training Accuracy= 0.81 loss =  0.6165497878888615\n",
      "Epoch  5\n",
      "Training Accuracy= 0.83 loss =  0.5777390255015582\n",
      "Epoch  6\n",
      "Training Accuracy= 0.84 loss =  0.5516322658465923\n",
      "Epoch  7\n",
      "Training Accuracy= 0.84 loss =  0.533071033438443\n",
      "Epoch  8\n",
      "Training Accuracy= 0.85 loss =  0.5192418826313333\n",
      "Epoch  9\n",
      "Training Accuracy= 0.85 loss =  0.5087690686212121\n",
      "Epoch  10\n",
      "Training Accuracy= 0.85 loss =  0.5003295076828531\n",
      "Epoch  11\n",
      "Training Accuracy= 0.87 loss =  0.4612341066670279\n",
      "Epoch  12\n",
      "Training Accuracy= 0.94 loss =  0.28005691582242775\n",
      "Epoch  13\n",
      "Training Accuracy= 0.94 loss =  0.25149847065757847\n",
      "Epoch  14\n",
      "Training Accuracy= 0.95 loss =  0.244662750611261\n",
      "Epoch  15\n",
      "Training Accuracy= 0.95 loss =  0.2424847086495012\n",
      "Epoch  16\n",
      "Training Accuracy= 0.95 loss =  0.24245426735176515\n",
      "Epoch  17\n",
      "Training Accuracy= 0.95 loss =  0.2431969061102348\n",
      "Epoch  18\n",
      "Training Accuracy= 0.95 loss =  0.24387686305198367\n",
      "Epoch  19\n",
      "Training Accuracy= 0.95 loss =  0.24420416918841892\n",
      "Epoch  20\n",
      "Training Accuracy= 0.95 loss =  0.24453964883680668\n",
      "Epoch  21\n",
      "Training Accuracy= 0.95 loss =  0.24474857622184723\n",
      "Epoch  22\n",
      "Training Accuracy= 0.95 loss =  0.2447745104237858\n",
      "Epoch  23\n",
      "Training Accuracy= 0.95 loss =  0.24481476888161202\n",
      "Epoch  24\n",
      "Training Accuracy= 0.95 loss =  0.24452817973412377\n",
      "Epoch  25\n",
      "Training Accuracy= 0.96 loss =  0.24412283664535683\n",
      "Epoch  26\n",
      "Training Accuracy= 0.95 loss =  0.24353543824920842\n",
      "Epoch  27\n",
      "Training Accuracy= 0.95 loss =  0.24272111650225678\n",
      "Epoch  28\n",
      "Training Accuracy= 0.95 loss =  0.24207627132604792\n",
      "Epoch  29\n",
      "Training Accuracy= 0.95 loss =  0.2412838593618804\n",
      "Epoch  30\n",
      "Training Accuracy= 0.95 loss =  0.24046939839310782\n",
      "Epoch  31\n",
      "Training Accuracy= 0.95 loss =  0.23964843264148472\n",
      "Epoch  32\n",
      "Training Accuracy= 0.95 loss =  0.23870510398821695\n",
      "Epoch  33\n",
      "Training Accuracy= 0.95 loss =  0.23771294134532228\n",
      "Epoch  34\n",
      "Training Accuracy= 0.95 loss =  0.23677177010054026\n",
      "Epoch  35\n",
      "Training Accuracy= 0.95 loss =  0.23577633857279023\n",
      "Epoch  36\n",
      "Training Accuracy= 0.95 loss =  0.23488553518562355\n",
      "Epoch  37\n",
      "Training Accuracy= 0.95 loss =  0.23400790290830759\n",
      "Epoch  38\n",
      "Training Accuracy= 0.95 loss =  0.23303039670299383\n",
      "Epoch  39\n",
      "Training Accuracy= 0.95 loss =  0.23209250399765144\n",
      "Epoch  40\n",
      "Training Accuracy= 0.95 loss =  0.23115294838147735\n",
      "Epoch  41\n",
      "Training Accuracy= 0.95 loss =  0.23024383621211508\n",
      "Epoch  42\n",
      "Training Accuracy= 0.95 loss =  0.2293246636034572\n",
      "Epoch  43\n",
      "Training Accuracy= 0.95 loss =  0.22845563177098016\n",
      "Epoch  44\n",
      "Training Accuracy= 0.95 loss =  0.22764481990311378\n",
      "Epoch  45\n",
      "Training Accuracy= 0.95 loss =  0.2267775493257348\n",
      "Epoch  46\n",
      "Training Accuracy= 0.96 loss =  0.22588304324488315\n",
      "Epoch  47\n",
      "Training Accuracy= 0.96 loss =  0.2249083711719295\n",
      "Epoch  48\n",
      "Training Accuracy= 0.96 loss =  0.2240284295402845\n",
      "Epoch  49\n",
      "Training Accuracy= 0.96 loss =  0.2231932563268076\n",
      "Epoch  50\n",
      "Training Accuracy= 0.96 loss =  0.2223837677278092\n",
      "Epoch  51\n",
      "Training Accuracy= 0.96 loss =  0.22165121771773963\n",
      "Epoch  52\n",
      "Training Accuracy= 0.96 loss =  0.2209300167912161\n",
      "Epoch  53\n",
      "Training Accuracy= 0.96 loss =  0.22037764793524858\n",
      "Epoch  54\n",
      "Training Accuracy= 0.96 loss =  0.21981469327554945\n",
      "Epoch  55\n",
      "Training Accuracy= 0.96 loss =  0.2193107707060849\n",
      "Epoch  56\n",
      "Training Accuracy= 0.96 loss =  0.21873529853915638\n",
      "Epoch  57\n",
      "Training Accuracy= 0.96 loss =  0.21807779411386377\n",
      "Epoch  58\n",
      "Training Accuracy= 0.96 loss =  0.2175310494146761\n",
      "Epoch  59\n",
      "Training Accuracy= 0.96 loss =  0.2169562757477183\n",
      "Epoch  60\n",
      "Training Accuracy= 0.96 loss =  0.216542273405712\n",
      "Epoch  61\n",
      "Training Accuracy= 0.96 loss =  0.2160793201927669\n",
      "Epoch  62\n",
      "Training Accuracy= 0.96 loss =  0.21560075237656542\n",
      "Epoch  63\n",
      "Training Accuracy= 0.96 loss =  0.21514021541506714\n",
      "Epoch  64\n",
      "Training Accuracy= 0.96 loss =  0.21473483919775121\n",
      "Epoch  65\n",
      "Training Accuracy= 0.96 loss =  0.21441951740289017\n",
      "Epoch  66\n",
      "Training Accuracy= 0.96 loss =  0.21407185440116508\n",
      "Epoch  67\n",
      "Training Accuracy= 0.96 loss =  0.21372248913329647\n",
      "Epoch  68\n",
      "Training Accuracy= 0.96 loss =  0.21338966287295574\n",
      "Epoch  69\n",
      "Training Accuracy= 0.96 loss =  0.21309999735452184\n",
      "Epoch  70\n",
      "Training Accuracy= 0.96 loss =  0.21287853007959484\n",
      "Epoch  71\n",
      "Training Accuracy= 0.96 loss =  0.21263617568692794\n",
      "Epoch  72\n",
      "Training Accuracy= 0.96 loss =  0.21230191192746928\n",
      "Epoch  73\n",
      "Training Accuracy= 0.96 loss =  0.21197900809616094\n",
      "Epoch  74\n",
      "Training Accuracy= 0.96 loss =  0.2116118453211048\n",
      "Epoch  75\n",
      "Training Accuracy= 0.96 loss =  0.2113196009045025\n",
      "Epoch  76\n",
      "Training Accuracy= 0.96 loss =  0.21096656210801001\n",
      "Epoch  77\n",
      "Training Accuracy= 0.96 loss =  0.21062407160815166\n",
      "Epoch  78\n",
      "Training Accuracy= 0.96 loss =  0.21025985431660527\n",
      "Epoch  79\n",
      "Training Accuracy= 0.96 loss =  0.20989663749609938\n",
      "Epoch  80\n",
      "Training Accuracy= 0.96 loss =  0.20949858849631658\n",
      "Epoch  81\n",
      "Training Accuracy= 0.96 loss =  0.20909893872981206\n",
      "Epoch  82\n",
      "Training Accuracy= 0.96 loss =  0.20876017330899088\n",
      "Epoch  83\n",
      "Training Accuracy= 0.96 loss =  0.20848085415715137\n",
      "Epoch  84\n",
      "Training Accuracy= 0.96 loss =  0.20823451382377398\n",
      "Epoch  85\n",
      "Training Accuracy= 0.96 loss =  0.2080010572970256\n",
      "Epoch  86\n",
      "Training Accuracy= 0.96 loss =  0.2074976557786812\n",
      "Epoch  87\n",
      "Training Accuracy= 0.96 loss =  0.20704935541388944\n",
      "Epoch  88\n",
      "Training Accuracy= 0.96 loss =  0.2066742549902173\n",
      "Epoch  89\n",
      "Training Accuracy= 0.96 loss =  0.20630558198323173\n",
      "Epoch  90\n",
      "Training Accuracy= 0.96 loss =  0.20594532731514598\n",
      "Epoch  91\n",
      "Training Accuracy= 0.96 loss =  0.20560415138254687\n",
      "Epoch  92\n",
      "Training Accuracy= 0.96 loss =  0.2052396747950243\n",
      "Epoch  93\n",
      "Training Accuracy= 0.96 loss =  0.20482444448186315\n",
      "Epoch  94\n",
      "Training Accuracy= 0.96 loss =  0.20440373903770692\n",
      "Epoch  95\n",
      "Training Accuracy= 0.96 loss =  0.20397867087287935\n",
      "Epoch  96\n",
      "Training Accuracy= 0.96 loss =  0.20353674367724559\n",
      "Epoch  97\n",
      "Training Accuracy= 0.96 loss =  0.2030484966412277\n",
      "Epoch  98\n",
      "Training Accuracy= 0.96 loss =  0.20257639234571362\n",
      "Epoch  99\n",
      "Training Accuracy= 0.96 loss =  0.2021116321419366\n",
      "Epoch  100\n",
      "Training Accuracy= 0.96 loss =  0.20162825006163415\n",
      "Epoch  101\n",
      "Training Accuracy= 0.96 loss =  0.20110663323230354\n",
      "Epoch  102\n",
      "Training Accuracy= 0.96 loss =  0.20053478874978087\n",
      "Epoch  103\n",
      "Training Accuracy= 0.96 loss =  0.19996413936136076\n",
      "Epoch  104\n",
      "Training Accuracy= 0.96 loss =  0.1994082122126691\n",
      "Epoch  105\n",
      "Training Accuracy= 0.96 loss =  0.1989063196779804\n",
      "Epoch  106\n",
      "Training Accuracy= 0.96 loss =  0.198373827692158\n",
      "Epoch  107\n",
      "Training Accuracy= 0.96 loss =  0.19790985415700219\n",
      "Epoch  108\n",
      "Training Accuracy= 0.96 loss =  0.19749844697212854\n",
      "Epoch  109\n",
      "Training Accuracy= 0.96 loss =  0.1970440627202722\n",
      "Epoch  110\n",
      "Training Accuracy= 0.96 loss =  0.1965815771890193\n",
      "Epoch  111\n",
      "Training Accuracy= 0.96 loss =  0.19615668980595818\n",
      "Epoch  112\n",
      "Training Accuracy= 0.96 loss =  0.19569855080481896\n",
      "Epoch  113\n",
      "Training Accuracy= 0.96 loss =  0.1952340294535202\n",
      "Epoch  114\n",
      "Training Accuracy= 0.96 loss =  0.19471443521213355\n",
      "Epoch  115\n",
      "Training Accuracy= 0.96 loss =  0.19421285692593904\n",
      "Epoch  116\n",
      "Training Accuracy= 0.96 loss =  0.19368814796658917\n",
      "Epoch  117\n",
      "Training Accuracy= 0.96 loss =  0.19311622391587266\n",
      "Epoch  118\n",
      "Training Accuracy= 0.96 loss =  0.1925861367351585\n",
      "Epoch  119\n",
      "Training Accuracy= 0.96 loss =  0.19201388187578858\n",
      "Epoch  120\n",
      "Training Accuracy= 0.96 loss =  0.19145900114986175\n",
      "Epoch  121\n",
      "Training Accuracy= 0.96 loss =  0.1908625343128562\n",
      "Epoch  122\n",
      "Training Accuracy= 0.97 loss =  0.1902580002936342\n",
      "Epoch  123\n",
      "Training Accuracy= 0.97 loss =  0.18960900598614266\n",
      "Epoch  124\n",
      "Training Accuracy= 0.97 loss =  0.188984248059177\n",
      "Epoch  125\n",
      "Training Accuracy= 0.97 loss =  0.18836147932364314\n",
      "Epoch  126\n",
      "Training Accuracy= 0.97 loss =  0.18789478594831133\n",
      "Epoch  127\n",
      "Training Accuracy= 0.97 loss =  0.1873312003584662\n",
      "Epoch  128\n",
      "Training Accuracy= 0.97 loss =  0.18678039433874172\n",
      "Epoch  129\n",
      "Training Accuracy= 0.97 loss =  0.18627736249933707\n",
      "Epoch  130\n",
      "Training Accuracy= 0.97 loss =  0.18570990484852157\n",
      "Epoch  131\n",
      "Training Accuracy= 0.97 loss =  0.18506621451400312\n",
      "Epoch  132\n",
      "Training Accuracy= 0.97 loss =  0.18444067185818352\n",
      "Epoch  133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.18385041851150583\n",
      "Epoch  134\n",
      "Training Accuracy= 0.97 loss =  0.18324149216628194\n",
      "Epoch  135\n",
      "Training Accuracy= 0.97 loss =  0.18266500893886145\n",
      "Epoch  136\n",
      "Training Accuracy= 0.97 loss =  0.18210760266273185\n",
      "Epoch  137\n",
      "Training Accuracy= 0.97 loss =  0.18153587841710006\n",
      "Epoch  138\n",
      "Training Accuracy= 0.97 loss =  0.18092178750896826\n",
      "Epoch  139\n",
      "Training Accuracy= 0.97 loss =  0.1803354590754298\n",
      "Epoch  140\n",
      "Training Accuracy= 0.97 loss =  0.17970678820608812\n",
      "Epoch  141\n",
      "Training Accuracy= 0.97 loss =  0.1791071718492589\n",
      "Epoch  142\n",
      "Training Accuracy= 0.97 loss =  0.17846862455555734\n",
      "Epoch  143\n",
      "Training Accuracy= 0.97 loss =  0.1778693453347735\n",
      "Epoch  144\n",
      "Training Accuracy= 0.97 loss =  0.17727883954177273\n",
      "Epoch  145\n",
      "Training Accuracy= 0.97 loss =  0.17667002324625483\n",
      "Epoch  146\n",
      "Training Accuracy= 0.97 loss =  0.17613510012962103\n",
      "Epoch  147\n",
      "Training Accuracy= 0.97 loss =  0.17555713816865634\n",
      "Epoch  148\n",
      "Training Accuracy= 0.97 loss =  0.1751011353048409\n",
      "Epoch  149\n",
      "Training Accuracy= 0.97 loss =  0.17449841086672685\n",
      "Epoch  150\n",
      "Training Accuracy= 0.97 loss =  0.17389140305312534\n",
      "Epoch  151\n",
      "Training Accuracy= 0.97 loss =  0.1733035276193182\n",
      "Epoch  152\n",
      "Training Accuracy= 0.97 loss =  0.17271832064359824\n",
      "Epoch  153\n",
      "Training Accuracy= 0.97 loss =  0.17194918873707093\n",
      "Epoch  154\n",
      "Training Accuracy= 0.97 loss =  0.17136810316395615\n",
      "Epoch  155\n",
      "Training Accuracy= 0.97 loss =  0.1707836627192979\n",
      "Epoch  156\n",
      "Training Accuracy= 0.97 loss =  0.17022658961942286\n",
      "Epoch  157\n",
      "Training Accuracy= 0.97 loss =  0.169664108976882\n",
      "Epoch  158\n",
      "Training Accuracy= 0.97 loss =  0.16911587135173922\n",
      "Epoch  159\n",
      "Training Accuracy= 0.97 loss =  0.16864800959568582\n",
      "Epoch  160\n",
      "Training Accuracy= 0.97 loss =  0.16813557398914253\n",
      "Epoch  161\n",
      "Training Accuracy= 0.97 loss =  0.16756273768083743\n",
      "Epoch  162\n",
      "Training Accuracy= 0.97 loss =  0.1669582597721876\n",
      "Epoch  163\n",
      "Training Accuracy= 0.97 loss =  0.16643986633744765\n",
      "Epoch  164\n",
      "Training Accuracy= 0.97 loss =  0.16597240399721017\n",
      "Epoch  165\n",
      "Training Accuracy= 0.97 loss =  0.16543630566838663\n",
      "Epoch  166\n",
      "Training Accuracy= 0.97 loss =  0.16491425277102864\n",
      "Epoch  167\n",
      "Training Accuracy= 0.97 loss =  0.16442764299942927\n",
      "Epoch  168\n",
      "Training Accuracy= 0.98 loss =  0.16395316877175553\n",
      "Epoch  169\n",
      "Training Accuracy= 0.97 loss =  0.1634260260618278\n",
      "Epoch  170\n",
      "Training Accuracy= 0.97 loss =  0.16294034338437952\n",
      "Epoch  171\n",
      "Training Accuracy= 0.97 loss =  0.16245300411774483\n",
      "Epoch  172\n",
      "Training Accuracy= 0.97 loss =  0.16193731436373224\n",
      "Epoch  173\n",
      "Training Accuracy= 0.97 loss =  0.16145663494470064\n",
      "Epoch  174\n",
      "Training Accuracy= 0.97 loss =  0.16099796909460265\n",
      "Epoch  175\n",
      "Training Accuracy= 0.97 loss =  0.16051856919549304\n",
      "Epoch  176\n",
      "Training Accuracy= 0.97 loss =  0.16002484407816858\n",
      "Epoch  177\n",
      "Training Accuracy= 0.97 loss =  0.15966079072137718\n",
      "Epoch  178\n",
      "Training Accuracy= 0.97 loss =  0.15920986058801084\n",
      "Epoch  179\n",
      "Training Accuracy= 0.97 loss =  0.15872350758553389\n",
      "Epoch  180\n",
      "Training Accuracy= 0.97 loss =  0.15829417149136893\n",
      "Epoch  181\n",
      "Training Accuracy= 0.97 loss =  0.1579347468877727\n",
      "Epoch  182\n",
      "Training Accuracy= 0.97 loss =  0.15753077798964132\n",
      "Epoch  183\n",
      "Training Accuracy= 0.97 loss =  0.157234649225455\n",
      "Epoch  184\n",
      "Training Accuracy= 0.97 loss =  0.1569177749113915\n",
      "Epoch  185\n",
      "Training Accuracy= 0.97 loss =  0.15651558142341118\n",
      "Epoch  186\n",
      "Training Accuracy= 0.97 loss =  0.15591673781163273\n",
      "Epoch  187\n",
      "Training Accuracy= 0.97 loss =  0.15546546025019575\n",
      "Epoch  188\n",
      "Training Accuracy= 0.97 loss =  0.15496227486335695\n",
      "Epoch  189\n",
      "Training Accuracy= 0.97 loss =  0.15445750510349815\n",
      "Epoch  190\n",
      "Training Accuracy= 0.97 loss =  0.15396901148590797\n",
      "Epoch  191\n",
      "Training Accuracy= 0.97 loss =  0.15351342069390178\n",
      "Epoch  192\n",
      "Training Accuracy= 0.97 loss =  0.15300034827738956\n",
      "Epoch  193\n",
      "Training Accuracy= 0.97 loss =  0.15257215343459699\n",
      "Epoch  194\n",
      "Training Accuracy= 0.97 loss =  0.1521482696486247\n",
      "Epoch  195\n",
      "Training Accuracy= 0.97 loss =  0.15161526024390282\n",
      "Epoch  196\n",
      "Training Accuracy= 0.97 loss =  0.15117442924116503\n",
      "Epoch  197\n",
      "Training Accuracy= 0.97 loss =  0.15073059910439598\n",
      "Epoch  198\n",
      "Training Accuracy= 0.97 loss =  0.1503095178026197\n",
      "Epoch  199\n",
      "Training Accuracy= 0.97 loss =  0.14996113665240346\n",
      "Epoch  200\n",
      "Training Accuracy= 0.97 loss =  0.14959895334801837\n",
      "Epoch  201\n",
      "Training Accuracy= 0.97 loss =  0.14938920163899788\n",
      "Epoch  202\n",
      "Training Accuracy= 0.97 loss =  0.14906298491518064\n",
      "Epoch  203\n",
      "Training Accuracy= 0.97 loss =  0.14866046339672137\n",
      "Epoch  204\n",
      "Training Accuracy= 0.97 loss =  0.14835494355172474\n",
      "Epoch  205\n",
      "Training Accuracy= 0.97 loss =  0.14803061514014237\n",
      "Epoch  206\n",
      "Training Accuracy= 0.97 loss =  0.14779119713632347\n",
      "Epoch  207\n",
      "Training Accuracy= 0.97 loss =  0.14754541973431945\n",
      "Epoch  208\n",
      "Training Accuracy= 0.97 loss =  0.14746521374181623\n",
      "Epoch  209\n",
      "Training Accuracy= 0.97 loss =  0.14727589794462517\n",
      "Epoch  210\n",
      "Training Accuracy= 0.97 loss =  0.14729764021888517\n",
      "Epoch  211\n",
      "Training Accuracy= 0.97 loss =  0.14710630685424972\n",
      "Epoch  212\n",
      "Training Accuracy= 0.97 loss =  0.1468807656702774\n",
      "Epoch  213\n",
      "Training Accuracy= 0.97 loss =  0.14667559192609908\n",
      "Epoch  214\n",
      "Training Accuracy= 0.97 loss =  0.14649198287688095\n",
      "Epoch  215\n",
      "Training Accuracy= 0.97 loss =  0.1463756521776437\n",
      "Epoch  216\n",
      "Training Accuracy= 0.97 loss =  0.1462036221830499\n",
      "Epoch  217\n",
      "Training Accuracy= 0.97 loss =  0.14605566355891542\n",
      "Epoch  218\n",
      "Training Accuracy= 0.97 loss =  0.14592014477342757\n",
      "Epoch  219\n",
      "Training Accuracy= 0.97 loss =  0.14583596507518096\n",
      "Epoch  220\n",
      "Training Accuracy= 0.97 loss =  0.14577633214428953\n",
      "Epoch  221\n",
      "Training Accuracy= 0.97 loss =  0.14577841950976278\n",
      "Epoch  222\n",
      "Training Accuracy= 0.97 loss =  0.14576236653766983\n",
      "Epoch  223\n",
      "Training Accuracy= 0.97 loss =  0.14578933300172878\n",
      "Epoch  224\n",
      "Training Accuracy= 0.97 loss =  0.14573015430274938\n",
      "Epoch  225\n",
      "Training Accuracy= 0.97 loss =  0.1457546655117002\n",
      "Epoch  226\n",
      "Training Accuracy= 0.97 loss =  0.1456794488805773\n",
      "Epoch  227\n",
      "Training Accuracy= 0.97 loss =  0.14551619257705548\n",
      "Epoch  228\n",
      "Training Accuracy= 0.97 loss =  0.14535770919615978\n",
      "Epoch  229\n",
      "Training Accuracy= 0.97 loss =  0.14531774865254313\n",
      "Epoch  230\n",
      "Training Accuracy= 0.97 loss =  0.14520762022568015\n",
      "Epoch  231\n",
      "Training Accuracy= 0.97 loss =  0.1451047402482269\n",
      "Epoch  232\n",
      "Training Accuracy= 0.97 loss =  0.14493949973610618\n",
      "Epoch  233\n",
      "Training Accuracy= 0.97 loss =  0.1447508317771247\n",
      "Epoch  234\n",
      "Training Accuracy= 0.97 loss =  0.14466750036700524\n",
      "Epoch  235\n",
      "Training Accuracy= 0.97 loss =  0.14465387419134018\n",
      "Epoch  236\n",
      "Training Accuracy= 0.97 loss =  0.14462297966048085\n",
      "Epoch  237\n",
      "Training Accuracy= 0.97 loss =  0.14458112603622714\n",
      "Epoch  238\n",
      "Training Accuracy= 0.97 loss =  0.14445000099622302\n",
      "Epoch  239\n",
      "Training Accuracy= 0.97 loss =  0.1444228705664553\n",
      "Epoch  240\n",
      "Training Accuracy= 0.97 loss =  0.14433107298273487\n",
      "Epoch  241\n",
      "Training Accuracy= 0.97 loss =  0.14426502316670026\n",
      "Epoch  242\n",
      "Training Accuracy= 0.97 loss =  0.14418602955046161\n",
      "Epoch  243\n",
      "Training Accuracy= 0.97 loss =  0.14407607278917878\n",
      "Epoch  244\n",
      "Training Accuracy= 0.97 loss =  0.1441295896198661\n",
      "Epoch  245\n",
      "Training Accuracy= 0.97 loss =  0.14402389367779908\n",
      "Epoch  246\n",
      "Training Accuracy= 0.97 loss =  0.1438424600970319\n",
      "Epoch  247\n",
      "Training Accuracy= 0.97 loss =  0.14366551976827158\n",
      "Epoch  248\n",
      "Training Accuracy= 0.97 loss =  0.14357961575602673\n",
      "Epoch  249\n",
      "Training Accuracy= 0.97 loss =  0.14339956587369435\n",
      "Epoch  250\n",
      "Training Accuracy= 0.97 loss =  0.14300194025736152\n",
      "Epoch  251\n",
      "Training Accuracy= 0.97 loss =  0.14283311010206506\n",
      "Epoch  252\n",
      "Training Accuracy= 0.97 loss =  0.14265527947669027\n",
      "Epoch  253\n",
      "Training Accuracy= 0.97 loss =  0.14251282665381992\n",
      "Epoch  254\n",
      "Training Accuracy= 0.97 loss =  0.14234219407470886\n",
      "Epoch  255\n",
      "Training Accuracy= 0.97 loss =  0.14227363128047457\n",
      "Epoch  256\n",
      "Training Accuracy= 0.97 loss =  0.142108895608769\n",
      "Epoch  257\n",
      "Training Accuracy= 0.97 loss =  0.14191761642552003\n",
      "Epoch  258\n",
      "Training Accuracy= 0.97 loss =  0.14169702725438413\n",
      "Epoch  259\n",
      "Training Accuracy= 0.97 loss =  0.14146235064297208\n",
      "Epoch  260\n",
      "Training Accuracy= 0.97 loss =  0.1411999026770947\n",
      "Epoch  261\n",
      "Training Accuracy= 0.97 loss =  0.14090845807734198\n",
      "Epoch  262\n",
      "Training Accuracy= 0.97 loss =  0.14058677201290332\n",
      "Epoch  263\n",
      "Training Accuracy= 0.97 loss =  0.14023921381986096\n",
      "Epoch  264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.1400026319881788\n",
      "Epoch  265\n",
      "Training Accuracy= 0.97 loss =  0.13967495742724978\n",
      "Epoch  266\n",
      "Training Accuracy= 0.97 loss =  0.13932969590296224\n",
      "Epoch  267\n",
      "Training Accuracy= 0.97 loss =  0.1389930038485824\n",
      "Epoch  268\n",
      "Training Accuracy= 0.97 loss =  0.13863344864318228\n",
      "Epoch  269\n",
      "Training Accuracy= 0.97 loss =  0.13824346444839247\n",
      "Epoch  270\n",
      "Training Accuracy= 0.97 loss =  0.1378768299546751\n",
      "Epoch  271\n",
      "Training Accuracy= 0.97 loss =  0.13753657623273402\n",
      "Epoch  272\n",
      "Training Accuracy= 0.97 loss =  0.13708399403855626\n",
      "Epoch  273\n",
      "Training Accuracy= 0.97 loss =  0.1366546479399247\n",
      "Epoch  274\n",
      "Training Accuracy= 0.97 loss =  0.13618585436275235\n",
      "Epoch  275\n",
      "Training Accuracy= 0.97 loss =  0.13575710493771298\n",
      "Epoch  276\n",
      "Training Accuracy= 0.97 loss =  0.13535906657264113\n",
      "Epoch  277\n",
      "Training Accuracy= 0.97 loss =  0.1349180626597442\n",
      "Epoch  278\n",
      "Training Accuracy= 0.97 loss =  0.1344370908686206\n",
      "Epoch  279\n",
      "Training Accuracy= 0.97 loss =  0.13381345789856247\n",
      "Epoch  280\n",
      "Training Accuracy= 0.97 loss =  0.1334258964314072\n",
      "Epoch  281\n",
      "Training Accuracy= 0.97 loss =  0.13295749968380216\n",
      "Epoch  282\n",
      "Training Accuracy= 0.97 loss =  0.132556646608524\n",
      "Epoch  283\n",
      "Training Accuracy= 0.97 loss =  0.13213407777416158\n",
      "Epoch  284\n",
      "Training Accuracy= 0.97 loss =  0.1318374164525282\n",
      "Epoch  285\n",
      "Training Accuracy= 0.97 loss =  0.13149526293723723\n",
      "Epoch  286\n",
      "Training Accuracy= 0.97 loss =  0.1311273673853627\n",
      "Epoch  287\n",
      "Training Accuracy= 0.97 loss =  0.13071264091542292\n",
      "Epoch  288\n",
      "Training Accuracy= 0.97 loss =  0.13024365355233875\n",
      "Epoch  289\n",
      "Training Accuracy= 0.97 loss =  0.1299268669029195\n",
      "Epoch  290\n",
      "Training Accuracy= 0.97 loss =  0.12966742065627945\n",
      "Epoch  291\n",
      "Training Accuracy= 0.97 loss =  0.12939630014385195\n",
      "Epoch  292\n",
      "Training Accuracy= 0.97 loss =  0.1290756842256966\n",
      "Epoch  293\n",
      "Training Accuracy= 0.97 loss =  0.12870239843156756\n",
      "Epoch  294\n",
      "Training Accuracy= 0.97 loss =  0.12841834455069517\n",
      "Epoch  295\n",
      "Training Accuracy= 0.97 loss =  0.12802372560872205\n",
      "Epoch  296\n",
      "Training Accuracy= 0.97 loss =  0.12761810963428907\n",
      "Epoch  297\n",
      "Training Accuracy= 0.97 loss =  0.1272391186144885\n",
      "Epoch  298\n",
      "Training Accuracy= 0.97 loss =  0.12683492638156804\n",
      "Epoch  299\n",
      "Training Accuracy= 0.97 loss =  0.12647464605465952\n",
      "Epoch  300\n",
      "Training Accuracy= 0.97 loss =  0.1260504603854831\n",
      "Epoch  301\n",
      "Training Accuracy= 0.97 loss =  0.12575647348132146\n",
      "Epoch  302\n",
      "Training Accuracy= 0.97 loss =  0.12538470645175134\n",
      "Epoch  303\n",
      "Training Accuracy= 0.97 loss =  0.12500723548446546\n",
      "Epoch  304\n",
      "Training Accuracy= 0.97 loss =  0.12462598453465087\n",
      "Epoch  305\n",
      "Training Accuracy= 0.97 loss =  0.12425274852544314\n",
      "Epoch  306\n",
      "Training Accuracy= 0.97 loss =  0.12389177725039112\n",
      "Epoch  307\n",
      "Training Accuracy= 0.97 loss =  0.12354232820004486\n",
      "Epoch  308\n",
      "Training Accuracy= 0.97 loss =  0.12310339815313512\n",
      "Epoch  309\n",
      "Training Accuracy= 0.97 loss =  0.1228231318341369\n",
      "Epoch  310\n",
      "Training Accuracy= 0.97 loss =  0.12257533656340623\n",
      "Epoch  311\n",
      "Training Accuracy= 0.97 loss =  0.12213854704336258\n",
      "Epoch  312\n",
      "Training Accuracy= 0.97 loss =  0.1215497835634334\n",
      "Epoch  313\n",
      "Training Accuracy= 0.97 loss =  0.12095616787240537\n",
      "Epoch  314\n",
      "Training Accuracy= 0.97 loss =  0.12043312853925084\n",
      "Epoch  315\n",
      "Training Accuracy= 0.97 loss =  0.11991997104258695\n",
      "Epoch  316\n",
      "Training Accuracy= 0.97 loss =  0.1194013214675772\n",
      "Epoch  317\n",
      "Training Accuracy= 0.97 loss =  0.11842059560768359\n",
      "Epoch  318\n",
      "Training Accuracy= 0.97 loss =  0.11769494566104997\n",
      "Epoch  319\n",
      "Training Accuracy= 0.97 loss =  0.11713220109768942\n",
      "Epoch  320\n",
      "Training Accuracy= 0.97 loss =  0.11652798885919337\n",
      "Epoch  321\n",
      "Training Accuracy= 0.97 loss =  0.11589537445302768\n",
      "Epoch  322\n",
      "Training Accuracy= 0.97 loss =  0.11531454159114765\n",
      "Epoch  323\n",
      "Training Accuracy= 0.97 loss =  0.11481732841599326\n",
      "Epoch  324\n",
      "Training Accuracy= 0.97 loss =  0.11436714597717357\n",
      "Epoch  325\n",
      "Training Accuracy= 0.97 loss =  0.11391297965077271\n",
      "Epoch  326\n",
      "Training Accuracy= 0.97 loss =  0.1133255351885051\n",
      "Epoch  327\n",
      "Training Accuracy= 0.97 loss =  0.11270086800700797\n",
      "Epoch  328\n",
      "Training Accuracy= 0.97 loss =  0.11216386302391872\n",
      "Epoch  329\n",
      "Training Accuracy= 0.97 loss =  0.11165148937644571\n",
      "Epoch  330\n",
      "Training Accuracy= 0.97 loss =  0.11120674831814104\n",
      "Epoch  331\n",
      "Training Accuracy= 0.97 loss =  0.10967379329656285\n",
      "Epoch  332\n",
      "Training Accuracy= 0.97 loss =  0.1092133302949521\n",
      "Epoch  333\n",
      "Training Accuracy= 0.97 loss =  0.10867979127815726\n",
      "Epoch  334\n",
      "Training Accuracy= 0.97 loss =  0.10820243788161028\n",
      "Epoch  335\n",
      "Training Accuracy= 0.97 loss =  0.10765368486259044\n",
      "Epoch  336\n",
      "Training Accuracy= 0.97 loss =  0.1072102437659461\n",
      "Epoch  337\n",
      "Training Accuracy= 0.97 loss =  0.1067392074268052\n",
      "Epoch  338\n",
      "Training Accuracy= 0.97 loss =  0.10635655779193014\n",
      "Epoch  339\n",
      "Training Accuracy= 0.97 loss =  0.10592338673117663\n",
      "Epoch  340\n",
      "Training Accuracy= 0.97 loss =  0.10557222271405599\n",
      "Epoch  341\n",
      "Training Accuracy= 0.97 loss =  0.10525217887146175\n",
      "Epoch  342\n",
      "Training Accuracy= 0.97 loss =  0.10494470595008454\n",
      "Epoch  343\n",
      "Training Accuracy= 0.97 loss =  0.10455414611437558\n",
      "Epoch  344\n",
      "Training Accuracy= 0.97 loss =  0.10414398197473122\n",
      "Epoch  345\n",
      "Training Accuracy= 0.97 loss =  0.10388787484369663\n",
      "Epoch  346\n",
      "Training Accuracy= 0.97 loss =  0.10364691816757443\n",
      "Epoch  347\n",
      "Training Accuracy= 0.97 loss =  0.10337135868824451\n",
      "Epoch  348\n",
      "Training Accuracy= 0.97 loss =  0.10305662756572102\n",
      "Epoch  349\n",
      "Training Accuracy= 0.97 loss =  0.10274767391321263\n",
      "Epoch  350\n",
      "Training Accuracy= 0.97 loss =  0.1024738954309577\n",
      "Epoch  351\n",
      "Training Accuracy= 0.97 loss =  0.10205044613819844\n",
      "Epoch  352\n",
      "Training Accuracy= 0.97 loss =  0.10189626779946356\n",
      "Epoch  353\n",
      "Training Accuracy= 0.97 loss =  0.10173612466909758\n",
      "Epoch  354\n",
      "Training Accuracy= 0.97 loss =  0.10149210222551037\n",
      "Epoch  355\n",
      "Training Accuracy= 0.97 loss =  0.10126734953776315\n",
      "Epoch  356\n",
      "Training Accuracy= 0.97 loss =  0.10106506205702029\n",
      "Epoch  357\n",
      "Training Accuracy= 0.97 loss =  0.10035358057171662\n",
      "Epoch  358\n",
      "Training Accuracy= 0.97 loss =  0.10007693031503487\n",
      "Epoch  359\n",
      "Training Accuracy= 0.97 loss =  0.09987785420289073\n",
      "Epoch  360\n",
      "Training Accuracy= 0.97 loss =  0.09980750140604781\n",
      "Epoch  361\n",
      "Training Accuracy= 0.97 loss =  0.09970438390943422\n",
      "Epoch  362\n",
      "Training Accuracy= 0.97 loss =  0.0996509728564806\n",
      "Epoch  363\n",
      "Training Accuracy= 0.97 loss =  0.09939859733625478\n",
      "Epoch  364\n",
      "Training Accuracy= 0.97 loss =  0.09939473134637357\n",
      "Epoch  365\n",
      "Training Accuracy= 0.97 loss =  0.0992616802038821\n",
      "Epoch  366\n",
      "Training Accuracy= 0.97 loss =  0.09925173096133186\n",
      "Epoch  367\n",
      "Training Accuracy= 0.97 loss =  0.09912238778863736\n",
      "Epoch  368\n",
      "Training Accuracy= 0.97 loss =  0.09899137172496393\n",
      "Epoch  369\n",
      "Training Accuracy= 0.97 loss =  0.09892767912028005\n",
      "Epoch  370\n",
      "Training Accuracy= 0.97 loss =  0.09884424917486076\n",
      "Epoch  371\n",
      "Training Accuracy= 0.97 loss =  0.09827961305693594\n",
      "Epoch  372\n",
      "Training Accuracy= 0.97 loss =  0.09837514762381255\n",
      "Epoch  373\n",
      "Training Accuracy= 0.97 loss =  0.09844341031209758\n",
      "Epoch  374\n",
      "Training Accuracy= 0.97 loss =  0.09844272527135624\n",
      "Epoch  375\n",
      "Training Accuracy= 0.97 loss =  0.09854502262635065\n",
      "Epoch  376\n",
      "Training Accuracy= 0.97 loss =  0.09847604565352468\n",
      "Epoch  377\n",
      "Training Accuracy= 0.97 loss =  0.09845700835233051\n",
      "Epoch  378\n",
      "Training Accuracy= 0.97 loss =  0.09847269773050904\n",
      "Epoch  379\n",
      "Training Accuracy= 0.97 loss =  0.09873291030350048\n",
      "Epoch  380\n",
      "Training Accuracy= 0.97 loss =  0.09879365328968853\n",
      "Epoch  381\n",
      "Training Accuracy= 0.97 loss =  0.09887994825171972\n",
      "Epoch  382\n",
      "Training Accuracy= 0.97 loss =  0.09897512503630101\n",
      "Epoch  383\n",
      "Training Accuracy= 0.97 loss =  0.09914718801544742\n",
      "Epoch  384\n",
      "Training Accuracy= 0.97 loss =  0.09923973475332323\n",
      "Epoch  385\n",
      "Training Accuracy= 0.97 loss =  0.09906515976997762\n",
      "Epoch  386\n",
      "Training Accuracy= 0.97 loss =  0.09906545290464319\n",
      "Epoch  387\n",
      "Training Accuracy= 0.97 loss =  0.09912042089557235\n",
      "Epoch  388\n",
      "Training Accuracy= 0.97 loss =  0.09914343854110373\n",
      "Epoch  389\n",
      "Training Accuracy= 0.97 loss =  0.09919371316898525\n",
      "Epoch  390\n",
      "Training Accuracy= 0.97 loss =  0.09919558392419425\n",
      "Epoch  391\n",
      "Training Accuracy= 0.97 loss =  0.0992544391162491\n",
      "Epoch  392\n",
      "Training Accuracy= 0.97 loss =  0.09926595986157934\n",
      "Epoch  393\n",
      "Training Accuracy= 0.97 loss =  0.09935452165783418\n",
      "Epoch  394\n",
      "Training Accuracy= 0.97 loss =  0.09941539989846099\n",
      "Epoch  395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.09948539599446683\n",
      "Epoch  396\n",
      "Training Accuracy= 0.97 loss =  0.09954223873640737\n",
      "Epoch  397\n",
      "Training Accuracy= 0.97 loss =  0.09958326595550951\n",
      "Epoch  398\n",
      "Training Accuracy= 0.97 loss =  0.09954417692345904\n",
      "Epoch  399\n",
      "Training Accuracy= 0.97 loss =  0.09952730386059344\n",
      "Epoch  400\n",
      "Training Accuracy= 0.97 loss =  0.09952187059094408\n",
      "Epoch  401\n",
      "Training Accuracy= 0.97 loss =  0.09958730353318694\n",
      "Epoch  402\n",
      "Training Accuracy= 0.97 loss =  0.09960994513597958\n",
      "Epoch  403\n",
      "Training Accuracy= 0.97 loss =  0.09961535135890333\n",
      "Epoch  404\n",
      "Training Accuracy= 0.97 loss =  0.09969518159813545\n",
      "Epoch  405\n",
      "Training Accuracy= 0.97 loss =  0.09981284101225434\n",
      "Epoch  406\n",
      "Training Accuracy= 0.97 loss =  0.09985428149796108\n",
      "Epoch  407\n",
      "Training Accuracy= 0.97 loss =  0.0999218450026167\n",
      "Epoch  408\n",
      "Training Accuracy= 0.97 loss =  0.1000040526915505\n",
      "Epoch  409\n",
      "Training Accuracy= 0.97 loss =  0.10011651807083809\n",
      "Epoch  410\n",
      "Training Accuracy= 0.97 loss =  0.1002248898260637\n",
      "Epoch  411\n",
      "Training Accuracy= 0.97 loss =  0.10033193398426948\n",
      "Epoch  412\n",
      "Training Accuracy= 0.97 loss =  0.10036176503705463\n",
      "Epoch  413\n",
      "Training Accuracy= 0.97 loss =  0.10030018870965127\n",
      "Epoch  414\n",
      "Training Accuracy= 0.97 loss =  0.10026771969940063\n",
      "Epoch  415\n",
      "Training Accuracy= 0.97 loss =  0.10018988146791091\n",
      "Epoch  416\n",
      "Training Accuracy= 0.97 loss =  0.10025097714890477\n",
      "Epoch  417\n",
      "Training Accuracy= 0.97 loss =  0.10027758083285498\n",
      "Epoch  418\n",
      "Training Accuracy= 0.97 loss =  0.10045329220207386\n",
      "Epoch  419\n",
      "Training Accuracy= 0.97 loss =  0.1006121742480086\n",
      "Epoch  420\n",
      "Training Accuracy= 0.97 loss =  0.10085105400030565\n",
      "Epoch  421\n",
      "Training Accuracy= 0.97 loss =  0.10102667343869401\n",
      "Epoch  422\n",
      "Training Accuracy= 0.97 loss =  0.1010815046470582\n",
      "Epoch  423\n",
      "Training Accuracy= 0.97 loss =  0.1011969502917281\n",
      "Epoch  424\n",
      "Training Accuracy= 0.97 loss =  0.10125854358397712\n",
      "Epoch  425\n",
      "Training Accuracy= 0.97 loss =  0.10136156911403921\n",
      "Epoch  426\n",
      "Training Accuracy= 0.97 loss =  0.10144441701837201\n",
      "Epoch  427\n",
      "Training Accuracy= 0.97 loss =  0.10159398324377442\n",
      "Epoch  428\n",
      "Training Accuracy= 0.97 loss =  0.1016499173665858\n",
      "Epoch  429\n",
      "Training Accuracy= 0.97 loss =  0.10180612265776744\n",
      "Epoch  430\n",
      "Training Accuracy= 0.97 loss =  0.10184302904397596\n",
      "Epoch  431\n",
      "Training Accuracy= 0.97 loss =  0.10200490506573812\n",
      "Epoch  432\n",
      "Training Accuracy= 0.97 loss =  0.10204767047153843\n",
      "Epoch  433\n",
      "Training Accuracy= 0.97 loss =  0.10200343221685569\n",
      "Epoch  434\n",
      "Training Accuracy= 0.97 loss =  0.1020539433089468\n",
      "Epoch  435\n",
      "Training Accuracy= 0.97 loss =  0.10250952353982087\n",
      "Epoch  436\n",
      "Training Accuracy= 0.97 loss =  0.10245034589964103\n",
      "Epoch  437\n",
      "Training Accuracy= 0.97 loss =  0.10246045538329557\n",
      "Epoch  438\n",
      "Training Accuracy= 0.97 loss =  0.10237111360321755\n",
      "Epoch  439\n",
      "Training Accuracy= 0.97 loss =  0.102375918409475\n",
      "Epoch  440\n",
      "Training Accuracy= 0.97 loss =  0.10239165851315905\n",
      "Epoch  441\n",
      "Training Accuracy= 0.97 loss =  0.10242607316363786\n",
      "Epoch  442\n",
      "Training Accuracy= 0.97 loss =  0.10241905776579102\n",
      "Epoch  443\n",
      "Training Accuracy= 0.97 loss =  0.10246341077033455\n",
      "Epoch  444\n",
      "Training Accuracy= 0.97 loss =  0.10256289082552947\n",
      "Epoch  445\n",
      "Training Accuracy= 0.97 loss =  0.10252365395701064\n",
      "Epoch  446\n",
      "Training Accuracy= 0.97 loss =  0.1025792666447404\n",
      "Epoch  447\n",
      "Training Accuracy= 0.97 loss =  0.10252833390753856\n",
      "Epoch  448\n",
      "Training Accuracy= 0.97 loss =  0.1025178676458581\n",
      "Epoch  449\n",
      "Training Accuracy= 0.97 loss =  0.10240470305482313\n",
      "Epoch  450\n",
      "Training Accuracy= 0.97 loss =  0.10233723191807625\n",
      "Epoch  451\n",
      "Training Accuracy= 0.97 loss =  0.10253228865175458\n",
      "Epoch  452\n",
      "Training Accuracy= 0.97 loss =  0.10249374207612125\n",
      "Epoch  453\n",
      "Training Accuracy= 0.97 loss =  0.10240838118364325\n",
      "Epoch  454\n",
      "Training Accuracy= 0.97 loss =  0.1022558146271074\n",
      "Epoch  455\n",
      "Training Accuracy= 0.97 loss =  0.10216821528146308\n",
      "Epoch  456\n",
      "Training Accuracy= 0.97 loss =  0.1023028138713712\n",
      "Epoch  457\n",
      "Training Accuracy= 0.97 loss =  0.10230103955044745\n",
      "Epoch  458\n",
      "Training Accuracy= 0.97 loss =  0.10217724815675026\n",
      "Epoch  459\n",
      "Training Accuracy= 0.97 loss =  0.10198502063230785\n",
      "Epoch  460\n",
      "Training Accuracy= 0.97 loss =  0.1017850591348361\n",
      "Epoch  461\n",
      "Training Accuracy= 0.97 loss =  0.10163012044770389\n",
      "Epoch  462\n",
      "Training Accuracy= 0.97 loss =  0.1015004551330789\n",
      "Epoch  463\n",
      "Training Accuracy= 0.97 loss =  0.10120950667030426\n",
      "Epoch  464\n",
      "Training Accuracy= 0.97 loss =  0.1011595173733635\n",
      "Epoch  465\n",
      "Training Accuracy= 0.97 loss =  0.10101446641122006\n",
      "Epoch  466\n",
      "Training Accuracy= 0.97 loss =  0.10084147242619065\n",
      "Epoch  467\n",
      "Training Accuracy= 0.97 loss =  0.10120726307032177\n",
      "Epoch  468\n",
      "Training Accuracy= 0.97 loss =  0.10105881528852695\n",
      "Epoch  469\n",
      "Training Accuracy= 0.97 loss =  0.10078655637448891\n",
      "Epoch  470\n",
      "Training Accuracy= 0.97 loss =  0.10066792897735574\n",
      "Epoch  471\n",
      "Training Accuracy= 0.97 loss =  0.10054877498356518\n",
      "Epoch  472\n",
      "Training Accuracy= 0.97 loss =  0.10036413261820851\n",
      "Epoch  473\n",
      "Training Accuracy= 0.97 loss =  0.10022963215134896\n",
      "Epoch  474\n",
      "Training Accuracy= 0.97 loss =  0.10003739575787819\n",
      "Epoch  475\n",
      "Training Accuracy= 0.97 loss =  0.0999118401779162\n",
      "Epoch  476\n",
      "Training Accuracy= 0.97 loss =  0.09985971748341262\n",
      "Epoch  477\n",
      "Training Accuracy= 0.97 loss =  0.09980487429256579\n",
      "Epoch  478\n",
      "Training Accuracy= 0.97 loss =  0.09969717417113662\n",
      "Epoch  479\n",
      "Training Accuracy= 0.97 loss =  0.099596953109999\n",
      "Epoch  480\n",
      "Training Accuracy= 0.97 loss =  0.09961484392548925\n",
      "Epoch  481\n",
      "Training Accuracy= 0.97 loss =  0.09942392716999962\n",
      "Epoch  482\n",
      "Training Accuracy= 0.97 loss =  0.09984180947330458\n",
      "Epoch  483\n",
      "Training Accuracy= 0.97 loss =  0.09990087944700603\n",
      "Epoch  484\n",
      "Training Accuracy= 0.97 loss =  0.1007281561806874\n",
      "Epoch  485\n",
      "Training Accuracy= 0.97 loss =  0.1007606234715161\n",
      "Epoch  486\n",
      "Training Accuracy= 0.97 loss =  0.10073161353207416\n",
      "Epoch  487\n",
      "Training Accuracy= 0.97 loss =  0.10070040804719155\n",
      "Epoch  488\n",
      "Training Accuracy= 0.97 loss =  0.10064928580781471\n",
      "Epoch  489\n",
      "Training Accuracy= 0.97 loss =  0.10062867260789735\n",
      "Epoch  490\n",
      "Training Accuracy= 0.97 loss =  0.1006317601984821\n",
      "Epoch  491\n",
      "Training Accuracy= 0.97 loss =  0.10068735717066787\n",
      "Epoch  492\n",
      "Training Accuracy= 0.97 loss =  0.10104369735458013\n",
      "Epoch  493\n",
      "Training Accuracy= 0.97 loss =  0.10126223437391073\n",
      "Epoch  494\n",
      "Training Accuracy= 0.97 loss =  0.10159618984020885\n",
      "Epoch  495\n",
      "Training Accuracy= 0.97 loss =  0.10174527813316728\n",
      "Epoch  496\n",
      "Training Accuracy= 0.97 loss =  0.1019810140004519\n",
      "Epoch  497\n",
      "Training Accuracy= 0.97 loss =  0.10222419511918414\n",
      "Epoch  498\n",
      "Training Accuracy= 0.97 loss =  0.10248583920850218\n",
      "Epoch  499\n",
      "Training Accuracy= 0.97 loss =  0.10266280084544613\n",
      "Epoch  500\n",
      "Training Accuracy= 0.97 loss =  0.10286944526520286\n",
      "Epoch  501\n",
      "Training Accuracy= 0.97 loss =  0.10331723887086205\n",
      "Epoch  502\n",
      "Training Accuracy= 0.97 loss =  0.10359467301076762\n",
      "Epoch  503\n",
      "Training Accuracy= 0.97 loss =  0.10392128013114307\n",
      "Epoch  504\n",
      "Training Accuracy= 0.97 loss =  0.10416431303146308\n",
      "Epoch  505\n",
      "Training Accuracy= 0.97 loss =  0.10443291913488678\n",
      "Epoch  506\n",
      "Training Accuracy= 0.97 loss =  0.10471374000497588\n",
      "Epoch  507\n",
      "Training Accuracy= 0.97 loss =  0.10505528928846364\n",
      "Epoch  508\n",
      "Training Accuracy= 0.97 loss =  0.10532676026997091\n",
      "Epoch  509\n",
      "Training Accuracy= 0.97 loss =  0.10568057685005403\n",
      "Epoch  510\n",
      "Training Accuracy= 0.97 loss =  0.10626349731743796\n",
      "Epoch  511\n",
      "Training Accuracy= 0.97 loss =  0.10678099933901182\n",
      "Epoch  512\n",
      "Training Accuracy= 0.97 loss =  0.10719381471073883\n",
      "Epoch  513\n",
      "Training Accuracy= 0.97 loss =  0.10754477940217308\n",
      "Epoch  514\n",
      "Training Accuracy= 0.97 loss =  0.10793646376851083\n",
      "Epoch  515\n",
      "Training Accuracy= 0.97 loss =  0.10838206247746734\n",
      "Epoch  516\n",
      "Training Accuracy= 0.97 loss =  0.10875771907518257\n",
      "Epoch  517\n",
      "Training Accuracy= 0.97 loss =  0.10912577361644779\n",
      "Epoch  518\n",
      "Training Accuracy= 0.97 loss =  0.1095073607059706\n",
      "Epoch  519\n",
      "Training Accuracy= 0.97 loss =  0.109773975130473\n",
      "Epoch  520\n",
      "Training Accuracy= 0.97 loss =  0.10987528141505849\n",
      "Epoch  521\n",
      "Training Accuracy= 0.97 loss =  0.11018406001214834\n",
      "Epoch  522\n",
      "Training Accuracy= 0.97 loss =  0.11045863273387335\n",
      "Epoch  523\n",
      "Training Accuracy= 0.97 loss =  0.1109511502736537\n",
      "Epoch  524\n",
      "Training Accuracy= 0.97 loss =  0.11136292132653186\n",
      "Epoch  525\n",
      "Training Accuracy= 0.97 loss =  0.11160099257752415\n",
      "Epoch  526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.11331884717170536\n",
      "Epoch  527\n",
      "Training Accuracy= 0.97 loss =  0.11350560271558084\n",
      "Epoch  528\n",
      "Training Accuracy= 0.97 loss =  0.11359924226677616\n",
      "Epoch  529\n",
      "Training Accuracy= 0.97 loss =  0.11383148459090035\n",
      "Epoch  530\n",
      "Training Accuracy= 0.97 loss =  0.11405285981377702\n",
      "Epoch  531\n",
      "Training Accuracy= 0.97 loss =  0.11432504524479407\n",
      "Epoch  532\n",
      "Training Accuracy= 0.97 loss =  0.11448485051033569\n",
      "Epoch  533\n",
      "Training Accuracy= 0.97 loss =  0.11443809279904313\n",
      "Epoch  534\n",
      "Training Accuracy= 0.97 loss =  0.11456305131937872\n",
      "Epoch  535\n",
      "Training Accuracy= 0.97 loss =  0.11476504968918165\n",
      "Epoch  536\n",
      "Training Accuracy= 0.97 loss =  0.11487233853645568\n",
      "Epoch  537\n",
      "Training Accuracy= 0.97 loss =  0.1146005850448132\n",
      "Epoch  538\n",
      "Training Accuracy= 0.97 loss =  0.11433131517941089\n",
      "Epoch  539\n",
      "Training Accuracy= 0.97 loss =  0.11432873358304725\n",
      "Epoch  540\n",
      "Training Accuracy= 0.97 loss =  0.11448447808538355\n",
      "Epoch  541\n",
      "Training Accuracy= 0.97 loss =  0.11460593892496353\n",
      "Epoch  542\n",
      "Training Accuracy= 0.97 loss =  0.11459948152802085\n",
      "Epoch  543\n",
      "Training Accuracy= 0.97 loss =  0.11476268336683695\n",
      "Epoch  544\n",
      "Training Accuracy= 0.97 loss =  0.11499800458028289\n",
      "Epoch  545\n",
      "Training Accuracy= 0.97 loss =  0.1152230307462403\n",
      "Epoch  546\n",
      "Training Accuracy= 0.97 loss =  0.11569587953471927\n",
      "Epoch  547\n",
      "Training Accuracy= 0.97 loss =  0.11594047537563107\n",
      "Epoch  548\n",
      "Training Accuracy= 0.97 loss =  0.11618529035722992\n",
      "Epoch  549\n",
      "Training Accuracy= 0.97 loss =  0.1163991819437507\n",
      "Epoch  550\n",
      "Training Accuracy= 0.97 loss =  0.11643124063819296\n",
      "Epoch  551\n",
      "Training Accuracy= 0.97 loss =  0.11668128151925788\n",
      "Epoch  552\n",
      "Training Accuracy= 0.97 loss =  0.11688813311000487\n",
      "Epoch  553\n",
      "Training Accuracy= 0.97 loss =  0.11710562854698163\n",
      "Epoch  554\n",
      "Training Accuracy= 0.97 loss =  0.11736286417403154\n",
      "Epoch  555\n",
      "Training Accuracy= 0.97 loss =  0.11760308873381287\n",
      "Epoch  556\n",
      "Training Accuracy= 0.97 loss =  0.11784629158208512\n",
      "Epoch  557\n",
      "Training Accuracy= 0.97 loss =  0.11806361702354039\n",
      "Epoch  558\n",
      "Training Accuracy= 0.97 loss =  0.11833270181099372\n",
      "Epoch  559\n",
      "Training Accuracy= 0.97 loss =  0.11860688838495734\n",
      "Epoch  560\n",
      "Training Accuracy= 0.97 loss =  0.11881273157327167\n",
      "Epoch  561\n",
      "Training Accuracy= 0.97 loss =  0.11915036903292178\n",
      "Epoch  562\n",
      "Training Accuracy= 0.97 loss =  0.1196110140502161\n",
      "Epoch  563\n",
      "Training Accuracy= 0.97 loss =  0.11998291272428116\n",
      "Epoch  564\n",
      "Training Accuracy= 0.97 loss =  0.12082149057511407\n",
      "Epoch  565\n",
      "Training Accuracy= 0.97 loss =  0.1212142297145398\n",
      "Epoch  566\n",
      "Training Accuracy= 0.97 loss =  0.12176050154658688\n",
      "Epoch  567\n",
      "Training Accuracy= 0.97 loss =  0.12217832839464085\n",
      "Epoch  568\n",
      "Training Accuracy= 0.97 loss =  0.1227542402335365\n",
      "Epoch  569\n",
      "Training Accuracy= 0.97 loss =  0.12292682982921127\n",
      "Epoch  570\n",
      "Training Accuracy= 0.97 loss =  0.12314831491298786\n",
      "Epoch  571\n",
      "Training Accuracy= 0.97 loss =  0.12340917955531874\n",
      "Epoch  572\n",
      "Training Accuracy= 0.97 loss =  0.12361557921687288\n",
      "Epoch  573\n",
      "Training Accuracy= 0.97 loss =  0.12392666028132278\n",
      "Epoch  574\n",
      "Training Accuracy= 0.97 loss =  0.12434786125960398\n",
      "Epoch  575\n",
      "Training Accuracy= 0.97 loss =  0.1245586522211226\n",
      "Epoch  576\n",
      "Training Accuracy= 0.97 loss =  0.12542167197426807\n",
      "Epoch  577\n",
      "Training Accuracy= 0.97 loss =  0.1255225450767604\n",
      "Epoch  578\n",
      "Training Accuracy= 0.97 loss =  0.12570357873018145\n",
      "Epoch  579\n",
      "Training Accuracy= 0.97 loss =  0.12597402614413028\n",
      "Epoch  580\n",
      "Training Accuracy= 0.97 loss =  0.12620030344451474\n",
      "Epoch  581\n",
      "Training Accuracy= 0.97 loss =  0.1264428573975438\n",
      "Epoch  582\n",
      "Training Accuracy= 0.97 loss =  0.12693991479122285\n",
      "Epoch  583\n",
      "Training Accuracy= 0.97 loss =  0.12724537890919196\n",
      "Epoch  584\n",
      "Training Accuracy= 0.97 loss =  0.12769064694171425\n",
      "Epoch  585\n",
      "Training Accuracy= 0.97 loss =  0.12792358076000732\n",
      "Epoch  586\n",
      "Training Accuracy= 0.97 loss =  0.12805068868518235\n",
      "Epoch  587\n",
      "Training Accuracy= 0.97 loss =  0.12814785247514648\n",
      "Epoch  588\n",
      "Training Accuracy= 0.97 loss =  0.12821950925028433\n",
      "Epoch  589\n",
      "Training Accuracy= 0.97 loss =  0.12832926264259625\n",
      "Epoch  590\n",
      "Training Accuracy= 0.97 loss =  0.12848515898276908\n",
      "Epoch  591\n",
      "Training Accuracy= 0.97 loss =  0.128565872461833\n",
      "Epoch  592\n",
      "Training Accuracy= 0.97 loss =  0.12851838020176676\n",
      "Epoch  593\n",
      "Training Accuracy= 0.97 loss =  0.12860721522883753\n",
      "Epoch  594\n",
      "Training Accuracy= 0.97 loss =  0.12874197432095075\n",
      "Epoch  595\n",
      "Training Accuracy= 0.97 loss =  0.12885017535677687\n",
      "Epoch  596\n",
      "Training Accuracy= 0.97 loss =  0.12896460903657805\n",
      "Epoch  597\n",
      "Training Accuracy= 0.97 loss =  0.12918421549941184\n",
      "Epoch  598\n",
      "Training Accuracy= 0.97 loss =  0.1301570698470897\n",
      "Epoch  599\n",
      "Training Accuracy= 0.97 loss =  0.13038300957054366\n",
      "Epoch  600\n",
      "Training Accuracy= 0.97 loss =  0.13034765611714733\n",
      "Epoch  601\n",
      "Training Accuracy= 0.97 loss =  0.13056639145846924\n",
      "Epoch  602\n",
      "Training Accuracy= 0.97 loss =  0.13062131128352242\n",
      "Epoch  603\n",
      "Training Accuracy= 0.97 loss =  0.13070963574203856\n",
      "Epoch  604\n",
      "Training Accuracy= 0.97 loss =  0.1308065265829062\n",
      "Epoch  605\n",
      "Training Accuracy= 0.97 loss =  0.130895360103556\n",
      "Epoch  606\n",
      "Training Accuracy= 0.97 loss =  0.1312690693128583\n",
      "Epoch  607\n",
      "Training Accuracy= 0.97 loss =  0.13291955033041686\n",
      "Epoch  608\n",
      "Training Accuracy= 0.97 loss =  0.13299632353828006\n",
      "Epoch  609\n",
      "Training Accuracy= 0.97 loss =  0.13316202920045753\n",
      "Epoch  610\n",
      "Training Accuracy= 0.97 loss =  0.13331282627247887\n",
      "Epoch  611\n",
      "Training Accuracy= 0.97 loss =  0.13327492528393264\n",
      "Epoch  612\n",
      "Training Accuracy= 0.97 loss =  0.13341854214278592\n",
      "Epoch  613\n",
      "Training Accuracy= 0.97 loss =  0.13360842132473916\n",
      "Epoch  614\n",
      "Training Accuracy= 0.97 loss =  0.1337734118889655\n",
      "Epoch  615\n",
      "Training Accuracy= 0.97 loss =  0.1339467691594824\n",
      "Epoch  616\n",
      "Training Accuracy= 0.97 loss =  0.1340979788814059\n",
      "Epoch  617\n",
      "Training Accuracy= 0.97 loss =  0.13429099491814045\n",
      "Epoch  618\n",
      "Training Accuracy= 0.97 loss =  0.13445512298781723\n",
      "Epoch  619\n",
      "Training Accuracy= 0.97 loss =  0.13461116325044403\n",
      "Epoch  620\n",
      "Training Accuracy= 0.97 loss =  0.1347960266702406\n",
      "Epoch  621\n",
      "Training Accuracy= 0.97 loss =  0.13504810463552477\n",
      "Epoch  622\n",
      "Training Accuracy= 0.97 loss =  0.1352610833038887\n",
      "Epoch  623\n",
      "Training Accuracy= 0.97 loss =  0.13548261703775882\n",
      "Epoch  624\n",
      "Training Accuracy= 0.97 loss =  0.13581553606585245\n",
      "Epoch  625\n",
      "Training Accuracy= 0.97 loss =  0.13607886508636913\n",
      "Epoch  626\n",
      "Training Accuracy= 0.97 loss =  0.13636790147114541\n",
      "Epoch  627\n",
      "Training Accuracy= 0.97 loss =  0.13669846357170237\n",
      "Epoch  628\n",
      "Training Accuracy= 0.97 loss =  0.13698663876410416\n",
      "Epoch  629\n",
      "Training Accuracy= 0.97 loss =  0.13707068575526676\n",
      "Epoch  630\n",
      "Training Accuracy= 0.97 loss =  0.13745655583161337\n",
      "Epoch  631\n",
      "Training Accuracy= 0.97 loss =  0.13780427618983848\n",
      "Epoch  632\n",
      "Training Accuracy= 0.97 loss =  0.1382877298491174\n",
      "Epoch  633\n",
      "Training Accuracy= 0.97 loss =  0.13858242884142008\n",
      "Epoch  634\n",
      "Training Accuracy= 0.97 loss =  0.13886918481014396\n",
      "Epoch  635\n",
      "Training Accuracy= 0.97 loss =  0.1392610065839752\n",
      "Epoch  636\n",
      "Training Accuracy= 0.97 loss =  0.13962745739811439\n",
      "Epoch  637\n",
      "Training Accuracy= 0.97 loss =  0.1400324768426069\n",
      "Epoch  638\n",
      "Training Accuracy= 0.97 loss =  0.14036144409169754\n",
      "Epoch  639\n",
      "Training Accuracy= 0.97 loss =  0.1407804486636609\n",
      "Epoch  640\n",
      "Training Accuracy= 0.97 loss =  0.14120503178084848\n",
      "Epoch  641\n",
      "Training Accuracy= 0.97 loss =  0.141568096690322\n",
      "Epoch  642\n",
      "Training Accuracy= 0.97 loss =  0.14194743508327934\n",
      "Epoch  643\n",
      "Training Accuracy= 0.97 loss =  0.14231138523021208\n",
      "Epoch  644\n",
      "Training Accuracy= 0.97 loss =  0.1427006846418398\n",
      "Epoch  645\n",
      "Training Accuracy= 0.97 loss =  0.14314206782235034\n",
      "Epoch  646\n",
      "Training Accuracy= 0.97 loss =  0.14354246699394657\n",
      "Epoch  647\n",
      "Training Accuracy= 0.97 loss =  0.1438818947651897\n",
      "Epoch  648\n",
      "Training Accuracy= 0.97 loss =  0.14424451003357022\n",
      "Epoch  649\n",
      "Training Accuracy= 0.97 loss =  0.144416238442507\n",
      "Epoch  650\n",
      "Training Accuracy= 0.97 loss =  0.14503525739285125\n",
      "Epoch  651\n",
      "Training Accuracy= 0.97 loss =  0.14540673980277485\n",
      "Epoch  652\n",
      "Training Accuracy= 0.97 loss =  0.14580412462906694\n",
      "Epoch  653\n",
      "Training Accuracy= 0.97 loss =  0.1462356481775856\n",
      "Epoch  654\n",
      "Training Accuracy= 0.97 loss =  0.14661994121757538\n",
      "Epoch  655\n",
      "Training Accuracy= 0.97 loss =  0.14698763355399902\n",
      "Epoch  656\n",
      "Training Accuracy= 0.97 loss =  0.14741604579159595\n",
      "Epoch  657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.14781180409535516\n",
      "Epoch  658\n",
      "Training Accuracy= 0.97 loss =  0.14820401173820572\n",
      "Epoch  659\n",
      "Training Accuracy= 0.97 loss =  0.14850117397965454\n",
      "Epoch  660\n",
      "Training Accuracy= 0.97 loss =  0.1488056626505993\n",
      "Epoch  661\n",
      "Training Accuracy= 0.97 loss =  0.149379934863642\n",
      "Epoch  662\n",
      "Training Accuracy= 0.97 loss =  0.15030590880140643\n",
      "Epoch  663\n",
      "Training Accuracy= 0.97 loss =  0.1506842199711203\n",
      "Epoch  664\n",
      "Training Accuracy= 0.97 loss =  0.1509404397276886\n",
      "Epoch  665\n",
      "Training Accuracy= 0.97 loss =  0.15125040996512462\n",
      "Epoch  666\n",
      "Training Accuracy= 0.97 loss =  0.15151208664638693\n",
      "Epoch  667\n",
      "Training Accuracy= 0.97 loss =  0.15179578682165373\n",
      "Epoch  668\n",
      "Training Accuracy= 0.97 loss =  0.15204246250341225\n",
      "Epoch  669\n",
      "Training Accuracy= 0.97 loss =  0.1523394854153931\n",
      "Epoch  670\n",
      "Training Accuracy= 0.97 loss =  0.15261231725735022\n",
      "Epoch  671\n",
      "Training Accuracy= 0.97 loss =  0.15286816412733495\n",
      "Epoch  672\n",
      "Training Accuracy= 0.97 loss =  0.15310577835695988\n",
      "Epoch  673\n",
      "Training Accuracy= 0.97 loss =  0.15330803909380397\n",
      "Epoch  674\n",
      "Training Accuracy= 0.97 loss =  0.15355451126793562\n",
      "Epoch  675\n",
      "Training Accuracy= 0.97 loss =  0.15367327133240946\n",
      "Epoch  676\n",
      "Training Accuracy= 0.97 loss =  0.15378893010471412\n",
      "Epoch  677\n",
      "Training Accuracy= 0.97 loss =  0.15398632485279157\n",
      "Epoch  678\n",
      "Training Accuracy= 0.97 loss =  0.15409056631707796\n",
      "Epoch  679\n",
      "Training Accuracy= 0.97 loss =  0.15424118104399076\n",
      "Epoch  680\n",
      "Training Accuracy= 0.97 loss =  0.15430335978373594\n",
      "Epoch  681\n",
      "Training Accuracy= 0.97 loss =  0.15445333174865924\n",
      "Epoch  682\n",
      "Training Accuracy= 0.97 loss =  0.15448931504737484\n",
      "Epoch  683\n",
      "Training Accuracy= 0.97 loss =  0.15472149101788493\n",
      "Epoch  684\n",
      "Training Accuracy= 0.97 loss =  0.15471462910926292\n",
      "Epoch  685\n",
      "Training Accuracy= 0.97 loss =  0.15476665655968738\n",
      "Epoch  686\n",
      "Training Accuracy= 0.97 loss =  0.15466410180820814\n",
      "Epoch  687\n",
      "Training Accuracy= 0.97 loss =  0.15460451385819265\n",
      "Epoch  688\n",
      "Training Accuracy= 0.97 loss =  0.15430152387617785\n",
      "Epoch  689\n",
      "Training Accuracy= 0.97 loss =  0.1541756137785865\n",
      "Epoch  690\n",
      "Training Accuracy= 0.97 loss =  0.15387296198759576\n",
      "Epoch  691\n",
      "Training Accuracy= 0.97 loss =  0.15377503006872012\n",
      "Epoch  692\n",
      "Training Accuracy= 0.97 loss =  0.1535525241282289\n",
      "Epoch  693\n",
      "Training Accuracy= 0.97 loss =  0.15333164598433113\n",
      "Epoch  694\n",
      "Training Accuracy= 0.97 loss =  0.15242523814100084\n",
      "Epoch  695\n",
      "Training Accuracy= 0.97 loss =  0.15227805173736197\n",
      "Epoch  696\n",
      "Training Accuracy= 0.97 loss =  0.15202910537566125\n",
      "Epoch  697\n",
      "Training Accuracy= 0.97 loss =  0.1520900878057532\n",
      "Epoch  698\n",
      "Training Accuracy= 0.97 loss =  0.15185720456111884\n",
      "Epoch  699\n",
      "Training Accuracy= 0.97 loss =  0.15176574514321384\n",
      "Epoch  700\n",
      "Training Accuracy= 0.97 loss =  0.15148085372049286\n",
      "Epoch  701\n",
      "Training Accuracy= 0.97 loss =  0.15120666630346621\n",
      "Epoch  702\n",
      "Training Accuracy= 0.97 loss =  0.1509256924103058\n",
      "Epoch  703\n",
      "Training Accuracy= 0.97 loss =  0.15063409684778675\n",
      "Epoch  704\n",
      "Training Accuracy= 0.97 loss =  0.1506252232127015\n",
      "Epoch  705\n",
      "Training Accuracy= 0.97 loss =  0.1504701362812116\n",
      "Epoch  706\n",
      "Training Accuracy= 0.97 loss =  0.15018082362365975\n",
      "Epoch  707\n",
      "Training Accuracy= 0.97 loss =  0.1498100603708699\n",
      "Epoch  708\n",
      "Training Accuracy= 0.97 loss =  0.14923853362722994\n",
      "Epoch  709\n",
      "Training Accuracy= 0.97 loss =  0.14906638316261944\n",
      "Epoch  710\n",
      "Training Accuracy= 0.97 loss =  0.1490357377203178\n",
      "Epoch  711\n",
      "Training Accuracy= 0.97 loss =  0.14866926101786482\n",
      "Epoch  712\n",
      "Training Accuracy= 0.97 loss =  0.148361401993221\n",
      "Epoch  713\n",
      "Training Accuracy= 0.97 loss =  0.14805151682800488\n",
      "Epoch  714\n",
      "Training Accuracy= 0.97 loss =  0.14805569975136398\n",
      "Epoch  715\n",
      "Training Accuracy= 0.97 loss =  0.14768923029580622\n",
      "Epoch  716\n",
      "Training Accuracy= 0.97 loss =  0.14737789554517755\n",
      "Epoch  717\n",
      "Training Accuracy= 0.97 loss =  0.1470331225703387\n",
      "Epoch  718\n",
      "Training Accuracy= 0.97 loss =  0.14684121290040889\n",
      "Epoch  719\n",
      "Training Accuracy= 0.97 loss =  0.14648721764478062\n",
      "Epoch  720\n",
      "Training Accuracy= 0.97 loss =  0.14605335868953911\n",
      "Epoch  721\n",
      "Training Accuracy= 0.97 loss =  0.1456476817112083\n",
      "Epoch  722\n",
      "Training Accuracy= 0.97 loss =  0.14563335095611096\n",
      "Epoch  723\n",
      "Training Accuracy= 0.97 loss =  0.1453238502550538\n",
      "Epoch  724\n",
      "Training Accuracy= 0.97 loss =  0.1448838224844787\n",
      "Epoch  725\n",
      "Training Accuracy= 0.97 loss =  0.14450939485649486\n",
      "Epoch  726\n",
      "Training Accuracy= 0.97 loss =  0.1440208259210492\n",
      "Epoch  727\n",
      "Training Accuracy= 0.97 loss =  0.1435566359036945\n",
      "Epoch  728\n",
      "Training Accuracy= 0.97 loss =  0.1430738574045408\n",
      "Epoch  729\n",
      "Training Accuracy= 0.97 loss =  0.14176238250552725\n",
      "Epoch  730\n",
      "Training Accuracy= 0.97 loss =  0.14109340590294722\n",
      "Epoch  731\n",
      "Training Accuracy= 0.97 loss =  0.1395432457092226\n",
      "Epoch  732\n",
      "Training Accuracy= 0.97 loss =  0.13900740684996782\n",
      "Epoch  733\n",
      "Training Accuracy= 0.97 loss =  0.1387934802974684\n",
      "Epoch  734\n",
      "Training Accuracy= 0.97 loss =  0.1382973735390053\n",
      "Epoch  735\n",
      "Training Accuracy= 0.97 loss =  0.13776577395751455\n",
      "Epoch  736\n",
      "Training Accuracy= 0.97 loss =  0.13747880374034885\n",
      "Epoch  737\n",
      "Training Accuracy= 0.97 loss =  0.13682710974411016\n",
      "Epoch  738\n",
      "Training Accuracy= 0.97 loss =  0.13626385047252254\n",
      "Epoch  739\n",
      "Training Accuracy= 0.97 loss =  0.135739335660681\n",
      "Epoch  740\n",
      "Training Accuracy= 0.97 loss =  0.13529845818096475\n",
      "Epoch  741\n",
      "Training Accuracy= 0.97 loss =  0.1349065238337335\n",
      "Epoch  742\n",
      "Training Accuracy= 0.97 loss =  0.13384948102364602\n",
      "Epoch  743\n",
      "Training Accuracy= 0.97 loss =  0.13318273172825051\n",
      "Epoch  744\n",
      "Training Accuracy= 0.97 loss =  0.13265464950043498\n",
      "Epoch  745\n",
      "Training Accuracy= 0.97 loss =  0.1321024348007386\n",
      "Epoch  746\n",
      "Training Accuracy= 0.97 loss =  0.1317363383474231\n",
      "Epoch  747\n",
      "Training Accuracy= 0.97 loss =  0.13114337483116412\n",
      "Epoch  748\n",
      "Training Accuracy= 0.97 loss =  0.13061688297765028\n",
      "Epoch  749\n",
      "Training Accuracy= 0.97 loss =  0.13004281605213253\n",
      "Epoch  750\n",
      "Training Accuracy= 0.97 loss =  0.12952475353063972\n",
      "Epoch  751\n",
      "Training Accuracy= 0.97 loss =  0.12906695898302473\n",
      "Epoch  752\n",
      "Training Accuracy= 0.97 loss =  0.12849104482114512\n",
      "Epoch  753\n",
      "Training Accuracy= 0.97 loss =  0.12795504139106784\n",
      "Epoch  754\n",
      "Training Accuracy= 0.97 loss =  0.12743727792342374\n",
      "Epoch  755\n",
      "Training Accuracy= 0.97 loss =  0.12693256513666756\n",
      "Epoch  756\n",
      "Training Accuracy= 0.97 loss =  0.12631511894109906\n",
      "Epoch  757\n",
      "Training Accuracy= 0.97 loss =  0.12584429888021165\n",
      "Epoch  758\n",
      "Training Accuracy= 0.97 loss =  0.1254162686357334\n",
      "Epoch  759\n",
      "Training Accuracy= 0.97 loss =  0.12492813385703848\n",
      "Epoch  760\n",
      "Training Accuracy= 0.97 loss =  0.12430653438161887\n",
      "Epoch  761\n",
      "Training Accuracy= 0.97 loss =  0.12372778547560458\n",
      "Epoch  762\n",
      "Training Accuracy= 0.97 loss =  0.12331399836120382\n",
      "Epoch  763\n",
      "Training Accuracy= 0.97 loss =  0.12279353661884292\n",
      "Epoch  764\n",
      "Training Accuracy= 0.97 loss =  0.12228713607921209\n",
      "Epoch  765\n",
      "Training Accuracy= 0.97 loss =  0.12182549427583139\n",
      "Epoch  766\n",
      "Training Accuracy= 0.97 loss =  0.12134623838264523\n",
      "Epoch  767\n",
      "Training Accuracy= 0.97 loss =  0.12083280615991993\n",
      "Epoch  768\n",
      "Training Accuracy= 0.97 loss =  0.1203473679423333\n",
      "Epoch  769\n",
      "Training Accuracy= 0.97 loss =  0.11985163313229671\n",
      "Epoch  770\n",
      "Training Accuracy= 0.97 loss =  0.11939384178989813\n",
      "Epoch  771\n",
      "Training Accuracy= 0.97 loss =  0.11895341099647806\n",
      "Epoch  772\n",
      "Training Accuracy= 0.97 loss =  0.1185362396266795\n",
      "Epoch  773\n",
      "Training Accuracy= 0.97 loss =  0.11798942712453796\n",
      "Epoch  774\n",
      "Training Accuracy= 0.97 loss =  0.11757031248430536\n",
      "Epoch  775\n",
      "Training Accuracy= 0.97 loss =  0.11721067941046401\n",
      "Epoch  776\n",
      "Training Accuracy= 0.97 loss =  0.1167841370030891\n",
      "Epoch  777\n",
      "Training Accuracy= 0.97 loss =  0.1161437537422942\n",
      "Epoch  778\n",
      "Training Accuracy= 0.97 loss =  0.11577120609641708\n",
      "Epoch  779\n",
      "Training Accuracy= 0.97 loss =  0.11550454347518824\n",
      "Epoch  780\n",
      "Training Accuracy= 0.97 loss =  0.11471263974084578\n",
      "Epoch  781\n",
      "Training Accuracy= 0.97 loss =  0.1145362051095874\n",
      "Epoch  782\n",
      "Training Accuracy= 0.97 loss =  0.11426686044597488\n",
      "Epoch  783\n",
      "Training Accuracy= 0.97 loss =  0.11398510960633877\n",
      "Epoch  784\n",
      "Training Accuracy= 0.97 loss =  0.11373964359106942\n",
      "Epoch  785\n",
      "Training Accuracy= 0.97 loss =  0.1137036487023141\n",
      "Epoch  786\n",
      "Training Accuracy= 0.97 loss =  0.11358518669274052\n",
      "Epoch  787\n",
      "Training Accuracy= 0.97 loss =  0.11342836789717828\n",
      "Epoch  788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.11336203472184536\n",
      "Epoch  789\n",
      "Training Accuracy= 0.97 loss =  0.113290019797707\n",
      "Epoch  790\n",
      "Training Accuracy= 0.97 loss =  0.11319597107886481\n",
      "Epoch  791\n",
      "Training Accuracy= 0.97 loss =  0.113071742889708\n",
      "Epoch  792\n",
      "Training Accuracy= 0.97 loss =  0.11317847869328748\n",
      "Epoch  793\n",
      "Training Accuracy= 0.97 loss =  0.11333639374281332\n",
      "Epoch  794\n",
      "Training Accuracy= 0.97 loss =  0.11340921217182655\n",
      "Epoch  795\n",
      "Training Accuracy= 0.97 loss =  0.11389899631923324\n",
      "Epoch  796\n",
      "Training Accuracy= 0.97 loss =  0.11399448024001611\n",
      "Epoch  797\n",
      "Training Accuracy= 0.97 loss =  0.11409645320557893\n",
      "Epoch  798\n",
      "Training Accuracy= 0.97 loss =  0.11421606691436749\n",
      "Epoch  799\n",
      "Training Accuracy= 0.97 loss =  0.11433541067358005\n",
      "Epoch  800\n",
      "Training Accuracy= 0.97 loss =  0.11440122844217498\n",
      "Epoch  801\n",
      "Training Accuracy= 0.97 loss =  0.11450149405676902\n",
      "Epoch  802\n",
      "Training Accuracy= 0.97 loss =  0.11464244731847527\n",
      "Epoch  803\n",
      "Training Accuracy= 0.97 loss =  0.11478489420932522\n",
      "Epoch  804\n",
      "Training Accuracy= 0.97 loss =  0.11496814490626905\n",
      "Epoch  805\n",
      "Training Accuracy= 0.97 loss =  0.11518097826019455\n",
      "Epoch  806\n",
      "Training Accuracy= 0.97 loss =  0.11545021752705573\n",
      "Epoch  807\n",
      "Training Accuracy= 0.97 loss =  0.11569414473078735\n",
      "Epoch  808\n",
      "Training Accuracy= 0.97 loss =  0.11602847895390775\n",
      "Epoch  809\n",
      "Training Accuracy= 0.97 loss =  0.11614792294211393\n",
      "Epoch  810\n",
      "Training Accuracy= 0.97 loss =  0.11647933284444985\n",
      "Epoch  811\n",
      "Training Accuracy= 0.97 loss =  0.11667902226905352\n",
      "Epoch  812\n",
      "Training Accuracy= 0.97 loss =  0.11686566030961261\n",
      "Epoch  813\n",
      "Training Accuracy= 0.97 loss =  0.11712033893992306\n",
      "Epoch  814\n",
      "Training Accuracy= 0.97 loss =  0.11741267420379095\n",
      "Epoch  815\n",
      "Training Accuracy= 0.97 loss =  0.1175944860910232\n",
      "Epoch  816\n",
      "Training Accuracy= 0.97 loss =  0.11785067261906745\n",
      "Epoch  817\n",
      "Training Accuracy= 0.97 loss =  0.11802335280641743\n",
      "Epoch  818\n",
      "Training Accuracy= 0.97 loss =  0.11845021541216084\n",
      "Epoch  819\n",
      "Training Accuracy= 0.97 loss =  0.11876801990321668\n",
      "Epoch  820\n",
      "Training Accuracy= 0.97 loss =  0.11920352606075568\n",
      "Epoch  821\n",
      "Training Accuracy= 0.97 loss =  0.1194701541884314\n",
      "Epoch  822\n",
      "Training Accuracy= 0.97 loss =  0.11973582045451828\n",
      "Epoch  823\n",
      "Training Accuracy= 0.97 loss =  0.11994674151425055\n",
      "Epoch  824\n",
      "Training Accuracy= 0.97 loss =  0.12022647408171569\n",
      "Epoch  825\n",
      "Training Accuracy= 0.97 loss =  0.1204431520849086\n",
      "Epoch  826\n",
      "Training Accuracy= 0.97 loss =  0.12079672287857601\n",
      "Epoch  827\n",
      "Training Accuracy= 0.97 loss =  0.1209596280689707\n",
      "Epoch  828\n",
      "Training Accuracy= 0.97 loss =  0.12109033540675065\n",
      "Epoch  829\n",
      "Training Accuracy= 0.97 loss =  0.12123858718937386\n",
      "Epoch  830\n",
      "Training Accuracy= 0.97 loss =  0.12118001756182274\n",
      "Epoch  831\n",
      "Training Accuracy= 0.97 loss =  0.1213266287512901\n",
      "Epoch  832\n",
      "Training Accuracy= 0.97 loss =  0.12149033242677576\n",
      "Epoch  833\n",
      "Training Accuracy= 0.97 loss =  0.12161883662972489\n",
      "Epoch  834\n",
      "Training Accuracy= 0.97 loss =  0.12175973244773881\n",
      "Epoch  835\n",
      "Training Accuracy= 0.97 loss =  0.12185216646466474\n",
      "Epoch  836\n",
      "Training Accuracy= 0.97 loss =  0.12181455490456944\n",
      "Epoch  837\n",
      "Training Accuracy= 0.97 loss =  0.12181961025333256\n",
      "Epoch  838\n",
      "Training Accuracy= 0.97 loss =  0.12183595133820117\n",
      "Epoch  839\n",
      "Training Accuracy= 0.97 loss =  0.12188790726180387\n",
      "Epoch  840\n",
      "Training Accuracy= 0.97 loss =  0.12187527752137174\n",
      "Epoch  841\n",
      "Training Accuracy= 0.97 loss =  0.12189421527591976\n",
      "Epoch  842\n",
      "Training Accuracy= 0.97 loss =  0.12190101537026996\n",
      "Epoch  843\n",
      "Training Accuracy= 0.97 loss =  0.12193921554371914\n",
      "Epoch  844\n",
      "Training Accuracy= 0.97 loss =  0.12200506236716627\n",
      "Epoch  845\n",
      "Training Accuracy= 0.97 loss =  0.12200378381607035\n",
      "Epoch  846\n",
      "Training Accuracy= 0.97 loss =  0.12199936089973261\n",
      "Epoch  847\n",
      "Training Accuracy= 0.97 loss =  0.12196702551848645\n",
      "Epoch  848\n",
      "Training Accuracy= 0.97 loss =  0.12196195121516853\n",
      "Epoch  849\n",
      "Training Accuracy= 0.97 loss =  0.12193932502620347\n",
      "Epoch  850\n",
      "Training Accuracy= 0.97 loss =  0.12184813474131992\n",
      "Epoch  851\n",
      "Training Accuracy= 0.97 loss =  0.12193516829338102\n",
      "Epoch  852\n",
      "Training Accuracy= 0.97 loss =  0.12198452760828635\n",
      "Epoch  853\n",
      "Training Accuracy= 0.97 loss =  0.12246579744510179\n",
      "Epoch  854\n",
      "Training Accuracy= 0.97 loss =  0.1225523673190935\n",
      "Epoch  855\n",
      "Training Accuracy= 0.97 loss =  0.12265955152514126\n",
      "Epoch  856\n",
      "Training Accuracy= 0.97 loss =  0.12273573860615433\n",
      "Epoch  857\n",
      "Training Accuracy= 0.97 loss =  0.12279294930723138\n",
      "Epoch  858\n",
      "Training Accuracy= 0.97 loss =  0.12285831055019394\n",
      "Epoch  859\n",
      "Training Accuracy= 0.97 loss =  0.12297238458606041\n",
      "Epoch  860\n",
      "Training Accuracy= 0.97 loss =  0.1229277867281377\n",
      "Epoch  861\n",
      "Training Accuracy= 0.97 loss =  0.12301175968483316\n",
      "Epoch  862\n",
      "Training Accuracy= 0.97 loss =  0.1230244767902165\n",
      "Epoch  863\n",
      "Training Accuracy= 0.97 loss =  0.12302656379629268\n",
      "Epoch  864\n",
      "Training Accuracy= 0.97 loss =  0.12303006602939931\n",
      "Epoch  865\n",
      "Training Accuracy= 0.97 loss =  0.12296210418414469\n",
      "Epoch  866\n",
      "Training Accuracy= 0.97 loss =  0.12306527670121967\n",
      "Epoch  867\n",
      "Training Accuracy= 0.97 loss =  0.12306954646402879\n",
      "Epoch  868\n",
      "Training Accuracy= 0.97 loss =  0.1230887204354994\n",
      "Epoch  869\n",
      "Training Accuracy= 0.97 loss =  0.12305828435489055\n",
      "Epoch  870\n",
      "Training Accuracy= 0.97 loss =  0.12303638750791296\n",
      "Epoch  871\n",
      "Training Accuracy= 0.97 loss =  0.12300902546056382\n",
      "Epoch  872\n",
      "Training Accuracy= 0.97 loss =  0.12315379585116058\n",
      "Epoch  873\n",
      "Training Accuracy= 0.97 loss =  0.12307473547891745\n",
      "Epoch  874\n",
      "Training Accuracy= 0.97 loss =  0.12304298272739095\n",
      "Epoch  875\n",
      "Training Accuracy= 0.97 loss =  0.12299430019403508\n",
      "Epoch  876\n",
      "Training Accuracy= 0.97 loss =  0.12295195554734334\n",
      "Epoch  877\n",
      "Training Accuracy= 0.97 loss =  0.12295223717154394\n",
      "Epoch  878\n",
      "Training Accuracy= 0.97 loss =  0.122880799647369\n",
      "Epoch  879\n",
      "Training Accuracy= 0.97 loss =  0.12276810163822725\n",
      "Epoch  880\n",
      "Training Accuracy= 0.97 loss =  0.1227019912952\n",
      "Epoch  881\n",
      "Training Accuracy= 0.97 loss =  0.12259514381041034\n",
      "Epoch  882\n",
      "Training Accuracy= 0.97 loss =  0.12248526871441334\n",
      "Epoch  883\n",
      "Training Accuracy= 0.97 loss =  0.12240109458981091\n",
      "Epoch  884\n",
      "Training Accuracy= 0.97 loss =  0.12228552846391622\n",
      "Epoch  885\n",
      "Training Accuracy= 0.97 loss =  0.12224322516663536\n",
      "Epoch  886\n",
      "Training Accuracy= 0.97 loss =  0.12210772113458863\n",
      "Epoch  887\n",
      "Training Accuracy= 0.97 loss =  0.12203433879521851\n",
      "Epoch  888\n",
      "Training Accuracy= 0.97 loss =  0.12184856452787789\n",
      "Epoch  889\n",
      "Training Accuracy= 0.97 loss =  0.12178446860725287\n",
      "Epoch  890\n",
      "Training Accuracy= 0.97 loss =  0.12163785123658019\n",
      "Epoch  891\n",
      "Training Accuracy= 0.97 loss =  0.12154994404635663\n",
      "Epoch  892\n",
      "Training Accuracy= 0.97 loss =  0.12134108266240501\n",
      "Epoch  893\n",
      "Training Accuracy= 0.97 loss =  0.12123408110024828\n",
      "Epoch  894\n",
      "Training Accuracy= 0.97 loss =  0.12106449128330266\n",
      "Epoch  895\n",
      "Training Accuracy= 0.97 loss =  0.12086165632197463\n",
      "Epoch  896\n",
      "Training Accuracy= 0.97 loss =  0.12062239229812084\n",
      "Epoch  897\n",
      "Training Accuracy= 0.97 loss =  0.1204553099009723\n",
      "Epoch  898\n",
      "Training Accuracy= 0.97 loss =  0.12026670714083634\n",
      "Epoch  899\n",
      "Training Accuracy= 0.97 loss =  0.12008246483654367\n",
      "Epoch  900\n",
      "Training Accuracy= 0.97 loss =  0.11993590077552385\n",
      "Epoch  901\n",
      "Training Accuracy= 0.97 loss =  0.11973424065821434\n",
      "Epoch  902\n",
      "Training Accuracy= 0.97 loss =  0.11961605322778396\n",
      "Epoch  903\n",
      "Training Accuracy= 0.97 loss =  0.11945760712221773\n",
      "Epoch  904\n",
      "Training Accuracy= 0.97 loss =  0.11921338873088058\n",
      "Epoch  905\n",
      "Training Accuracy= 0.97 loss =  0.11914026945439268\n",
      "Epoch  906\n",
      "Training Accuracy= 0.97 loss =  0.11891852248614655\n",
      "Epoch  907\n",
      "Training Accuracy= 0.97 loss =  0.11867712312543105\n",
      "Epoch  908\n",
      "Training Accuracy= 0.97 loss =  0.11838506783372596\n",
      "Epoch  909\n",
      "Training Accuracy= 0.97 loss =  0.11814778847783258\n",
      "Epoch  910\n",
      "Training Accuracy= 0.97 loss =  0.11794299296274019\n",
      "Epoch  911\n",
      "Training Accuracy= 0.97 loss =  0.1177486084765341\n",
      "Epoch  912\n",
      "Training Accuracy= 0.97 loss =  0.11751335539174093\n",
      "Epoch  913\n",
      "Training Accuracy= 0.97 loss =  0.1172947345585414\n",
      "Epoch  914\n",
      "Training Accuracy= 0.97 loss =  0.1170724611519578\n",
      "Epoch  915\n",
      "Training Accuracy= 0.97 loss =  0.11684217413510486\n",
      "Epoch  916\n",
      "Training Accuracy= 0.97 loss =  0.11663593999726522\n",
      "Epoch  917\n",
      "Training Accuracy= 0.97 loss =  0.11633593788335202\n",
      "Epoch  918\n",
      "Training Accuracy= 0.97 loss =  0.11602797203745308\n",
      "Epoch  919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.97 loss =  0.11572171888305158\n",
      "Epoch  920\n",
      "Training Accuracy= 0.97 loss =  0.11548494408534189\n",
      "Epoch  921\n",
      "Training Accuracy= 0.97 loss =  0.11526178991329797\n",
      "Epoch  922\n",
      "Training Accuracy= 0.97 loss =  0.11502911697011706\n",
      "Epoch  923\n",
      "Training Accuracy= 0.97 loss =  0.11499253133282006\n",
      "Epoch  924\n",
      "Training Accuracy= 0.97 loss =  0.11472400774472857\n",
      "Epoch  925\n",
      "Training Accuracy= 0.97 loss =  0.11439558778185714\n",
      "Epoch  926\n",
      "Training Accuracy= 0.97 loss =  0.11410269651965421\n",
      "Epoch  927\n",
      "Training Accuracy= 0.97 loss =  0.11388158912544719\n",
      "Epoch  928\n",
      "Training Accuracy= 0.97 loss =  0.11359822623169837\n",
      "Epoch  929\n",
      "Training Accuracy= 0.97 loss =  0.1133505706418215\n",
      "Epoch  930\n",
      "Training Accuracy= 0.97 loss =  0.11309281279453687\n",
      "Epoch  931\n",
      "Training Accuracy= 0.97 loss =  0.11284337723018044\n",
      "Epoch  932\n",
      "Training Accuracy= 0.97 loss =  0.1125953461577888\n",
      "Epoch  933\n",
      "Training Accuracy= 0.97 loss =  0.1121823468390401\n",
      "Epoch  934\n",
      "Training Accuracy= 0.97 loss =  0.11191007693940289\n",
      "Epoch  935\n",
      "Training Accuracy= 0.97 loss =  0.11166330780326672\n",
      "Epoch  936\n",
      "Training Accuracy= 0.97 loss =  0.1114323438437632\n",
      "Epoch  937\n",
      "Training Accuracy= 0.97 loss =  0.11139734834361167\n",
      "Epoch  938\n",
      "Training Accuracy= 0.97 loss =  0.11115563217059973\n",
      "Epoch  939\n",
      "Training Accuracy= 0.97 loss =  0.1109389750233933\n",
      "Epoch  940\n",
      "Training Accuracy= 0.97 loss =  0.11065454080522474\n",
      "Epoch  941\n",
      "Training Accuracy= 0.97 loss =  0.11040461559299578\n",
      "Epoch  942\n",
      "Training Accuracy= 0.97 loss =  0.11213825805405804\n",
      "Epoch  943\n",
      "Training Accuracy= 0.97 loss =  0.1118556667854668\n",
      "Epoch  944\n",
      "Training Accuracy= 0.97 loss =  0.11166084106992388\n",
      "Epoch  945\n",
      "Training Accuracy= 0.97 loss =  0.11142815970500619\n",
      "Epoch  946\n",
      "Training Accuracy= 0.97 loss =  0.11120602900136316\n",
      "Epoch  947\n",
      "Training Accuracy= 0.97 loss =  0.1109594284601353\n",
      "Epoch  948\n",
      "Training Accuracy= 0.97 loss =  0.11068228587205989\n",
      "Epoch  949\n",
      "Training Accuracy= 0.97 loss =  0.11035820335602829\n",
      "Epoch  950\n",
      "Training Accuracy= 0.97 loss =  0.11014072477186951\n",
      "Epoch  951\n",
      "Training Accuracy= 0.97 loss =  0.10993620445649005\n",
      "Epoch  952\n",
      "Training Accuracy= 0.97 loss =  0.10978893741159496\n",
      "Epoch  953\n",
      "Training Accuracy= 0.97 loss =  0.10961235143193969\n",
      "Epoch  954\n",
      "Training Accuracy= 0.97 loss =  0.10947472794881402\n",
      "Epoch  955\n",
      "Training Accuracy= 0.97 loss =  0.10927750679537929\n",
      "Epoch  956\n",
      "Training Accuracy= 0.97 loss =  0.1091318140169821\n",
      "Epoch  957\n",
      "Training Accuracy= 0.97 loss =  0.10893602616841994\n",
      "Epoch  958\n",
      "Training Accuracy= 0.97 loss =  0.10876222722310998\n",
      "Epoch  959\n",
      "Training Accuracy= 0.97 loss =  0.10863084998010467\n",
      "Epoch  960\n",
      "Training Accuracy= 0.97 loss =  0.10846047549446106\n",
      "Epoch  961\n",
      "Training Accuracy= 0.97 loss =  0.10832748630677115\n",
      "Epoch  962\n",
      "Training Accuracy= 0.97 loss =  0.10817618757695091\n",
      "Epoch  963\n",
      "Training Accuracy= 0.97 loss =  0.10809051312341332\n",
      "Epoch  964\n",
      "Training Accuracy= 0.97 loss =  0.10798644482904807\n",
      "Epoch  965\n",
      "Training Accuracy= 0.97 loss =  0.1078588121012735\n",
      "Epoch  966\n",
      "Training Accuracy= 0.97 loss =  0.10774447022034966\n",
      "Epoch  967\n",
      "Training Accuracy= 0.97 loss =  0.10762390434537106\n",
      "Epoch  968\n",
      "Training Accuracy= 0.97 loss =  0.10757801955559414\n",
      "Epoch  969\n",
      "Training Accuracy= 0.97 loss =  0.10745942582509246\n",
      "Epoch  970\n",
      "Training Accuracy= 0.97 loss =  0.10740960050203874\n",
      "Epoch  971\n",
      "Training Accuracy= 0.97 loss =  0.10733572936332675\n",
      "Epoch  972\n",
      "Training Accuracy= 0.97 loss =  0.10727364250283414\n",
      "Epoch  973\n",
      "Training Accuracy= 0.97 loss =  0.1072341468335237\n",
      "Epoch  974\n",
      "Training Accuracy= 0.97 loss =  0.10722376051714652\n",
      "Epoch  975\n",
      "Training Accuracy= 0.97 loss =  0.10724437971383632\n",
      "Epoch  976\n",
      "Training Accuracy= 0.97 loss =  0.1071836533738389\n",
      "Epoch  977\n",
      "Training Accuracy= 0.97 loss =  0.10713967191288373\n",
      "Epoch  978\n",
      "Training Accuracy= 0.97 loss =  0.10715096642719481\n",
      "Epoch  979\n",
      "Training Accuracy= 0.97 loss =  0.10720253211860639\n",
      "Epoch  980\n",
      "Training Accuracy= 0.97 loss =  0.10730578170689067\n",
      "Epoch  981\n",
      "Training Accuracy= 0.97 loss =  0.10739090564147333\n",
      "Epoch  982\n",
      "Training Accuracy= 0.97 loss =  0.10746125602580589\n",
      "Epoch  983\n",
      "Training Accuracy= 0.97 loss =  0.10751960926180648\n",
      "Epoch  984\n",
      "Training Accuracy= 0.97 loss =  0.1075512587625875\n",
      "Epoch  985\n",
      "Training Accuracy= 0.97 loss =  0.10939076196022351\n",
      "Epoch  986\n",
      "Training Accuracy= 0.97 loss =  0.10933211850000712\n",
      "Epoch  987\n",
      "Training Accuracy= 0.97 loss =  0.10949436172536617\n",
      "Epoch  988\n",
      "Training Accuracy= 0.97 loss =  0.10960738865101002\n",
      "Epoch  989\n",
      "Training Accuracy= 0.97 loss =  0.10970817016796156\n",
      "Epoch  990\n",
      "Training Accuracy= 0.97 loss =  0.10977329063382524\n",
      "Epoch  991\n",
      "Training Accuracy= 0.97 loss =  0.10982868586013739\n",
      "Epoch  992\n",
      "Training Accuracy= 0.97 loss =  0.10995064946931635\n",
      "Epoch  993\n",
      "Training Accuracy= 0.97 loss =  0.11004822337403009\n",
      "Epoch  994\n",
      "Training Accuracy= 0.97 loss =  0.11015906901274473\n",
      "Epoch  995\n",
      "Training Accuracy= 0.97 loss =  0.11018008117007387\n",
      "Epoch  996\n",
      "Training Accuracy= 0.97 loss =  0.11026718237240048\n",
      "Epoch  997\n",
      "Training Accuracy= 0.97 loss =  0.11034618692176199\n",
      "Epoch  998\n",
      "Training Accuracy= 0.97 loss =  0.11043274062021727\n",
      "Epoch  999\n",
      "Training Accuracy= 0.97 loss =  0.11053322298802867\n",
      "Time cost :  1938.3967134952545\n"
     ]
    }
   ],
   "source": [
    "Layers = (784, 50, 10)\n",
    "BatchSize = 100\n",
    "EpochCount = 1000\n",
    "LearningRate = 0.1\n",
    "Eta = 0.00001\n",
    "\n",
    "mlp60000 = MLP(Layers, BatchSize)\n",
    "t = time.time()\n",
    "mlp60000.train(train_60000_x, train_60000_y, EpochCount, LearningRate, Eta)\n",
    "print(\"Time cost : \", time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f584d3f780>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAF/xJREFUeJzt3XtwXOV5x/Hvs5J8A8c2thKMbSGTECi5YaMALinD5Ao0gUlDEtNMIAmpJ7eWXKatTTIkIW1zmTYXhkyIE8itCZCaQJ1gSimBENpgLBtjA8bYAQcLDLYRyHdZ0j79Y1/Jy3pXZyWtdPY9/n1mdvbsOa/O+7z2+qfjd8+eY+6OiIhkSy7tAkREpPYU7iIiGaRwFxHJIIW7iEgGKdxFRDJI4S4ikkEKdxGRDFK4i4hkkMJdRCSDGpMamNkE4F5gfGi/zN2/WNJmPPBT4DTgeeD97r5lsP3OmDHDW1tbh1e1iMgRavXq1TvdvTmpXWK4A93Am919j5k1AfeZ2e3ufn9Rm8uAF9z9VWa2EPg68P7Bdtra2kp7e3sV3YuISD8z+1M17RKnZbxgT3jZFB6lF6S5EPhJWF4GvMXMrMpaRUSkxqqaczezBjNbC2wH7nT3lSVNZgFbAdy9F+gCpteyUBERqV5V4e7ufe5+KjAbON3MXlvSpNxR+mGXmzSzRWbWbmbtO3bsGHq1IiJSlSGdLePuLwL3AOeWbOoA5gCYWSMwBegs8/NL3b3N3duamxM/DxARkWFKDHczazazqWF5IvBW4LGSZsuBS8PyRcBvXReKFxFJTTVny8wEfmJmDRR+GfzS3X9jZlcB7e6+HLgO+JmZbaZwxL5w1CoWEZFEieHu7uuAeWXWX1m0fAB4b21LExGR4YruG6oHevpYtroDzfqIiFRWzbRMXfnS8ke4cdVWpk5s4q2nvCLtckRE6lJ0R+43rtoKwLqnu1KuRESkfkUX7gPfe9W0jIhIRfGFe3jOK9tFRCqKL9zDobsf/gVYEREJ4gv38KxZGRGRyqIL99zAkbuIiFQSXbj3H7rndeguIlJRdOGeG5iXSbUMEZG6Fl24G5qWERFJEl+4hyN3XX5ARKSy+MI9POs8dxGRyuIL9/6zZRTuIiIVRRjuhWd9iUlEpLL4wj0868hdRKSy+MJ9YFpG6S4iUkl04Z4bmJYREZFKogt3faAqIpIsvnAPz7r8gIhIZfGFuy4cJiKSKMJwLzzrwF1EpLL4wj0862wZEZHK4gt3HbmLiCSKLtxzus2eiEiixHA3szlmdreZbTCzR8zs8jJtzjGzLjNbGx5Xjk65unCYiEg1Gqto0wt8zt3XmNlkYLWZ3enuj5a0+727v7P2Jb5U/9kyIiJSWeKRu7tvc/c1YXk3sAGYNdqFJdF57iIilQ1pzt3MWoF5wMoymxeY2UNmdruZvaYGtZWV669Y2S4iUlE10zIAmNnRwM3Ap919V8nmNcDx7r7HzM4HbgVOLLOPRcAigJaWlmEVrNvsiYgkq+rI3cyaKAT7z939V6Xb3X2Xu+8JyyuAJjObUabdUndvc/e25ubmYRXcP+WuaRkRkcqqOVvGgOuADe7+zQptjg3tMLPTw36fr2WhA32FZ2W7iEhl1UzLnAV8EFhvZmvDuiuAFgB3vxa4CPi4mfUC+4GFPkpfIc3p2jIiIokSw93d7+PQAXOlNtcA19SqqEFpWkZEJFF031Ad+C2jbBcRqSi6cNflB0REkkUX7rpwmIhIsvjCPUzMaM5dRKSy+MJdR+4iIokiDHedCikikiS+cA/PuhOTiEhl8YW7pmVERBJFF+76hqqISLLowl0XDhMRSRZfuKddgIhIBKIL9/5Dd91DVUSksujCPTfwgarSXUSkkujCXUREkkUX7gNny+jAXUSkoujCfeBLTDoZUkSkovjCvf9UyHy6dYiI1LMIw13XcxcRSRJfuIdnzbmLiFQWX7jr2jIiIoniC3c0LSMikiS6cM+FinXkLiJSWXThrtvsiYgkiy/cdeUwEZFE0YV7P104TESkssRwN7M5Zna3mW0ws0fM7PIybczMrjazzWa2zszmj065hy4/0LW/Z7S6EBGJXmMVbXqBz7n7GjObDKw2szvd/dGiNucBJ4bHGcD3wnPN9U/LPLlz72jsXkQkExKP3N19m7uvCcu7gQ3ArJJmFwI/9YL7galmNrPm1XLoyF1ERCob0py7mbUC84CVJZtmAVuLXndw+C8AEREZI1WHu5kdDdwMfNrdd5VuLvMjh33kaWaLzKzdzNp37NgxtEoDnQIpIpKsqnA3syYKwf5zd/9VmSYdwJyi17OBZ0obuftSd29z97bm5ubh1Ms9G4f3S0FE5EhSzdkyBlwHbHD3b1Zothy4JJw1cybQ5e7baliniIgMQTVny5wFfBBYb2Zrw7orgBYAd78WWAGcD2wG9gEfrn2pIiJSrcRwd/f7KD+nXtzGgU/WqigRERmZaL+hKiIilSncRUQySOEuIpJBCncRkQxSuIuIZJDCXUQkg6IL94acLhwmIpIkunCfN2dq2iWIiNS96MJdV/wVEUkWYbgr3UVEkkQX7ppyFxFJFl242+CXuREREWIMd2W7iEii6MJd91AVEUkWXbgr20VEkkUX7jpyFxFJFl24K9tFRJLFF+5pFyAiEoHowv01x01JuwQRkboXXbifdvy0tEsQEal70YW75mVERJJFF+7KdhGRZPGFu06XERFJFF+4p12AiEgEogt3ERFJlhjuZna9mW03s4crbD/HzLrMbG14XFn7Mov7O7TcuffgaHYlIhKtao7cfwycm9Dm9+5+anhcNfKyKiu+5G/7ls7R7EpEJFqJ4e7u9wJKURGRiNRqzn2BmT1kZreb2WtqtM+ydLKMiEiyxhrsYw1wvLvvMbPzgVuBE8s1NLNFwCKAlpaWYXWmbBcRSTbiI3d33+Xue8LyCqDJzGZUaLvU3dvcva25uXl4HRalu855FxEpb8ThbmbHWkhZMzs97PP5ke5XRESGL3FaxsxuAM4BZphZB/BFoAnA3a8FLgI+bma9wH5gobv7qFUsIiKJEsPd3S9O2H4NcE3NKkpQfCrkgZ6+sepWRCQqUX9D9W9veDDtEkRE6lLU4S4iIuUp3EVEMkjhLiKSQdGFu05tFxFJFl24i4hIMoW7iEgGRRfuzZPHp12CiEjdiy7cZ0+bmHYJIiJ1L7pwFxGRZNGFu+mivyIiiaIL91I793SnXYKISN2JLtxLz3Pf292bTiEiInUsunAvpYsLi4gcLvpw3/jc7rRLEBGpO9GFe+nHqVf9+tFU6hARqWfRhXsp3fRJRORw0YV76U2xn+k6kFIlIiL1K7pwFxGRZAp3EZEMii7cy30/tWt/z5jXISJSz6IL93Lmf+XOtEsQEakrmQj3vrzOmBERKRZduOs2eyIiyaILdxERSZYY7mZ2vZltN7OHK2w3M7vazDab2Tozm1/7Ml/S32juXkQkE6o5cv8xcO4g288DTgyPRcD3Rl6WiIiMRGK4u/u9QOcgTS4EfuoF9wNTzWxmrQoUEZGhq8Wc+yxga9HrjrBORERSUotwLzcJXvbcRDNbZGbtZta+Y8eOGnQtIiLl1CLcO4A5Ra9nA8+Ua+juS929zd3bmpuba9D1IXmd6y4iMqAW4b4cuCScNXMm0OXu22qw3yH54X1PjHWXIiJ1q5pTIW8A/gCcZGYdZnaZmX3MzD4WmqwAngA2Az8APjFq1Q7iX1Y8lka3IiJ1qTGpgbtfnLDdgU/WrKIRcHedBy8iQsa+ofpQR1faJYiI1IVMhfvim9elXYKISF3IVLg/9uzutEsQEakLmQp30CmRIiKQwXC/d5O+HCUikrlw/9CPVqVdgohI6jIX7gDdvX1plyAikqpMhvtJX/ivtEsQEUlVJsMd4O6N29MuQUQkNZkN9w//aBWbntOpkSJyZMpsuAO87Vv3sq7jxbTLEBEZc5kOd4ALrvlfLr3+AQqXwBEROTJkPtwBfvf4DuYuWcHqP72QdikiImPiiAj3fu/53v/Ruvg2evryaZciIjKqjqhw73fi52+nc+/BtMsQERk1R2S4A8z/yp2s2tKZdhkiIqPiiA13gPde+wfO/sbd+rBVRDLniA53gKc69zF3yQp27O5OuxQRkZo54sO93xv/+X9Y8NW7dMlgEckEhXuRbV0HOOGKFdy06qm0SxERGRGFexn/ePN6Whffxpade9MuRURkWBTugzjnX++hdfFtvLhPp02KSFwU7lU49ao7aV18G3u7e9MuRUSkKgr3IXjNF++gdfFtPP3i/rRLEREZVGPaBcTorK/9FoBFZ5/A4nNPJpezlCsSEXmpqo7czexcM9toZpvNbHGZ7R8ysx1mtjY8Plr7UuvP0nuf4IQrVtC6+Db+/f4/6TRKEakbiUfuZtYAfBd4G9ABrDKz5e7+aEnTm9z9U6NQYxS+cOvDfOHWhwF497xZfPWvXseEpoaUqxKRI1U10zKnA5vd/QkAM7sRuBAoDXcJbnnwaW558OmB1zf8zZkseOX0FCsSkSNNNeE+C9ha9LoDOKNMu/eY2dnA48Bn3H1rmTY1Mb9lKmueiucOSxf/4P7ENq+bNYWTjp3M62dP4Yy502k5ZhLjG3NRzOf35Z2tnfvIuzN5QhPTJjXR2KDP6kXSVE24l0uX0snlXwM3uHu3mX0M+Anw5sN2ZLYIWATQ0tIyxFIPGd+YvemO9U93sf7pLpat7qiq/cfPeSXva5tD6/RJmI3NL4DdB3pY39HFXY9t57r7nhzRvha+cQ6fffurefnkCTWqTuqVu5N3aEj5QMXd2d/Tx7auA2zevocN23axrqOLh7a+yPNjfAnw3/39ORw//ahR7cOSrohoZguAL7n7O8LrJQDu/tUK7RuATnefMth+29ravL29fVhFX3L9A9z7+I5h/awUTJnYxMnHTub46ZPYtb+XnXu6aa+jO1XNOHo8HzijhXNfeywnHzt5zH6BSbLn93Tzi5VPccvapzlqXCO5nPFc1wGe3XUg7dKisuVrfzmsnzOz1e7eltSumiP3VcCJZjYXeBpYCPx1SWcz3X1beHkBsGGI9Q7JR85qVbiPUNf+HlY+2cnKJ+vzmvY793Tznbs28Z27NpXdftFps7nsTXOHFPydew+ybPVW7tqwnSd27qVz70H6whlO31l4Ku96/XFRTIONtZ17uvnMTWv5/aadaZciQ5AY7u7ea2afAu4AGoDr3f0RM7sKaHf35cDfmdkFQC/QCXxoFGtmos5COeItW91R9RRWNS6/cS2X37h24PW7581i8Xkn84qXDW3aKJ93Xth3kBf29dDTl2f6UeOYcfT4uv+l4e5s393Nrx96hn+6bVSPzWSMVPUlJndfAawoWXdl0fISYEltS6vsuKkTx6orOUKVnvGUhje2TuM7C+eNyvv9mRf3c8Ut67lno/4HnFVRfkN1zjGT0i5BZNSt2vICfx6+DS0yVDpfTUQkgxTuIiIZpHAXEcmgaMP96ovnpV2CiEjdijbc3/X6mWmXICJSt6INd31jUUSksmjDHeD+JW9JuwQRkboUdbgfO0UXnRIRKSfqcBcRkfIU7iIiGaRwFxHJIIW7iEgGKdxFRDJI4S4ikkEKdxGRDIo+3F/xsvFplyAiUneiD3cRETmcwl1EJIMU7iIiGaRwFxHJIIW7iEgGRR/u7mlXICJSf6IPdxEROVz04f5v73tD2iWIiNSdqsLdzM41s41mttnMFpfZPt7MbgrbV5pZa60LreS1x00Zq65ERKKRGO5m1gB8FzgPOAW42MxOKWl2GfCCu78K+Bbw9VoXWsm0o8Zx9qubx6o7EZEoNFbR5nRgs7s/AWBmNwIXAo8WtbkQ+FJYXgZcY2bmPjYfd/70I6fj7qx56gW+/OtHWdfRNRbdiojUrWrCfRawteh1B3BGpTbu3mtmXcB0YGctiqyGmXHa8cew/FNvOmybu/Pcrm4e2NLJ6i2drHyyk8ee3T1WpYmIjLlqwt3KrCs9Iq+mDWa2CFgE0NLSUkXXtWFmHDtlAhe84TgueMNxNd9/X97Zd7CXfQf72Hewj/0H+9jf08fe7sK67t6+gW3dvX109+TZ39PHwd48B/vydPfkOdDTR3dvfmB7d1++sL23j968h+U8vXmnty9PT95xd3rz/pLTQXMGOTOaGnI05IzGhsJyU85obMjxVOe+mo+/VM4o9J3LvfR1Q47GXKGe8Y05xjXmGN/UwMSmHJPGNTJpXAPjGxsY15hj0rgGJjY1FNo25ZjQmKOhIUeDGeMaczQ1WOijsL+cHXrdkDP68s7e8Pew50Av+3v66OnL09Obp7s3T08+f+gdatCYMyY0NjChqYGjxjdy1PhC373hz7mnz+nu7aNrfw+dew6yresAz+46wLPhuS+vc3Klerdf/hej3kc14d4BzCl6PRt4pkKbDjNrBKYAnaU7cvelwFKAtra2zPxraMgZkyc0MXlCU9qliIgA1Z0tswo40czmmtk4YCGwvKTNcuDSsHwR8Nuxmm8XEZHDJR65hzn0TwF3AA3A9e7+iJldBbS7+3LgOuBnZraZwhH7wtEsWkREBlfNtAzuvgJYUbLuyqLlA8B7a1uaiIgMV/TfUBURkcMp3EVEMkjhLiKSQQp3EZEMUriLiGSQpXU6upntAP40zB+fwRhe2mCUaSz1KStjyco4QGPpd7y7J14tMbVwHwkza3f3trTrqAWNpT5lZSxZGQdoLEOlaRkRkQxSuIuIZFCs4b407QJqSGOpT1kZS1bGARrLkEQ55y4iIoOL9chdREQGEV24J92sewzruN7MtpvZw0XrjjGzO81sU3ieFtabmV0dal5nZvOLfubS0H6TmV1atP40M1sffuZqM7PB+hjhWOaY2d1mtsHMHjGzy2Mdj5lNMLMHzOyhMJYvh/Vzw83bN4WbuY8L6yve3N3MloT1G83sHUXry74HK/UxwvE0mNmDZvabyMexJfz9rzWz9rAuuvdX2OdUM1tmZo+FfzML6nIs7h7Ng8Ilh/8InACMAx4CTkmplrOB+cDDReu+ASwOy4uBr4fl84HbKdyx6kxgZVh/DPBEeJ4WlqeFbQ8AC8LP3A6cN1gfIxzLTGB+WJ4MPE7hZujRjSfs/+iw3ASsDDX+ElgY1l8LfDwsfwK4NiwvBG4Ky6eE99d4YG543zUM9h6s1McIx/NZ4BfAbwbrI4JxbAFmlKyL7v0V9vMT4KNheRwwtR7HMuahOMI/1AXAHUWvlwBLUqynlZeG+0ZgZlieCWwMy98HLi5tB1wMfL9o/ffDupnAY0XrB9pV6qPG4/pP4G2xjweYBKyhcM/fnUBj6fuIwn0KFoTlxtDOSt9b/e0qvQfDz5TtYwT1zwbuAt4M/GawPup5HGE/Wzg83KN7fwEvA54kfF5Zz2OJbVqm3M26Z6VUSzmvcPdtAOH55WF9pboHW99RZv1gfdRE+O/8PApHvFGOJ0xlrAW2A3dSOEJ90d17y/T/kpu7A/03dx/qGKcP0sdwfRv4ByAfXg/WRz2PAwp3rP1vM1tthXspQ5zvrxOAHcCPwnTZD83sqHocS2zhXtWNuOtQpbqHun5UmdnRwM3Ap91912BNy6yrm/G4e5+7n0rhyPd04M8G6b9WY6npGM3sncB2d19dvHqQPupyHEXOcvf5wHnAJ83s7EHa1kvN5TRSmI79nrvPA/ZSmCKpJLWxxBbu1dysO03PmdlMgPC8PayvVPdg62eXWT9YHyNiZk0Ugv3n7v6r2McD4O4vAvdQmOucaoWbt5f2P1CzvfTm7kMd485B+hiOs4ALzGwLcCOFqZlvRzgOANz9mfC8HbiFwi/dGN9fHUCHu68Mr5dRCPu6G0ts4V7NzbrTVHyj8EspzF33r78kfHJ+JtAV/lt1B/B2M5sWPvl+O4X5zW3AbjM7M3xSfknJvsr1MWyhj+uADe7+zZjHY2bNZjY1LE8E3gpsAO6mcPP2cmPp77/45u7LgYVWOAtlLnAihQ+6yr4Hw89U6mPI3H2Ju89299bQx2/d/QOxjQPAzI4ys8n9yxTeFw8T4fvL3Z8FtprZSWHVW4BH63IsI/2gZKwfFD59fpzCPOrnU6zjBmAb0EPht+1lFOYr7wI2hedjQlsDvhtqXg+0Fe3nI8Dm8Phw0fo2Cv8A/ghcw6EvnJXtY4RjeROF//qtA9aGx/kxjgd4PfBgGMvDwJVh/QkUQm0z8B/A+LB+Qni9OWw/oWhfnw/1biScsTDYe7BSHzX4+zmHQ2fLRDeOsL+HwuOR/r5ifH+FfZ4KtIf32K0Uznapu7HoG6oiIhkU27SMiIhUQeEuIpJBCncRkQxSuIuIZJDCXUQkgxTuIiIZpHAXEckghbuISAb9P5TS4jj8oYUPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f58468f198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp60000.J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried more than 100 times of every parameters.<br>\n",
    "I found that batch size around 25~50 can get a better result and converge faster.<br>\n",
    "Epoch count can set a larger number to make the training continue to a convergent result.<br>\n",
    "Maybe we can set a stop condition when loss value is almost not changing for a long time.<br>\n",
    "Learning rate also effects the loss value's convergent.<br>\n",
    "My code is now trying momentum learning rate.<br>\n",
    "Learning rate decreases after every epoch, and stop decreasing at lr=0.001<br>\n",
    "This setting helps the speed of convergent.<br>\n",
    "Finally, this lab is a great example to learn how machine works and the parameters meanings inside the model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
